[{"data":1,"prerenderedAt":653},["Reactive",2],{"content-query-1DxZ1vYQk5":3,"content-query-B6mqoO8PxR":499},{"_path":4,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":7,"description":5,"hideTitle":8,"disableFancyImage":8,"body":9,"_type":494,"_id":495,"_source":496,"_file":497,"_extension":498},"/","",false,"Home",true,{"type":10,"children":11,"toc":484},"root",[12,17,25,68,73,114,120,124,133,139,180,214,239,245,255,263,271,291,299,306,314,323,331,339,348,357,363,403,409,447,453,464,474],{"type":13,"tag":14,"props":15,"children":16},"element","IndexHeader",{},[],{"type":13,"tag":18,"props":19,"children":21},"h2",{"id":20},"Ô∏è-about-me",[22],{"type":23,"value":24},"text","ü¶∏üèª‚Äç‚ôÇÔ∏è About Me",{"type":13,"tag":26,"props":27,"children":28},"p",{},[29,31,40,42,49,51,57,59,66],{"type":23,"value":30},"I am a third-year Computer Science Ph.D. candidate at ",{"type":13,"tag":32,"props":33,"children":37},"a",{"href":34,"rel":35},"https://en.sjtu.edu.cn/",[36],"nofollow",[38],{"type":23,"value":39},"Shanghai Jiao Tong University",{"type":23,"value":41}," and a member of ",{"type":13,"tag":32,"props":43,"children":46},{"href":44,"rel":45},"https://www.mvig.org/",[36],[47],{"type":23,"value":48},"Machine Intelligence and Vision Group (MVIG)",{"type":23,"value":50}," under the supervision of ",{"type":13,"tag":32,"props":52,"children":54},{"href":44,"rel":53},[36],[55],{"type":23,"value":56},"Prof. Cewu Lu",{"type":23,"value":58}," (Âç¢Á≠ñÂêæ). I am also a member of ",{"type":13,"tag":32,"props":60,"children":63},{"href":61,"rel":62},"https://ai.sjtu.edu.cn/info/announcements/204",[36],[64],{"type":23,"value":65},"Wu Wen Jun Honorary Doctoral Program",{"type":23,"value":67}," (Âê¥Êñá‰øäËç£Ë™âÂçöÂ£´Áè≠).",{"type":13,"tag":26,"props":69,"children":70},{},[71],{"type":23,"value":72},"I receive my bachelor degree from Shanghai Jiao Tong Universiy in 2021. My research interests lie in Robotics and 3D Vision. Previously, I have been working on deformable object perception and manipulation. Now I am particularly interested in imitation learning with tactile/force sensing and low-cost data collection system.",{"type":13,"tag":26,"props":74,"children":75},{},[76,78,85,87,94,96,103,105,112],{"type":23,"value":77},"In the past, I have interned at ",{"type":13,"tag":32,"props":79,"children":82},{"href":80,"rel":81},"https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/",[36],[83],{"type":23,"value":84},"Microsoft Research Asia",{"type":23,"value":86}," under the supervsion of ",{"type":13,"tag":32,"props":88,"children":91},{"href":89,"rel":90},"https://ancientmooner.github.io/",[36],[92],{"type":23,"value":93},"Han Hu",{"type":23,"value":95}," and ",{"type":13,"tag":32,"props":97,"children":100},{"href":98,"rel":99},"http://yue-cao.me/",[36],[101],{"type":23,"value":102},"Yue Cao",{"type":23,"value":104},". I also spent time with Prof. ",{"type":13,"tag":32,"props":106,"children":109},{"href":107,"rel":108},"http://hxu.rocks/index.html",[36],[110],{"type":23,"value":111},"Huazhe Xu",{"type":23,"value":113}," at Tsinghua University on my projects. In my spare time, I enjoy watching movies and playing with robotsü§ñ.",{"type":13,"tag":18,"props":115,"children":117},{"id":116},"news",[118],{"type":23,"value":119},"üì∞ News",{"type":13,"tag":121,"props":122,"children":123},"ShortNews",{},[],{"type":13,"tag":26,"props":125,"children":126},{},[127],{"type":13,"tag":32,"props":128,"children":130},{"href":129},"/news/",[131],{"type":23,"value":132},"More news >>>",{"type":13,"tag":18,"props":134,"children":136},{"id":135},"experiences",[137],{"type":23,"value":138},"üè´ Experiences",{"type":13,"tag":140,"props":141,"children":143},"ExperienceRow",{"icon":142},"sjtu.png",[144],{"type":13,"tag":26,"props":145,"children":146},{},[147,152,156,158,161,163,168,170,175,178],{"type":13,"tag":148,"props":149,"children":150},"strong",{},[151],{"type":23,"value":39},{"type":13,"tag":153,"props":154,"children":155},"br",{},[],{"type":23,"value":157},"\nPh.D. Student / Master Student.",{"type":13,"tag":153,"props":159,"children":160},{},[],{"type":23,"value":162},"\nResearch assistant in ",{"type":13,"tag":32,"props":164,"children":166},{"href":44,"rel":165},[36],[167],{"type":23,"value":48},{"type":23,"value":169},", advised by ",{"type":13,"tag":32,"props":171,"children":173},{"href":44,"rel":172},[36],[174],{"type":23,"value":56},{"type":13,"tag":153,"props":176,"children":177},{},[],{"type":23,"value":179},"\nSep. 2021 - Present",{"type":13,"tag":140,"props":181,"children":183},{"icon":182},"microsoft.png",[184],{"type":13,"tag":26,"props":185,"children":186},{},[187,191,194,196,201,202,207,209,212],{"type":13,"tag":148,"props":188,"children":189},{},[190],{"type":23,"value":84},{"type":13,"tag":153,"props":192,"children":193},{},[],{"type":23,"value":195},"\nResearch Intern, advised by ",{"type":13,"tag":32,"props":197,"children":199},{"href":89,"rel":198},[36],[200],{"type":23,"value":93},{"type":23,"value":95},{"type":13,"tag":32,"props":203,"children":205},{"href":98,"rel":204},[36],[206],{"type":23,"value":102},{"type":23,"value":208},".",{"type":13,"tag":153,"props":210,"children":211},{},[],{"type":23,"value":213},"\nJul. 2019 - Mar. 2020",{"type":13,"tag":140,"props":215,"children":216},{"icon":142},[217],{"type":13,"tag":26,"props":218,"children":219},{},[220,224,227,229,232,234,237],{"type":13,"tag":148,"props":221,"children":222},{},[223],{"type":23,"value":39},{"type":13,"tag":153,"props":225,"children":226},{},[],{"type":23,"value":228},"\nBachelor of Engineering in Computer Science. ",{"type":13,"tag":153,"props":230,"children":231},{},[],{"type":23,"value":233},"\nGPA 4.04/4.3, Rank 3/150 (Top 2%)",{"type":13,"tag":153,"props":235,"children":236},{},[],{"type":23,"value":238},"\nSep 2016 - Jun. 2021",{"type":13,"tag":18,"props":240,"children":242},{"id":241},"selected-publications",[243],{"type":23,"value":244},"üìÑ Selected Publications",{"type":13,"tag":246,"props":247,"children":254},"PublicationRow",{":artifactLinks":248,":authors":249,":venue":250,"thumbnail":251,"title":252,"type":253},"{\"Website\":\"https://reactive-diffusion-policy.github.io/\",\"arXiv\":\"https://arxiv.org/abs/2503.02881\"}","[\"Han Xue*\",\"Jieji Ren*\",\"Wendi Chen*\",\"Gu Zhang\",\"Yuan Fang\",\"Guoying Gu\",\"Huazhe Xu‚Ä†\",\"Cewu Lu‚Ä† (‚Ä†Equal advising)\"]","{\"acronym\":\"arXiv\",\"year\":2025,\"name\":\"In submission\"}","rdp.gif","Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation","conference",[],{"type":13,"tag":246,"props":256,"children":262},{":artifactLinks":257,":authors":258,":venue":259,"thumbnail":260,"title":261,"type":253},"{\"Website\":\"https://deform-pam.robotflow.ai/\",\"arXiv\":\"https://arxiv.org/abs/2410.11584\",\"Code\":\"https://github.com/xiaoxiaoxh/DeformPAM\"}","[\"Wendi Chen*\",\"Han Xue*\",\"Fangyuan Zhou\",\"Yuan Fang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICRA\",\"year\":2025,\"name\":\"IEEE International Conference on Robotics and Automation (ICRA)\"}","deform-pam.gif","DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object Manipulation via Preference-based Action Alignment",[],{"type":13,"tag":246,"props":264,"children":270},{":artifactLinks":265,":authors":266,":venue":267,"thumbnail":268,"title":269,"type":253},"{\"Proceeding\":\"https://openreview.net/pdf?id=ANJuNDFdvP\",\"arXiv\":\"https://arxiv.org/abs/2311.01267\",\"Code\":\"https://github.com/xiaoxiaoxh/UniFolding\",\"Website\":\"https://unifolding.robotflow.ai/\"}","[\"Han Xue*\",\"Yutong Li*\",\"Wenqiang Xu\",\"Huanyu Li\",\"Dongzhe Zheng\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"CoRL\",\"year\":2023,\"name\":\"7th Annual Conference on Robot Learning.\"}","unifolding.gif","UniFolding: Towards Sample-efficient, Scalable, and Generalizable Robotic Garment Folding",[],{"type":13,"tag":246,"props":272,"children":278},{":artifactLinks":273,":authors":274,":venue":275,"thumbnail":276,"title":277,"type":253},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf\"}","[\"Wenqiang Xu*\",\"Wenxin Du*\",\"Han Xue\",\"Yutong Li\",\"Ruolin Ye\",\"Yan-Feng Wang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICCV\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF International Conference on Computer Vision\"}","clothpose.png","ClothPose: A Real-world Benchmark for Visual Analysis of Garment Pose via An Indirect Recording Solution",[279],{"type":13,"tag":26,"props":280,"children":281},{},[282],{"type":13,"tag":283,"props":284,"children":288},"span",{"className":285},[286,287],"text-red-600","font-bold",[289],{"type":23,"value":290},"üî• Oral Presentation.",{"type":13,"tag":246,"props":292,"children":298},{":artifactLinks":293,":authors":294,":venue":295,"thumbnail":296,"title":297,"type":253},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_GarmentTracking_Category-Level_Garment_Pose_Tracking_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.13913.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/GarmentTracking\",\"Website\":\"https://garment-tracking.robotflow.ai/\"}","[\"Han Xue\",\"Wenqiang Xu\",\"Jieyi Zhang\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Ruolin Ye\",\"Cewu Lu\"]","{\"acronym\":\"CVPR\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\"}","garment-tracking.gif","GarmentTracking: Category-Level Garment Pose Tracking",[],{"type":13,"tag":246,"props":300,"children":305},{":artifactLinks":301,":authors":302,":venue":295,"thumbnail":303,"title":304,"type":253},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Visual-Tactile_Sensing_for_In-Hand_Object_Reconstruction_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.14498.pdf\",\"Code\":\"https://github.com/jeffsonyu/VTacO\",\"Website\":\"https://sites.google.com/view/vtaco/\"}","[\"Wenqiang Xu*\",\"Zhenjun Yu*\",\"Han Xue\",\"Ruolin Ye\",\"Siqiong Yao\",\"Cewu Lu (*Equal contribution)\"]","vtaco.png","Visual-Tactile Sensing for In-Hand Object Reconstruction",[],{"type":13,"tag":246,"props":307,"children":313},{":artifactLinks":308,":authors":309,":venue":310,"thumbnail":311,"title":312,"type":253},"{\"Proceeding\":\"https://www.roboticsproceedings.org/rss19/p087.pdf\",\"Code\":\"https://github.com/mvig-robotflow/pyrfuniverse\",\"Website\":\"https://sites.google.com/view/rfuniverse\"}","[\"Haoyuan Fu*\",\"Wenqiang Xu*\",\"Ruolin Ye*\",\"Han Xue\",\"Zhenjun Yu\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Jieyi Zhang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"RSS\",\"year\":2023,\"name\":\"Robotics: Science and Systems.\"}","rfuniverse.png","Demonstrating RFUniverse: A Multiphysics Simulation Platform for Embodied AI",[],{"type":13,"tag":246,"props":315,"children":322},{":artifactLinks":316,":authors":317,":venue":318,"thumbnail":319,"title":320,"type":321},"{\"arXiv\":\"https://arxiv.org/pdf/2105.03260\"}","[\"Liu Liu*\",\"Han Xue*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"TIP\",\"year\":2022,\"name\":\"IEEE Transactions on Image Processing.\"}","articulation_real.png","Toward Real-World Category-Level Articulation Pose Estimation","journal",[],{"type":13,"tag":246,"props":324,"children":330},{":artifactLinks":325,":authors":326,":venue":327,"thumbnail":328,"title":329,"type":253},"{\"Proceeding\":\"https://www.bmvc2021-virtualconference.com/assets/papers/0544.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2112.07334.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/OMAD\"}","[\"Han Xue*\",\"Liu Liu*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"BMVC\",\"year\":2021,\"name\":\"The 32nd British Machine Vision Conference.\"}","omad.png","OMAD: Object Model with Articulated Deformations for Pose Estimation and Retrieval",[],{"type":13,"tag":246,"props":332,"children":338},{":artifactLinks":333,":authors":334,":venue":335,"thumbnail":336,"title":337,"type":253},"{\"Proceeding\":\"https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660222.pdf\",\"arXiv\":\"https://arxiv.org/pdf/1912.11473.pdf\",\"Code\":\"https://github.com/justimyhxu/Dense-RepPoints\"}","[\"Ze Yang*\",\"Yinghao Xu*\",\"Han Xue*\",\"Zheng Zhang\",\"Raquel Urtasun\",\"Liwei Wang\",\"Stephen Lin\",\"Han Hu (*Equal contribution)\"]","{\"acronym\":\"ECCV\",\"year\":2020,\"name\":\"The European Conference on Computer Vision.\"}","dense_reppoints.jpg","Dense RepPoints: Representing Visual Objects with Dense Point Sets",[],{"type":13,"tag":246,"props":340,"children":347},{":artifactLinks":341,":authors":342,":venue":343,"thumbnail":344,"title":345,"type":253,":hideBottomBorder":346},"{\"Proceeding\":\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf\",\"Code\":\"https://github.com/driving-behavior/DBNet\",\"Website\":\"http://www.dbehavior.net/\"}","[\"Yiping Chen*\",\"Jingkang Wang*\",\"Jonathan Li\",\"Cewu Lu\",\"Zhipeng Luo\",\"Han Xue\",\"Cheng Wang (*Equal contribution)\"]","{\"acronym\":\"CVPR\",\"year\":2018,\"name\":\"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\"}","dbnet.jpg","Lidar-video driving dataset: Learning driving policies effectively","true",[],{"type":13,"tag":26,"props":349,"children":350},{},[351],{"type":13,"tag":32,"props":352,"children":354},{"href":353},"/publication/",[355],{"type":23,"value":356},"Full publication list >>>",{"type":13,"tag":18,"props":358,"children":360},{"id":359},"talks",[361],{"type":23,"value":362},"‚ú® Talks",{"type":13,"tag":364,"props":365,"children":366},"ul",{},[367,381],{"type":13,"tag":368,"props":369,"children":370},"li",{},[371,373,379],{"type":23,"value":372},"[Oct. 2024] Invited talk @ ",{"type":13,"tag":32,"props":374,"children":376},{"href":107,"rel":375},[36],[377],{"type":23,"value":378},"TEA lab",{"type":23,"value":380}," in Tsinghua University, IIIS on \"Efficient Learning for Long-horizon Deformable Object Manipulation\"",{"type":13,"tag":368,"props":382,"children":383},{},[384,386,393,395,402],{"type":23,"value":385},"[Dec. 2024] Invited talk @ ",{"type":13,"tag":32,"props":387,"children":390},{"href":388,"rel":389},"https://www.roscon.cn/2024/index.html",[36],[391],{"type":23,"value":392},"ROSCon China 2024",{"type":23,"value":394}," workshop on \"Development Trends and Challenges in Embodied AI\". The link of the recorded video is ",{"type":13,"tag":32,"props":396,"children":399},{"href":397,"rel":398},"https://www.bilibili.com/video/BV1Z3cteAEnD/?spm_id_from=333.337.search-card.all.click&vd_source=8e062051896958b92b4759e0f4753657",[36],[400],{"type":23,"value":401},"here",{"type":23,"value":208},{"type":13,"tag":18,"props":404,"children":406},{"id":405},"awards",[407],{"type":23,"value":408},"üèÜ Awards",{"type":13,"tag":364,"props":410,"children":411},{},[412,417,422,427,432,437,442],{"type":13,"tag":368,"props":413,"children":414},{},[415],{"type":23,"value":416},"Outstanding Graduates in Shanghai (Top 3%) [‰∏äÊµ∑Â∏Ç‰ºòÁßÄÊØï‰∏öÁîü] in 2021.",{"type":13,"tag":368,"props":418,"children":419},{},[420],{"type":23,"value":421},"Rongchang Technology Innovation Scholarship (Top 10 students in SJTU) [Ëç£Êò∂ÁßëÊäÄÂàõÊñ∞Â•ñÂ≠¶Èáë] in 2020.",{"type":13,"tag":368,"props":423,"children":424},{},[425],{"type":23,"value":426},"SenseTime Scholarship (Top 21 undergraduates in China) [ÂïÜÊ±§Â•ñÂ≠¶Èáë] in 2020.",{"type":13,"tag":368,"props":428,"children":429},{},[430],{"type":23,"value":431},"National Scholarship (Top 3 students in CS Department) in 2017, 2018 and 2019 (three consecutive years) [ÂõΩÂÆ∂Â•ñÂ≠¶ÈáëÔºàËøûÁª≠‰∏âÂπ¥Ôºâ].",{"type":13,"tag":368,"props":433,"children":434},{},[435],{"type":23,"value":436},"Academic Excellence Scholarship (Class A) of SJTU (Top 1% in SJTU) in 2018.",{"type":13,"tag":368,"props":438,"children":439},{},[440],{"type":23,"value":441},"Meritorious Winner Prize of Mathematical Contest in Modeling in 2018.",{"type":13,"tag":368,"props":443,"children":444},{},[445],{"type":23,"value":446},"1st Prize in China Undergraduate Mathematical Contest in Modeling (Shanghai Division) in 2017.",{"type":13,"tag":18,"props":448,"children":450},{"id":449},"find-me",[451],{"type":23,"value":452},"üìß Find Me",{"type":13,"tag":454,"props":455,"children":458},"contact-item",{"icon":456,"url":457},"email","mailto:xiaoxiaoxh@sjtu.edu.cn",[459],{"type":13,"tag":26,"props":460,"children":461},{},[462],{"type":23,"value":463},"Email",{"type":13,"tag":454,"props":465,"children":468},{"icon":466,"url":467},"github","https://github.com/xiaoxiaoxh",[469],{"type":13,"tag":26,"props":470,"children":471},{},[472],{"type":23,"value":473},"GitHub",{"type":13,"tag":454,"props":475,"children":478},{"icon":476,"url":477},"twitter","https://twitter.com/HanXue012",[479],{"type":13,"tag":26,"props":480,"children":481},{},[482],{"type":23,"value":483},"Twitter",{"title":5,"searchDepth":485,"depth":485,"links":486},2,[487,488,489,490,491,492,493],{"id":20,"depth":485,"text":24},{"id":116,"depth":485,"text":119},{"id":135,"depth":485,"text":138},{"id":241,"depth":485,"text":244},{"id":359,"depth":485,"text":362},{"id":405,"depth":485,"text":408},{"id":449,"depth":485,"text":452},"markdown","content:index.md","content","index.md","md",{"_path":500,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":501,"description":5,"leadingImage":502,"disableFancyImage":8,"body":503,"_type":494,"_id":651,"_source":496,"_file":652,"_extension":498},"/news","News","me-news-google.png",{"type":10,"children":504,"toc":649},[505,510,557],{"type":13,"tag":506,"props":507,"children":509},"MarkdownHeader",{"subtitle":508,"title":501},"üì¢ Latest: One paper (DeformPAM) is accepted by ICRA 2025!",[],{"type":13,"tag":364,"props":511,"children":512},{},[513,532],{"type":13,"tag":368,"props":514,"children":515},{},[516,521,523,530],{"type":13,"tag":148,"props":517,"children":518},{},[519],{"type":23,"value":520},"01/29/2025",{"type":23,"value":522}," üéâ One paper (",{"type":13,"tag":32,"props":524,"children":527},{"href":525,"rel":526},"https://deform-pam.robotflow.ai/",[36],[528],{"type":23,"value":529},"DeformPAM",{"type":23,"value":531},") is accepted by ICRA 2025!",{"type":13,"tag":368,"props":533,"children":534},{},[535,540,542,549,551,556],{"type":13,"tag":148,"props":536,"children":537},{},[538],{"type":23,"value":539},"10/14/2024",{"type":23,"value":541}," üñ•Ô∏è We have released the ",{"type":13,"tag":32,"props":543,"children":546},{"href":544,"rel":545},"https://github.com/xiaoxiaoxh/DeformPAM",[36],[547],{"type":23,"value":548},"code",{"type":23,"value":550}," of ",{"type":13,"tag":32,"props":552,"children":554},{"href":525,"rel":553},[36],[555],{"type":23,"value":529},{"type":23,"value":208},{"type":13,"tag":364,"props":558,"children":559},{},[560,578,604,622],{"type":13,"tag":368,"props":561,"children":562},{},[563,568,569,576],{"type":13,"tag":148,"props":564,"children":565},{},[566],{"type":23,"value":567},"08/31/2023",{"type":23,"value":522},{"type":13,"tag":32,"props":570,"children":573},{"href":571,"rel":572},"https://unifolding.robotflow.ai/",[36],[574],{"type":23,"value":575},"UniFolding",{"type":23,"value":577},") is accepted by CoRL 2023!",{"type":13,"tag":368,"props":579,"children":580},{},[581,586,588,595,597,602],{"type":13,"tag":148,"props":582,"children":583},{},[584],{"type":23,"value":585},"07/14/2023",{"type":23,"value":587}," üî• One paper (",{"type":13,"tag":32,"props":589,"children":592},{"href":590,"rel":591},"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf",[36],[593],{"type":23,"value":594},"ClothPose",{"type":23,"value":596},") is accepted by ICCV 2023 as an ",{"type":13,"tag":148,"props":598,"children":599},{},[600],{"type":23,"value":601},"oral presentation",{"type":23,"value":603},"!",{"type":13,"tag":368,"props":605,"children":606},{},[607,612,613,620],{"type":13,"tag":148,"props":608,"children":609},{},[610],{"type":23,"value":611},"05/13/2023",{"type":23,"value":522},{"type":13,"tag":32,"props":614,"children":617},{"href":615,"rel":616},"https://sites.google.com/view/rfuniverse",[36],[618],{"type":23,"value":619},"RFUniverse",{"type":23,"value":621},") is accepted by RSS 2023!",{"type":13,"tag":368,"props":623,"children":624},{},[625,630,632,639,640,647],{"type":13,"tag":148,"props":626,"children":627},{},[628],{"type":23,"value":629},"02/28/2023",{"type":23,"value":631}," üéâ Two papers (",{"type":13,"tag":32,"props":633,"children":636},{"href":634,"rel":635},"https://garment-tracking.robotflow.ai/",[36],[637],{"type":23,"value":638},"GarmentTracking",{"type":23,"value":95},{"type":13,"tag":32,"props":641,"children":644},{"href":642,"rel":643},"https://sites.google.com/view/vtaco/",[36],[645],{"type":23,"value":646},"VTaCo",{"type":23,"value":648},") are accepted by CVPR 2023!",{"title":5,"searchDepth":485,"depth":485,"links":650},[],"content:news.md","news.md",1741333053568]