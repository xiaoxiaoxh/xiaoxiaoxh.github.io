[{"data":1,"prerenderedAt":660},["Reactive",2],{"content-query-1DxZ1vYQk5":3,"content-query-B6mqoO8PxR":504},{"_path":4,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":7,"description":5,"hideTitle":8,"disableFancyImage":8,"body":9,"_type":499,"_id":500,"_source":501,"_file":502,"_extension":503},"/","",false,"Home",true,{"type":10,"children":11,"toc":489},"root",[12,17,25,68,73,114,119,125,129,138,144,185,219,244,250,260,268,276,296,304,311,319,328,336,344,353,362,368,408,414,452,458,469,479],{"type":13,"tag":14,"props":15,"children":16},"element","IndexHeader",{},[],{"type":13,"tag":18,"props":19,"children":21},"h2",{"id":20},"Ô∏è-about-me",[22],{"type":23,"value":24},"text","ü¶∏üèª‚Äç‚ôÇÔ∏è About Me",{"type":13,"tag":26,"props":27,"children":28},"p",{},[29,31,40,42,49,51,57,59,66],{"type":23,"value":30},"I am a fourth-year Computer Science Ph.D. candidate at ",{"type":13,"tag":32,"props":33,"children":37},"a",{"href":34,"rel":35},"https://en.sjtu.edu.cn/",[36],"nofollow",[38],{"type":23,"value":39},"Shanghai Jiao Tong University",{"type":23,"value":41}," and a member of ",{"type":13,"tag":32,"props":43,"children":46},{"href":44,"rel":45},"https://www.mvig.org/",[36],[47],{"type":23,"value":48},"Machine Intelligence and Vision Group (MVIG)",{"type":23,"value":50}," under the supervision of ",{"type":13,"tag":32,"props":52,"children":54},{"href":44,"rel":53},[36],[55],{"type":23,"value":56},"Prof. Cewu Lu",{"type":23,"value":58}," (Âç¢Á≠ñÂêæ). I am also a member of ",{"type":13,"tag":32,"props":60,"children":63},{"href":61,"rel":62},"https://ai.sjtu.edu.cn/info/announcements/204",[36],[64],{"type":23,"value":65},"Wu Wen Jun Honorary Doctoral Program",{"type":23,"value":67}," (Âê¥Êñá‰øäËç£Ë™âÂçöÂ£´Áè≠).",{"type":13,"tag":26,"props":69,"children":70},{},[71],{"type":23,"value":72},"I receive my bachelor degree from Shanghai Jiao Tong Universiy in 2021. My research interests lie in Robotics and 3D Vision. Previously, I have been working on deformable object perception and manipulation. Now I am particularly interested in imitation learning with tactile/force sensing and low-cost data collection system.",{"type":13,"tag":26,"props":74,"children":75},{},[76,78,85,87,94,96,103,105,112],{"type":23,"value":77},"In the past, I have interned at ",{"type":13,"tag":32,"props":79,"children":82},{"href":80,"rel":81},"https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/",[36],[83],{"type":23,"value":84},"Microsoft Research Asia",{"type":23,"value":86}," under the supervsion of ",{"type":13,"tag":32,"props":88,"children":91},{"href":89,"rel":90},"https://ancientmooner.github.io/",[36],[92],{"type":23,"value":93},"Han Hu",{"type":23,"value":95}," and ",{"type":13,"tag":32,"props":97,"children":100},{"href":98,"rel":99},"http://yue-cao.me/",[36],[101],{"type":23,"value":102},"Yue Cao",{"type":23,"value":104},". I also spent time with Prof. ",{"type":13,"tag":32,"props":106,"children":109},{"href":107,"rel":108},"http://hxu.rocks/index.html",[36],[110],{"type":23,"value":111},"Huazhe Xu",{"type":23,"value":113}," at Tsinghua University on my projects. In my spare time, I enjoy watching movies and playing with robotsü§ñ.",{"type":13,"tag":26,"props":115,"children":116},{},[117],{"type":23,"value":118},"I'm on the job market now!",{"type":13,"tag":18,"props":120,"children":122},{"id":121},"news",[123],{"type":23,"value":124},"üì∞ News",{"type":13,"tag":126,"props":127,"children":128},"ShortNews",{},[],{"type":13,"tag":26,"props":130,"children":131},{},[132],{"type":13,"tag":32,"props":133,"children":135},{"href":134},"/news/",[136],{"type":23,"value":137},"More news >>>",{"type":13,"tag":18,"props":139,"children":141},{"id":140},"experiences",[142],{"type":23,"value":143},"üè´ Experiences",{"type":13,"tag":145,"props":146,"children":148},"ExperienceRow",{"icon":147},"sjtu.png",[149],{"type":13,"tag":26,"props":150,"children":151},{},[152,157,161,163,166,168,173,175,180,183],{"type":13,"tag":153,"props":154,"children":155},"strong",{},[156],{"type":23,"value":39},{"type":13,"tag":158,"props":159,"children":160},"br",{},[],{"type":23,"value":162},"\nPh.D. Student ",{"type":13,"tag":158,"props":164,"children":165},{},[],{"type":23,"value":167},"\nResearch assistant in ",{"type":13,"tag":32,"props":169,"children":171},{"href":44,"rel":170},[36],[172],{"type":23,"value":48},{"type":23,"value":174},", advised by ",{"type":13,"tag":32,"props":176,"children":178},{"href":44,"rel":177},[36],[179],{"type":23,"value":56},{"type":13,"tag":158,"props":181,"children":182},{},[],{"type":23,"value":184},"\nSep. 2021 - Present",{"type":13,"tag":145,"props":186,"children":188},{"icon":187},"microsoft.png",[189],{"type":13,"tag":26,"props":190,"children":191},{},[192,196,199,201,206,207,212,214,217],{"type":13,"tag":153,"props":193,"children":194},{},[195],{"type":23,"value":84},{"type":13,"tag":158,"props":197,"children":198},{},[],{"type":23,"value":200},"\nResearch Intern, advised by ",{"type":13,"tag":32,"props":202,"children":204},{"href":89,"rel":203},[36],[205],{"type":23,"value":93},{"type":23,"value":95},{"type":13,"tag":32,"props":208,"children":210},{"href":98,"rel":209},[36],[211],{"type":23,"value":102},{"type":23,"value":213},".",{"type":13,"tag":158,"props":215,"children":216},{},[],{"type":23,"value":218},"\nJul. 2019 - Mar. 2020",{"type":13,"tag":145,"props":220,"children":221},{"icon":147},[222],{"type":13,"tag":26,"props":223,"children":224},{},[225,229,232,234,237,239,242],{"type":13,"tag":153,"props":226,"children":227},{},[228],{"type":23,"value":39},{"type":13,"tag":158,"props":230,"children":231},{},[],{"type":23,"value":233},"\nBachelor of Engineering in Computer Science. ",{"type":13,"tag":158,"props":235,"children":236},{},[],{"type":23,"value":238},"\nGPA 4.04/4.3, Rank 3/150 (Top 2%)",{"type":13,"tag":158,"props":240,"children":241},{},[],{"type":23,"value":243},"\nSep 2016 - Jun. 2021",{"type":13,"tag":18,"props":245,"children":247},{"id":246},"selected-publications",[248],{"type":23,"value":249},"üìÑ Selected Publications",{"type":13,"tag":251,"props":252,"children":259},"PublicationRow",{":artifactLinks":253,":authors":254,":venue":255,"thumbnail":256,"title":257,"type":258},"{\"Website\":\"https://reactive-diffusion-policy.github.io/\",\"arXiv\":\"https://arxiv.org/abs/2503.02881\",\"Code\":\"https://github.com/xiaoxiaoxh/reactive_diffusion_policy\"}","[\"Han Xue*\",\"Jieji Ren*\",\"Wendi Chen*\",\"Gu Zhang\",\"Yuan Fang\",\"Guoying Gu\",\"Huazhe Xu‚Ä†\",\"Cewu Lu‚Ä† (‚Ä†Equal advising)\"]","{\"acronym\":\"arXiv\",\"year\":2025,\"name\":\"arXiv\"}","rdp.gif","Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation","conference",[],{"type":13,"tag":251,"props":261,"children":267},{":artifactLinks":262,":authors":263,":venue":264,"thumbnail":265,"title":266,"type":258},"{\"Website\":\"https://deform-pam.robotflow.ai/\",\"arXiv\":\"https://arxiv.org/abs/2410.11584\",\"Code\":\"https://github.com/xiaoxiaoxh/DeformPAM\"}","[\"Wendi Chen*\",\"Han Xue*\",\"Fangyuan Zhou\",\"Yuan Fang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICRA\",\"year\":2025,\"name\":\"IEEE International Conference on Robotics and Automation (ICRA)\"}","deform-pam.gif","DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object Manipulation via Preference-based Action Alignment",[],{"type":13,"tag":251,"props":269,"children":275},{":artifactLinks":270,":authors":271,":venue":272,"thumbnail":273,"title":274,"type":258},"{\"Proceeding\":\"https://openreview.net/pdf?id=ANJuNDFdvP\",\"arXiv\":\"https://arxiv.org/abs/2311.01267\",\"Code\":\"https://github.com/xiaoxiaoxh/UniFolding\",\"Website\":\"https://unifolding.robotflow.ai/\"}","[\"Han Xue*\",\"Yutong Li*\",\"Wenqiang Xu\",\"Huanyu Li\",\"Dongzhe Zheng\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"CoRL\",\"year\":2023,\"name\":\"7th Annual Conference on Robot Learning.\"}","unifolding.gif","UniFolding: Towards Sample-efficient, Scalable, and Generalizable Robotic Garment Folding",[],{"type":13,"tag":251,"props":277,"children":283},{":artifactLinks":278,":authors":279,":venue":280,"thumbnail":281,"title":282,"type":258},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf\"}","[\"Wenqiang Xu*\",\"Wenxin Du*\",\"Han Xue\",\"Yutong Li\",\"Ruolin Ye\",\"Yan-Feng Wang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICCV\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF International Conference on Computer Vision\"}","clothpose.png","ClothPose: A Real-world Benchmark for Visual Analysis of Garment Pose via An Indirect Recording Solution",[284],{"type":13,"tag":26,"props":285,"children":286},{},[287],{"type":13,"tag":288,"props":289,"children":293},"span",{"className":290},[291,292],"text-red-600","font-bold",[294],{"type":23,"value":295},"üî• Oral Presentation.",{"type":13,"tag":251,"props":297,"children":303},{":artifactLinks":298,":authors":299,":venue":300,"thumbnail":301,"title":302,"type":258},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_GarmentTracking_Category-Level_Garment_Pose_Tracking_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.13913.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/GarmentTracking\",\"Website\":\"https://garment-tracking.robotflow.ai/\"}","[\"Han Xue\",\"Wenqiang Xu\",\"Jieyi Zhang\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Ruolin Ye\",\"Cewu Lu\"]","{\"acronym\":\"CVPR\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\"}","garment-tracking.gif","GarmentTracking: Category-Level Garment Pose Tracking",[],{"type":13,"tag":251,"props":305,"children":310},{":artifactLinks":306,":authors":307,":venue":300,"thumbnail":308,"title":309,"type":258},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Visual-Tactile_Sensing_for_In-Hand_Object_Reconstruction_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.14498.pdf\",\"Code\":\"https://github.com/jeffsonyu/VTacO\",\"Website\":\"https://sites.google.com/view/vtaco/\"}","[\"Wenqiang Xu*\",\"Zhenjun Yu*\",\"Han Xue\",\"Ruolin Ye\",\"Siqiong Yao\",\"Cewu Lu (*Equal contribution)\"]","vtaco.png","Visual-Tactile Sensing for In-Hand Object Reconstruction",[],{"type":13,"tag":251,"props":312,"children":318},{":artifactLinks":313,":authors":314,":venue":315,"thumbnail":316,"title":317,"type":258},"{\"Proceeding\":\"https://www.roboticsproceedings.org/rss19/p087.pdf\",\"Code\":\"https://github.com/mvig-robotflow/pyrfuniverse\",\"Website\":\"https://sites.google.com/view/rfuniverse\"}","[\"Haoyuan Fu*\",\"Wenqiang Xu*\",\"Ruolin Ye*\",\"Han Xue\",\"Zhenjun Yu\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Jieyi Zhang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"RSS\",\"year\":2023,\"name\":\"Robotics: Science and Systems.\"}","rfuniverse.png","Demonstrating RFUniverse: A Multiphysics Simulation Platform for Embodied AI",[],{"type":13,"tag":251,"props":320,"children":327},{":artifactLinks":321,":authors":322,":venue":323,"thumbnail":324,"title":325,"type":326},"{\"arXiv\":\"https://arxiv.org/pdf/2105.03260\"}","[\"Liu Liu*\",\"Han Xue*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"TIP\",\"year\":2022,\"name\":\"IEEE Transactions on Image Processing.\"}","articulation_real.png","Toward Real-World Category-Level Articulation Pose Estimation","journal",[],{"type":13,"tag":251,"props":329,"children":335},{":artifactLinks":330,":authors":331,":venue":332,"thumbnail":333,"title":334,"type":258},"{\"Proceeding\":\"https://www.bmvc2021-virtualconference.com/assets/papers/0544.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2112.07334.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/OMAD\"}","[\"Han Xue*\",\"Liu Liu*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"BMVC\",\"year\":2021,\"name\":\"The 32nd British Machine Vision Conference.\"}","omad.png","OMAD: Object Model with Articulated Deformations for Pose Estimation and Retrieval",[],{"type":13,"tag":251,"props":337,"children":343},{":artifactLinks":338,":authors":339,":venue":340,"thumbnail":341,"title":342,"type":258},"{\"Proceeding\":\"https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660222.pdf\",\"arXiv\":\"https://arxiv.org/pdf/1912.11473.pdf\",\"Code\":\"https://github.com/justimyhxu/Dense-RepPoints\"}","[\"Ze Yang*\",\"Yinghao Xu*\",\"Han Xue*\",\"Zheng Zhang\",\"Raquel Urtasun\",\"Liwei Wang\",\"Stephen Lin\",\"Han Hu (*Equal contribution)\"]","{\"acronym\":\"ECCV\",\"year\":2020,\"name\":\"The European Conference on Computer Vision.\"}","dense_reppoints.jpg","Dense RepPoints: Representing Visual Objects with Dense Point Sets",[],{"type":13,"tag":251,"props":345,"children":352},{":artifactLinks":346,":authors":347,":venue":348,"thumbnail":349,"title":350,"type":258,":hideBottomBorder":351},"{\"Proceeding\":\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf\",\"Code\":\"https://github.com/driving-behavior/DBNet\",\"Website\":\"http://www.dbehavior.net/\"}","[\"Yiping Chen*\",\"Jingkang Wang*\",\"Jonathan Li\",\"Cewu Lu\",\"Zhipeng Luo\",\"Han Xue\",\"Cheng Wang (*Equal contribution)\"]","{\"acronym\":\"CVPR\",\"year\":2018,\"name\":\"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\"}","dbnet.jpg","Lidar-video driving dataset: Learning driving policies effectively","true",[],{"type":13,"tag":26,"props":354,"children":355},{},[356],{"type":13,"tag":32,"props":357,"children":359},{"href":358},"/publication/",[360],{"type":23,"value":361},"Full publication list >>>",{"type":13,"tag":18,"props":363,"children":365},{"id":364},"talks",[366],{"type":23,"value":367},"‚ú® Talks",{"type":13,"tag":369,"props":370,"children":371},"ul",{},[372,386],{"type":13,"tag":373,"props":374,"children":375},"li",{},[376,378,384],{"type":23,"value":377},"[Oct. 2024] Invited talk @ ",{"type":13,"tag":32,"props":379,"children":381},{"href":107,"rel":380},[36],[382],{"type":23,"value":383},"TEA lab",{"type":23,"value":385}," in Tsinghua University, IIIS on \"Efficient Learning for Long-horizon Deformable Object Manipulation\"",{"type":13,"tag":373,"props":387,"children":388},{},[389,391,398,400,407],{"type":23,"value":390},"[Dec. 2024] Invited talk @ ",{"type":13,"tag":32,"props":392,"children":395},{"href":393,"rel":394},"https://www.roscon.cn/2024/index.html",[36],[396],{"type":23,"value":397},"ROSCon China 2024",{"type":23,"value":399}," workshop on \"Development Trends and Challenges in Embodied AI\". The link of the recorded video is ",{"type":13,"tag":32,"props":401,"children":404},{"href":402,"rel":403},"https://www.bilibili.com/video/BV1Z3cteAEnD/?spm_id_from=333.337.search-card.all.click&vd_source=8e062051896958b92b4759e0f4753657",[36],[405],{"type":23,"value":406},"here",{"type":23,"value":213},{"type":13,"tag":18,"props":409,"children":411},{"id":410},"awards",[412],{"type":23,"value":413},"üèÜ Awards",{"type":13,"tag":369,"props":415,"children":416},{},[417,422,427,432,437,442,447],{"type":13,"tag":373,"props":418,"children":419},{},[420],{"type":23,"value":421},"Outstanding Graduates in Shanghai (Top 3%) [‰∏äÊµ∑Â∏Ç‰ºòÁßÄÊØï‰∏öÁîü] in 2021.",{"type":13,"tag":373,"props":423,"children":424},{},[425],{"type":23,"value":426},"Rongchang Technology Innovation Scholarship (Top 10 students in SJTU) [Ëç£Êò∂ÁßëÊäÄÂàõÊñ∞Â•ñÂ≠¶Èáë] in 2020.",{"type":13,"tag":373,"props":428,"children":429},{},[430],{"type":23,"value":431},"SenseTime Scholarship (Top 21 undergraduates in China) [ÂïÜÊ±§Â•ñÂ≠¶Èáë] in 2020.",{"type":13,"tag":373,"props":433,"children":434},{},[435],{"type":23,"value":436},"National Scholarship (Top 3 students in CS Department) in 2017, 2018 and 2019 (three consecutive years) [ÂõΩÂÆ∂Â•ñÂ≠¶ÈáëÔºàËøûÁª≠‰∏âÂπ¥Ôºâ].",{"type":13,"tag":373,"props":438,"children":439},{},[440],{"type":23,"value":441},"Academic Excellence Scholarship (Class A) of SJTU (Top 1% in SJTU) in 2018.",{"type":13,"tag":373,"props":443,"children":444},{},[445],{"type":23,"value":446},"Meritorious Winner Prize of Mathematical Contest in Modeling in 2018.",{"type":13,"tag":373,"props":448,"children":449},{},[450],{"type":23,"value":451},"1st Prize in China Undergraduate Mathematical Contest in Modeling (Shanghai Division) in 2017.",{"type":13,"tag":18,"props":453,"children":455},{"id":454},"find-me",[456],{"type":23,"value":457},"üìß Find Me",{"type":13,"tag":459,"props":460,"children":463},"contact-item",{"icon":461,"url":462},"email","mailto:xiaoxiaoxh@sjtu.edu.cn",[464],{"type":13,"tag":26,"props":465,"children":466},{},[467],{"type":23,"value":468},"Email",{"type":13,"tag":459,"props":470,"children":473},{"icon":471,"url":472},"github","https://github.com/xiaoxiaoxh",[474],{"type":13,"tag":26,"props":475,"children":476},{},[477],{"type":23,"value":478},"GitHub",{"type":13,"tag":459,"props":480,"children":483},{"icon":481,"url":482},"twitter","https://twitter.com/HanXue012",[484],{"type":13,"tag":26,"props":485,"children":486},{},[487],{"type":23,"value":488},"Twitter",{"title":5,"searchDepth":490,"depth":490,"links":491},2,[492,493,494,495,496,497,498],{"id":20,"depth":490,"text":24},{"id":121,"depth":490,"text":124},{"id":140,"depth":490,"text":143},{"id":246,"depth":490,"text":249},{"id":364,"depth":490,"text":367},{"id":410,"depth":490,"text":413},{"id":454,"depth":490,"text":457},"markdown","content:index.md","content","index.md","md",{"_path":505,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":506,"description":5,"leadingImage":507,"disableFancyImage":8,"body":508,"_type":499,"_id":658,"_source":501,"_file":659,"_extension":503},"/news","News","me-news-google.png",{"type":10,"children":509,"toc":656},[510,515,565],{"type":13,"tag":511,"props":512,"children":514},"MarkdownHeader",{"subtitle":513,"title":506},"üì¢ Latest: One paper (DeformPAM) is accepted by ICRA 2025!",[],{"type":13,"tag":369,"props":516,"children":517},{},[518,546],{"type":13,"tag":373,"props":519,"children":520},{},[521,526,528,535,537,544],{"type":13,"tag":153,"props":522,"children":523},{},[524],{"type":23,"value":525},"04/03/2025",{"type":23,"value":527}," üñ•Ô∏è We have released the ",{"type":13,"tag":32,"props":529,"children":532},{"href":530,"rel":531},"https://github.com/xiaoxiaoxh/reactive_diffusion_policy",[36],[533],{"type":23,"value":534},"code",{"type":23,"value":536}," of ",{"type":13,"tag":32,"props":538,"children":541},{"href":539,"rel":540},"https://reactive-diffusion-policy.github.io/",[36],[542],{"type":23,"value":543},"Reactive Diffusion Policy",{"type":23,"value":545},"!",{"type":13,"tag":373,"props":547,"children":548},{},[549,554,556,563],{"type":13,"tag":153,"props":550,"children":551},{},[552],{"type":23,"value":553},"01/29/2025",{"type":23,"value":555}," üéâ One paper (",{"type":13,"tag":32,"props":557,"children":560},{"href":558,"rel":559},"https://deform-pam.robotflow.ai/",[36],[561],{"type":23,"value":562},"DeformPAM",{"type":23,"value":564},") is accepted by ICRA 2025!",{"type":13,"tag":369,"props":566,"children":567},{},[568,586,611,629],{"type":13,"tag":373,"props":569,"children":570},{},[571,576,577,584],{"type":13,"tag":153,"props":572,"children":573},{},[574],{"type":23,"value":575},"08/31/2023",{"type":23,"value":555},{"type":13,"tag":32,"props":578,"children":581},{"href":579,"rel":580},"https://unifolding.robotflow.ai/",[36],[582],{"type":23,"value":583},"UniFolding",{"type":23,"value":585},") is accepted by CoRL 2023!",{"type":13,"tag":373,"props":587,"children":588},{},[589,594,596,603,605,610],{"type":13,"tag":153,"props":590,"children":591},{},[592],{"type":23,"value":593},"07/14/2023",{"type":23,"value":595}," üî• One paper (",{"type":13,"tag":32,"props":597,"children":600},{"href":598,"rel":599},"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf",[36],[601],{"type":23,"value":602},"ClothPose",{"type":23,"value":604},") is accepted by ICCV 2023 as an ",{"type":13,"tag":153,"props":606,"children":607},{},[608],{"type":23,"value":609},"oral presentation",{"type":23,"value":545},{"type":13,"tag":373,"props":612,"children":613},{},[614,619,620,627],{"type":13,"tag":153,"props":615,"children":616},{},[617],{"type":23,"value":618},"05/13/2023",{"type":23,"value":555},{"type":13,"tag":32,"props":621,"children":624},{"href":622,"rel":623},"https://sites.google.com/view/rfuniverse",[36],[625],{"type":23,"value":626},"RFUniverse",{"type":23,"value":628},") is accepted by RSS 2023!",{"type":13,"tag":373,"props":630,"children":631},{},[632,637,639,646,647,654],{"type":13,"tag":153,"props":633,"children":634},{},[635],{"type":23,"value":636},"02/28/2023",{"type":23,"value":638}," üéâ Two papers (",{"type":13,"tag":32,"props":640,"children":643},{"href":641,"rel":642},"https://garment-tracking.robotflow.ai/",[36],[644],{"type":23,"value":645},"GarmentTracking",{"type":23,"value":95},{"type":13,"tag":32,"props":648,"children":651},{"href":649,"rel":650},"https://sites.google.com/view/vtaco/",[36],[652],{"type":23,"value":653},"VTaCo",{"type":23,"value":655},") are accepted by CVPR 2023!",{"title":5,"searchDepth":490,"depth":490,"links":657},[],"content:news.md","news.md",1744166902415]