[{"data":1,"prerenderedAt":583},["Reactive",2],{"content-query-1DxZ1vYQk5":3,"content-query-B6mqoO8PxR":440},{"_path":4,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":7,"description":5,"hideTitle":8,"disableFancyImage":8,"body":9,"_type":435,"_id":436,"_source":437,"_file":438,"_extension":439},"/","",false,"Home",true,{"type":10,"children":11,"toc":426},"root",[12,17,25,59,64,96,102,106,115,121,162,195,215,221,231,239,259,267,274,282,291,299,307,316,325,331,389,395,406,416],{"type":13,"tag":14,"props":15,"children":16},"element","IndexHeader",{},[],{"type":13,"tag":18,"props":19,"children":21},"h2",{"id":20},"Ô∏è-about-me",[22],{"type":23,"value":24},"text","ü¶∏üèª‚Äç‚ôÇÔ∏è About Me",{"type":13,"tag":26,"props":27,"children":28},"p",{},[29,31,40,42,49,51,57],{"type":23,"value":30},"I am a third-year Computer Science Ph.D. candidate at ",{"type":13,"tag":32,"props":33,"children":37},"a",{"href":34,"rel":35},"https://en.sjtu.edu.cn/",[36],"nofollow",[38],{"type":23,"value":39},"Shanghai Jiao Tong University",{"type":23,"value":41}," and a member of ",{"type":13,"tag":32,"props":43,"children":46},{"href":44,"rel":45},"https://www.mvig.org/",[36],[47],{"type":23,"value":48},"Machine Intelligence and Vision Group (MVIG)",{"type":23,"value":50}," under the supervision of ",{"type":13,"tag":32,"props":52,"children":54},{"href":44,"rel":53},[36],[55],{"type":23,"value":56},"Prof. Cewu Lu",{"type":23,"value":58},".",{"type":13,"tag":26,"props":60,"children":61},{},[62],{"type":23,"value":63},"I receive my bachelor degree from Shanghai Jiao Tong Universiy in 2021. My research interests lie in Robotics and 3D Vision. Previously, I have been working on deformable object perception and manipulation. Now I am particularly interested in tactile sensing and bimanual manipulation.",{"type":13,"tag":26,"props":65,"children":66},{},[67,69,76,78,85,87,94],{"type":23,"value":68},"In the past, I have interned at ",{"type":13,"tag":32,"props":70,"children":73},{"href":71,"rel":72},"https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/",[36],[74],{"type":23,"value":75},"Microsoft Research Asia",{"type":23,"value":77}," under the supervsion of ",{"type":13,"tag":32,"props":79,"children":82},{"href":80,"rel":81},"https://ancientmooner.github.io/",[36],[83],{"type":23,"value":84},"Han Hu",{"type":23,"value":86}," and ",{"type":13,"tag":32,"props":88,"children":91},{"href":89,"rel":90},"http://yue-cao.me/",[36],[92],{"type":23,"value":93},"Yue Cao",{"type":23,"value":95},". In my spare time, I enjoy watching movies and playing with robotsü§ñ.",{"type":13,"tag":18,"props":97,"children":99},{"id":98},"news",[100],{"type":23,"value":101},"üì∞ News",{"type":13,"tag":103,"props":104,"children":105},"ShortNews",{},[],{"type":13,"tag":26,"props":107,"children":108},{},[109],{"type":13,"tag":32,"props":110,"children":112},{"href":111},"/news/",[113],{"type":23,"value":114},"More news >>>",{"type":13,"tag":18,"props":116,"children":118},{"id":117},"experiences",[119],{"type":23,"value":120},"üè´ Experiences",{"type":13,"tag":122,"props":123,"children":125},"ExperienceRow",{"icon":124},"sjtu.png",[126],{"type":13,"tag":26,"props":127,"children":128},{},[129,134,138,140,143,145,150,152,157,160],{"type":13,"tag":130,"props":131,"children":132},"strong",{},[133],{"type":23,"value":39},{"type":13,"tag":135,"props":136,"children":137},"br",{},[],{"type":23,"value":139},"\nPh.D. Student / Master Student.",{"type":13,"tag":135,"props":141,"children":142},{},[],{"type":23,"value":144},"\nResearch assistant in ",{"type":13,"tag":32,"props":146,"children":148},{"href":44,"rel":147},[36],[149],{"type":23,"value":48},{"type":23,"value":151},", advised by ",{"type":13,"tag":32,"props":153,"children":155},{"href":44,"rel":154},[36],[156],{"type":23,"value":56},{"type":13,"tag":135,"props":158,"children":159},{},[],{"type":23,"value":161},"\nSep. 2021 - Present",{"type":13,"tag":122,"props":163,"children":165},{"icon":164},"microsoft.png",[166],{"type":13,"tag":26,"props":167,"children":168},{},[169,173,176,178,183,184,189,190,193],{"type":13,"tag":130,"props":170,"children":171},{},[172],{"type":23,"value":75},{"type":13,"tag":135,"props":174,"children":175},{},[],{"type":23,"value":177},"\nResearch Intern, advised by ",{"type":13,"tag":32,"props":179,"children":181},{"href":80,"rel":180},[36],[182],{"type":23,"value":84},{"type":23,"value":86},{"type":13,"tag":32,"props":185,"children":187},{"href":89,"rel":186},[36],[188],{"type":23,"value":93},{"type":23,"value":58},{"type":13,"tag":135,"props":191,"children":192},{},[],{"type":23,"value":194},"\nJul. 2019 - Mar. 2020",{"type":13,"tag":122,"props":196,"children":197},{"icon":124},[198],{"type":13,"tag":26,"props":199,"children":200},{},[201,205,208,210,213],{"type":13,"tag":130,"props":202,"children":203},{},[204],{"type":23,"value":39},{"type":13,"tag":135,"props":206,"children":207},{},[],{"type":23,"value":209},"\nBachelor of Engineering in Computer Science. ",{"type":13,"tag":135,"props":211,"children":212},{},[],{"type":23,"value":214},"\nSep 2016 - Jun. 2021",{"type":13,"tag":18,"props":216,"children":218},{"id":217},"selected-publications",[219],{"type":23,"value":220},"üìÑ Selected Publications",{"type":13,"tag":222,"props":223,"children":230},"PublicationRow",{":artifactLinks":224,":authors":225,":venue":226,"thumbnail":227,"title":228,"type":229},"{\"Website\":\"https://deform-pam.robotflow.ai/\",\"arXiv\":\"https://arxiv.org/abs/2410.11584\",\"Code\":\"https://github.com/xiaoxiaoxh/DeformPAM\"}","[\"Wendi Chen*\",\"Han Xue*\",\"Fangyuan Zhou\",\"Yuan Fang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"arXiv\",\"year\":2024,\"name\":\"Under Review\"}","deform-pam.gif","DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object Manipulation via Preference-based Action Alignment","conference",[],{"type":13,"tag":222,"props":232,"children":238},{":artifactLinks":233,":authors":234,":venue":235,"thumbnail":236,"title":237,"type":229},"{\"Proceeding\":\"https://openreview.net/pdf?id=ANJuNDFdvP\",\"arXiv\":\"https://arxiv.org/abs/2311.01267\",\"Code\":\"https://github.com/xiaoxiaoxh/UniFolding\",\"Website\":\"https://unifolding.robotflow.ai/\"}","[\"Han Xue*\",\"Yutong Li*\",\"Wenqiang Xu\",\"Huanyu Li\",\"Dongzhe Zheng\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"CoRL\",\"year\":2023,\"name\":\"7th Annual Conference on Robot Learning.\"}","unifolding.gif","UniFolding: Towards Sample-efficient, Scalable, and Generalizable Robotic Garment Folding",[],{"type":13,"tag":222,"props":240,"children":246},{":artifactLinks":241,":authors":242,":venue":243,"thumbnail":244,"title":245,"type":229},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf\"}","[\"Wenqiang Xu*\",\"Wenxin Du*\",\"Han Xue\",\"Yutong Li\",\"Ruolin Ye\",\"Yan-Feng Wang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICCV\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF International Conference on Computer Vision\"}","clothpose.png","ClothPose: A Real-world Benchmark for Visual Analysis of Garment Pose via An Indirect Recording Solution",[247],{"type":13,"tag":26,"props":248,"children":249},{},[250],{"type":13,"tag":251,"props":252,"children":256},"span",{"className":253},[254,255],"text-red-600","font-bold",[257],{"type":23,"value":258},"üî• Oral Presentation.",{"type":13,"tag":222,"props":260,"children":266},{":artifactLinks":261,":authors":262,":venue":263,"thumbnail":264,"title":265,"type":229},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_GarmentTracking_Category-Level_Garment_Pose_Tracking_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.13913.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/GarmentTracking\",\"Website\":\"https://garment-tracking.robotflow.ai/\"}","[\"Han Xue\",\"Wenqiang Xu\",\"Jieyi Zhang\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Ruolin Ye\",\"Cewu Lu\"]","{\"acronym\":\"CVPR\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\"}","garment-tracking.gif","GarmentTracking: Category-Level Garment Pose Tracking",[],{"type":13,"tag":222,"props":268,"children":273},{":artifactLinks":269,":authors":270,":venue":263,"thumbnail":271,"title":272,"type":229},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Visual-Tactile_Sensing_for_In-Hand_Object_Reconstruction_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.14498.pdf\",\"Code\":\"https://github.com/jeffsonyu/VTacO\",\"Website\":\"https://sites.google.com/view/vtaco/\"}","[\"Wenqiang Xu*\",\"Zhenjun Yu*\",\"Han Xue\",\"Ruolin Ye\",\"Siqiong Yao\",\"Cewu Lu (*Equal contribution)\"]","vtaco.png","Visual-Tactile Sensing for In-Hand Object Reconstruction",[],{"type":13,"tag":222,"props":275,"children":281},{":artifactLinks":276,":authors":277,":venue":278,"thumbnail":279,"title":280,"type":229},"{\"Proceeding\":\"https://www.roboticsproceedings.org/rss19/p087.pdf\",\"Code\":\"https://github.com/mvig-robotflow/pyrfuniverse\",\"Website\":\"https://sites.google.com/view/rfuniverse\"}","[\"Haoyuan Fu*\",\"Wenqiang Xu*\",\"Ruolin Ye*\",\"Han Xue\",\"Zhenjun Yu\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Jieyi Zhang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"RSS\",\"year\":2023,\"name\":\"Robotics: Science and Systems.\"}","rfuniverse.png","Demonstrating RFUniverse: A Multiphysics Simulation Platform for Embodied AI",[],{"type":13,"tag":222,"props":283,"children":290},{":artifactLinks":284,":authors":285,":venue":286,"thumbnail":287,"title":288,"type":289},"{\"arXiv\":\"https://arxiv.org/pdf/2105.03260\"}","[\"Liu Liu*\",\"Han Xue*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"TIP\",\"year\":2022,\"name\":\"IEEE Transactions on Image Processing.\"}","articulation_real.png","Toward Real-World Category-Level Articulation Pose Estimation","journal",[],{"type":13,"tag":222,"props":292,"children":298},{":artifactLinks":293,":authors":294,":venue":295,"thumbnail":296,"title":297,"type":229},"{\"Proceeding\":\"https://www.bmvc2021-virtualconference.com/assets/papers/0544.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2112.07334.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/OMAD\"}","[\"Han Xue*\",\"Liu Liu*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"BMVC\",\"year\":2021,\"name\":\"The 32nd British Machine Vision Conference.\"}","omad.png","OMAD: Object Model with Articulated Deformations for Pose Estimation and Retrieval",[],{"type":13,"tag":222,"props":300,"children":306},{":artifactLinks":301,":authors":302,":venue":303,"thumbnail":304,"title":305,"type":229},"{\"Proceeding\":\"https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660222.pdf\",\"arXiv\":\"https://arxiv.org/pdf/1912.11473.pdf\",\"Code\":\"https://github.com/justimyhxu/Dense-RepPoints\"}","[\"Ze Yang*\",\"Yinghao Xu*\",\"Han Xue*\",\"Zheng Zhang\",\"Raquel Urtasun\",\"Liwei Wang\",\"Stephen Lin\",\"Han Hu (*Equal contribution)\"]","{\"acronym\":\"ECCV\",\"year\":2020,\"name\":\"The European Conference on Computer Vision.\"}","dense_reppoints.jpg","Dense RepPoints: Representing Visual Objects with Dense Point Sets",[],{"type":13,"tag":222,"props":308,"children":315},{":artifactLinks":309,":authors":310,":venue":311,"thumbnail":312,"title":313,"type":229,":hideBottomBorder":314},"{\"Proceeding\":\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf\",\"Code\":\"https://github.com/driving-behavior/DBNet\",\"Website\":\"http://www.dbehavior.net/\"}","[\"Yiping Chen*\",\"Jingkang Wang*\",\"Jonathan Li\",\"Cewu Lu\",\"Zhipeng Luo\",\"Han Xue\",\"Cheng Wang (*Equal contribution)\"]","{\"acronym\":\"CVPR\",\"year\":2018,\"name\":\"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\"}","dbnet.jpg","Lidar-video driving dataset: Learning driving policies effectively","true",[],{"type":13,"tag":26,"props":317,"children":318},{},[319],{"type":13,"tag":32,"props":320,"children":322},{"href":321},"/publication/",[323],{"type":23,"value":324},"Full publication list >>>",{"type":13,"tag":18,"props":326,"children":328},{"id":327},"awards",[329],{"type":23,"value":330},"üèÜ Awards",{"type":13,"tag":332,"props":333,"children":334},"ul",{},[335,341,346,351,374,379,384],{"type":13,"tag":336,"props":337,"children":338},"li",{},[339],{"type":23,"value":340},"Outstanding Graduates in Shanghai (Top 3%) in 2021.",{"type":13,"tag":336,"props":342,"children":343},{},[344],{"type":23,"value":345},"Rongchang Technology Innovation Scholarship (Top 10 students in SJTU) in 2020.",{"type":13,"tag":336,"props":347,"children":348},{},[349],{"type":23,"value":350},"SenseTime Scholarship (Top 21 undergraduates in China) in 2020.",{"type":13,"tag":336,"props":352,"children":353},{},[354,356],{"type":23,"value":355},"National Scholarship (Top 3 students in CS Department).\n",{"type":13,"tag":332,"props":357,"children":358},{},[359,364,369],{"type":13,"tag":336,"props":360,"children":361},{},[362],{"type":23,"value":363},"National Scholarship in 2019",{"type":13,"tag":336,"props":365,"children":366},{},[367],{"type":23,"value":368},"National Scholarship in 2018",{"type":13,"tag":336,"props":370,"children":371},{},[372],{"type":23,"value":373},"National Scholarship in 2017",{"type":13,"tag":336,"props":375,"children":376},{},[377],{"type":23,"value":378},"Academic Excellence Scholarship (Class A) of SJTU (Top 1% in SJTU) in 2018.",{"type":13,"tag":336,"props":380,"children":381},{},[382],{"type":23,"value":383},"Meritorious Winner Prize of Mathematical Contest in Modeling in 2018.",{"type":13,"tag":336,"props":385,"children":386},{},[387],{"type":23,"value":388},"1st Prize in China Undergraduate Mathematical Contest in Modeling (Shanghai Division) in 2017.",{"type":13,"tag":18,"props":390,"children":392},{"id":391},"find-me",[393],{"type":23,"value":394},"üìß Find Me",{"type":13,"tag":396,"props":397,"children":400},"contact-item",{"icon":398,"url":399},"email","mailto:xiaoxiaoxh@sjtu.edu.cn",[401],{"type":13,"tag":26,"props":402,"children":403},{},[404],{"type":23,"value":405},"Email",{"type":13,"tag":396,"props":407,"children":410},{"icon":408,"url":409},"github","https://github.com/xiaoxiaoxh",[411],{"type":13,"tag":26,"props":412,"children":413},{},[414],{"type":23,"value":415},"GitHub",{"type":13,"tag":396,"props":417,"children":420},{"icon":418,"url":419},"twitter","https://twitter.com/HanXue012",[421],{"type":13,"tag":26,"props":422,"children":423},{},[424],{"type":23,"value":425},"Twitter",{"title":5,"searchDepth":427,"depth":427,"links":428},2,[429,430,431,432,433,434],{"id":20,"depth":427,"text":24},{"id":98,"depth":427,"text":101},{"id":117,"depth":427,"text":120},{"id":217,"depth":427,"text":220},{"id":327,"depth":427,"text":330},{"id":391,"depth":427,"text":394},"markdown","content:index.md","content","index.md","md",{"_path":441,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":442,"description":5,"leadingImage":443,"disableFancyImage":8,"body":444,"_type":435,"_id":581,"_source":437,"_file":582,"_extension":439},"/news","News","me-news-google.png",{"type":10,"children":445,"toc":579},[446,451],{"type":13,"tag":447,"props":448,"children":450},"MarkdownHeader",{"subtitle":449,"title":442},"üì¢ Latest: I'm a Ph.D. candidate now!",[],{"type":13,"tag":332,"props":452,"children":453},{},[454,481,491,508,534,552],{"type":13,"tag":336,"props":455,"children":456},{},[457,462,464,471,473,480],{"type":13,"tag":130,"props":458,"children":459},{},[460],{"type":23,"value":461},"11/05/2023",{"type":23,"value":463}," üéâ We have released the ",{"type":13,"tag":32,"props":465,"children":468},{"href":466,"rel":467},"https://github.com/xiaoxiaoxh/UniFolding",[36],[469],{"type":23,"value":470},"code",{"type":23,"value":472}," of our CoRL 2023 paper ",{"type":13,"tag":32,"props":474,"children":477},{"href":475,"rel":476},"https://unifolding.robotflow.ai/",[36],[478],{"type":23,"value":479},"UniFolding",{"type":23,"value":58},{"type":13,"tag":336,"props":482,"children":483},{},[484,489],{"type":13,"tag":130,"props":485,"children":486},{},[487],{"type":23,"value":488},"10/31/2023",{"type":23,"value":490}," üéì Passed my classes and research qualifications, I'm a Ph.D. candidate now!",{"type":13,"tag":336,"props":492,"children":493},{},[494,499,501,506],{"type":13,"tag":130,"props":495,"children":496},{},[497],{"type":23,"value":498},"08/31/2023",{"type":23,"value":500}," üéâ One paper (",{"type":13,"tag":32,"props":502,"children":504},{"href":475,"rel":503},[36],[505],{"type":23,"value":479},{"type":23,"value":507},") is accepted by CoRL 2023!",{"type":13,"tag":336,"props":509,"children":510},{},[511,516,518,525,527,532],{"type":13,"tag":130,"props":512,"children":513},{},[514],{"type":23,"value":515},"07/14/2023",{"type":23,"value":517}," üî• One paper (",{"type":13,"tag":32,"props":519,"children":522},{"href":520,"rel":521},"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf",[36],[523],{"type":23,"value":524},"ClothPose",{"type":23,"value":526},") is accepted by ICCV 2023 as an ",{"type":13,"tag":130,"props":528,"children":529},{},[530],{"type":23,"value":531},"oral presentation",{"type":23,"value":533},"!",{"type":13,"tag":336,"props":535,"children":536},{},[537,542,543,550],{"type":13,"tag":130,"props":538,"children":539},{},[540],{"type":23,"value":541},"05/13/2023",{"type":23,"value":500},{"type":13,"tag":32,"props":544,"children":547},{"href":545,"rel":546},"https://sites.google.com/view/rfuniverse",[36],[548],{"type":23,"value":549},"RFUniverse",{"type":23,"value":551},") is accepted by RSS 2023!",{"type":13,"tag":336,"props":553,"children":554},{},[555,560,562,569,570,577],{"type":13,"tag":130,"props":556,"children":557},{},[558],{"type":23,"value":559},"02/28/2023",{"type":23,"value":561}," üéâ Two papers (",{"type":13,"tag":32,"props":563,"children":566},{"href":564,"rel":565},"https://garment-tracking.robotflow.ai/",[36],[567],{"type":23,"value":568},"GarmentTracking",{"type":23,"value":86},{"type":13,"tag":32,"props":571,"children":574},{"href":572,"rel":573},"https://sites.google.com/view/vtaco/",[36],[575],{"type":23,"value":576},"VTaCo",{"type":23,"value":578},") are accepted by CVPR 2023!",{"title":5,"searchDepth":427,"depth":427,"links":580},[],"content:news.md","news.md",1729510234960]