[{"data":1,"prerenderedAt":741},["Reactive",2],{"content-query-1DxZ1vYQk5":3,"content-query-B6mqoO8PxR":559},{"_path":4,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":7,"description":5,"hideTitle":8,"disableFancyImage":8,"body":9,"_type":554,"_id":555,"_source":556,"_file":557,"_extension":558},"/","",false,"Home",true,{"type":10,"children":11,"toc":544},"root",[12,17,25,68,73,114,119,125,129,138,144,185,219,244,250,280,305,313,330,338,345,353,362,370,378,387,396,402,463,469,507,513,524,534],{"type":13,"tag":14,"props":15,"children":16},"element","IndexHeader",{},[],{"type":13,"tag":18,"props":19,"children":21},"h2",{"id":20},"Ô∏è-about-me",[22],{"type":23,"value":24},"text","ü¶∏üèª‚Äç‚ôÇÔ∏è About Me",{"type":13,"tag":26,"props":27,"children":28},"p",{},[29,31,40,42,49,51,57,59,66],{"type":23,"value":30},"I am a fourth-year Computer Science Ph.D. candidate at ",{"type":13,"tag":32,"props":33,"children":37},"a",{"href":34,"rel":35},"https://en.sjtu.edu.cn/",[36],"nofollow",[38],{"type":23,"value":39},"Shanghai Jiao Tong University",{"type":23,"value":41}," and a member of ",{"type":13,"tag":32,"props":43,"children":46},{"href":44,"rel":45},"https://www.mvig.org/",[36],[47],{"type":23,"value":48},"Machine Intelligence and Vision Group (MVIG)",{"type":23,"value":50}," under the supervision of ",{"type":13,"tag":32,"props":52,"children":54},{"href":44,"rel":53},[36],[55],{"type":23,"value":56},"Prof. Cewu Lu",{"type":23,"value":58}," (Âç¢Á≠ñÂêæ). I am also a member of ",{"type":13,"tag":32,"props":60,"children":63},{"href":61,"rel":62},"https://ai.sjtu.edu.cn/info/announcements/204",[36],[64],{"type":23,"value":65},"Wu Wen Jun Honorary Doctoral Program",{"type":23,"value":67}," (Âê¥Êñá‰øäËç£Ë™âÂçöÂ£´Áè≠).",{"type":13,"tag":26,"props":69,"children":70},{},[71],{"type":23,"value":72},"I receive my bachelor degree from Shanghai Jiao Tong Universiy in 2021. My research interests lie in Robotics and Computer Vision. Previously, I have been working on deformable object perception and manipulation. Now I am particularly interested in imitation learning with tactile/force sensing and low-cost data collection system.",{"type":13,"tag":26,"props":74,"children":75},{},[76,78,85,87,94,96,103,105,112],{"type":23,"value":77},"In the past, I have interned at ",{"type":13,"tag":32,"props":79,"children":82},{"href":80,"rel":81},"https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/",[36],[83],{"type":23,"value":84},"Microsoft Research Asia",{"type":23,"value":86}," under the supervsion of ",{"type":13,"tag":32,"props":88,"children":91},{"href":89,"rel":90},"https://ancientmooner.github.io/",[36],[92],{"type":23,"value":93},"Han Hu",{"type":23,"value":95}," and ",{"type":13,"tag":32,"props":97,"children":100},{"href":98,"rel":99},"http://yue-cao.me/",[36],[101],{"type":23,"value":102},"Yue Cao",{"type":23,"value":104},". I also spent time with Prof. ",{"type":13,"tag":32,"props":106,"children":109},{"href":107,"rel":108},"http://hxu.rocks/index.html",[36],[110],{"type":23,"value":111},"Huazhe Xu",{"type":23,"value":113}," at Tsinghua University on my projects. In my spare time, I enjoy watching movies and playing with robotsü§ñ.",{"type":13,"tag":26,"props":115,"children":116},{},[117],{"type":23,"value":118},"I'm on the job market now!",{"type":13,"tag":18,"props":120,"children":122},{"id":121},"news",[123],{"type":23,"value":124},"üì∞ News",{"type":13,"tag":126,"props":127,"children":128},"ShortNews",{},[],{"type":13,"tag":26,"props":130,"children":131},{},[132],{"type":13,"tag":32,"props":133,"children":135},{"href":134},"/news/",[136],{"type":23,"value":137},"More news >>>",{"type":13,"tag":18,"props":139,"children":141},{"id":140},"experiences",[142],{"type":23,"value":143},"üè´ Experiences",{"type":13,"tag":145,"props":146,"children":148},"ExperienceRow",{"icon":147},"sjtu.png",[149],{"type":13,"tag":26,"props":150,"children":151},{},[152,157,161,163,166,168,173,175,180,183],{"type":13,"tag":153,"props":154,"children":155},"strong",{},[156],{"type":23,"value":39},{"type":13,"tag":158,"props":159,"children":160},"br",{},[],{"type":23,"value":162},"\nPh.D. Student ",{"type":13,"tag":158,"props":164,"children":165},{},[],{"type":23,"value":167},"\nResearch assistant in ",{"type":13,"tag":32,"props":169,"children":171},{"href":44,"rel":170},[36],[172],{"type":23,"value":48},{"type":23,"value":174},", advised by ",{"type":13,"tag":32,"props":176,"children":178},{"href":44,"rel":177},[36],[179],{"type":23,"value":56},{"type":13,"tag":158,"props":181,"children":182},{},[],{"type":23,"value":184},"\nSep. 2021 - Present",{"type":13,"tag":145,"props":186,"children":188},{"icon":187},"microsoft.png",[189],{"type":13,"tag":26,"props":190,"children":191},{},[192,196,199,201,206,207,212,214,217],{"type":13,"tag":153,"props":193,"children":194},{},[195],{"type":23,"value":84},{"type":13,"tag":158,"props":197,"children":198},{},[],{"type":23,"value":200},"\nResearch Intern, advised by ",{"type":13,"tag":32,"props":202,"children":204},{"href":89,"rel":203},[36],[205],{"type":23,"value":93},{"type":23,"value":95},{"type":13,"tag":32,"props":208,"children":210},{"href":98,"rel":209},[36],[211],{"type":23,"value":102},{"type":23,"value":213},".",{"type":13,"tag":158,"props":215,"children":216},{},[],{"type":23,"value":218},"\nJul. 2019 - Mar. 2020",{"type":13,"tag":145,"props":220,"children":221},{"icon":147},[222],{"type":13,"tag":26,"props":223,"children":224},{},[225,229,232,234,237,239,242],{"type":13,"tag":153,"props":226,"children":227},{},[228],{"type":23,"value":39},{"type":13,"tag":158,"props":230,"children":231},{},[],{"type":23,"value":233},"\nBachelor of Engineering in Computer Science. ",{"type":13,"tag":158,"props":235,"children":236},{},[],{"type":23,"value":238},"\nGPA 4.04/4.3, Rank 3/150 (Top 2%)",{"type":13,"tag":158,"props":240,"children":241},{},[],{"type":23,"value":243},"\nSep 2016 - Jun. 2021",{"type":13,"tag":18,"props":245,"children":247},{"id":246},"selected-publications",[248],{"type":23,"value":249},"üìÑ Selected Publications",{"type":13,"tag":251,"props":252,"children":259},"PublicationRow",{":artifactLinks":253,":authors":254,":venue":255,"thumbnail":256,"title":257,"type":258},"{\"Website\":\"https://reactive-diffusion-policy.github.io/\",\"arXiv\":\"https://arxiv.org/abs/2503.02881\",\"Code\":\"https://github.com/xiaoxiaoxh/reactive_diffusion_policy\"}","[\"Han Xue*\",\"Jieji Ren*\",\"Wendi Chen*\",\"Gu Zhang\",\"Yuan Fang\",\"Guoying Gu\",\"Huazhe Xu‚Ä†\",\"Cewu Lu‚Ä† (‚Ä†Equal advising)\"]","{\"acronym\":\"RSS\",\"year\":2025,\"name\":\"Robotics: Science and Systems (RSS)\"}","rdp.gif","Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation","conference",[260],{"type":13,"tag":26,"props":261,"children":262},{},[263],{"type":13,"tag":264,"props":265,"children":269},"span",{"className":266},[267,268],"text-red-600","font-bold",[270,272,279],{"type":23,"value":271},"üî• Best Paper Award ",{"type":13,"tag":32,"props":273,"children":276},{"href":274,"rel":275},"https://sites.google.com/view/icra-2025-beyond-pick-place/home",[36],[277],{"type":23,"value":278},"@ Beyond P&P Workshop in ICRA 2025",{"type":23,"value":213},{"type":13,"tag":251,"props":281,"children":287},{":artifactLinks":282,":authors":283,":venue":284,"thumbnail":285,"title":286,"type":258},"{\"Website\":\"https://deform-pam.robotflow.ai/\",\"arXiv\":\"https://arxiv.org/abs/2410.11584\",\"Code\":\"https://github.com/xiaoxiaoxh/DeformPAM\"}","[\"Wendi Chen*\",\"Han Xue*\",\"Fangyuan Zhou\",\"Yuan Fang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICRA\",\"year\":2025,\"name\":\"IEEE International Conference on Robotics and Automation (ICRA)\"}","deform-pam.gif","DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object Manipulation via Preference-based Action Alignment",[288],{"type":13,"tag":26,"props":289,"children":290},{},[291],{"type":13,"tag":264,"props":292,"children":294},{"className":293},[267,268],[295,297,304],{"type":23,"value":296},"üî• Best Paper Finalist ",{"type":13,"tag":32,"props":298,"children":301},{"href":299,"rel":300},"https://deformable-workshop.github.io/icra2025/",[36],[302],{"type":23,"value":303},"@ RMDO Workshop in ICRA 2025",{"type":23,"value":213},{"type":13,"tag":251,"props":306,"children":312},{":artifactLinks":307,":authors":308,":venue":309,"thumbnail":310,"title":311,"type":258},"{\"Proceeding\":\"https://openreview.net/pdf?id=ANJuNDFdvP\",\"arXiv\":\"https://arxiv.org/abs/2311.01267\",\"Code\":\"https://github.com/xiaoxiaoxh/UniFolding\",\"Website\":\"https://unifolding.robotflow.ai/\"}","[\"Han Xue*\",\"Yutong Li*\",\"Wenqiang Xu\",\"Huanyu Li\",\"Dongzhe Zheng\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"CoRL\",\"year\":2023,\"name\":\"7th Annual Conference on Robot Learning.\"}","unifolding.gif","UniFolding: Towards Sample-efficient, Scalable, and Generalizable Robotic Garment Folding",[],{"type":13,"tag":251,"props":314,"children":320},{":artifactLinks":315,":authors":316,":venue":317,"thumbnail":318,"title":319,"type":258},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf\"}","[\"Wenqiang Xu*\",\"Wenxin Du*\",\"Han Xue\",\"Yutong Li\",\"Ruolin Ye\",\"Yan-Feng Wang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICCV\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF International Conference on Computer Vision\"}","clothpose.png","ClothPose: A Real-world Benchmark for Visual Analysis of Garment Pose via An Indirect Recording Solution",[321],{"type":13,"tag":26,"props":322,"children":323},{},[324],{"type":13,"tag":264,"props":325,"children":327},{"className":326},[267,268],[328],{"type":23,"value":329},"üî• Oral Presentation.",{"type":13,"tag":251,"props":331,"children":337},{":artifactLinks":332,":authors":333,":venue":334,"thumbnail":335,"title":336,"type":258},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_GarmentTracking_Category-Level_Garment_Pose_Tracking_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.13913.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/GarmentTracking\",\"Website\":\"https://garment-tracking.robotflow.ai/\"}","[\"Han Xue\",\"Wenqiang Xu\",\"Jieyi Zhang\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Ruolin Ye\",\"Cewu Lu\"]","{\"acronym\":\"CVPR\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\"}","garment-tracking.gif","GarmentTracking: Category-Level Garment Pose Tracking",[],{"type":13,"tag":251,"props":339,"children":344},{":artifactLinks":340,":authors":341,":venue":334,"thumbnail":342,"title":343,"type":258},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Visual-Tactile_Sensing_for_In-Hand_Object_Reconstruction_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.14498.pdf\",\"Code\":\"https://github.com/jeffsonyu/VTacO\",\"Website\":\"https://sites.google.com/view/vtaco/\"}","[\"Wenqiang Xu*\",\"Zhenjun Yu*\",\"Han Xue\",\"Ruolin Ye\",\"Siqiong Yao\",\"Cewu Lu (*Equal contribution)\"]","vtaco.png","Visual-Tactile Sensing for In-Hand Object Reconstruction",[],{"type":13,"tag":251,"props":346,"children":352},{":artifactLinks":347,":authors":348,":venue":349,"thumbnail":350,"title":351,"type":258},"{\"Proceeding\":\"https://www.roboticsproceedings.org/rss19/p087.pdf\",\"Code\":\"https://github.com/mvig-robotflow/pyrfuniverse\",\"Website\":\"https://sites.google.com/view/rfuniverse\"}","[\"Haoyuan Fu*\",\"Wenqiang Xu*\",\"Ruolin Ye*\",\"Han Xue\",\"Zhenjun Yu\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Jieyi Zhang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"RSS\",\"year\":2023,\"name\":\"Robotics: Science and Systems.\"}","rfuniverse.png","Demonstrating RFUniverse: A Multiphysics Simulation Platform for Embodied AI",[],{"type":13,"tag":251,"props":354,"children":361},{":artifactLinks":355,":authors":356,":venue":357,"thumbnail":358,"title":359,"type":360},"{\"arXiv\":\"https://arxiv.org/pdf/2105.03260\"}","[\"Liu Liu*\",\"Han Xue*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"TIP\",\"year\":2022,\"name\":\"IEEE Transactions on Image Processing.\"}","articulation_real.png","Toward Real-World Category-Level Articulation Pose Estimation","journal",[],{"type":13,"tag":251,"props":363,"children":369},{":artifactLinks":364,":authors":365,":venue":366,"thumbnail":367,"title":368,"type":258},"{\"Proceeding\":\"https://www.bmvc2021-virtualconference.com/assets/papers/0544.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2112.07334.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/OMAD\"}","[\"Han Xue*\",\"Liu Liu*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"BMVC\",\"year\":2021,\"name\":\"The 32nd British Machine Vision Conference.\"}","omad.png","OMAD: Object Model with Articulated Deformations for Pose Estimation and Retrieval",[],{"type":13,"tag":251,"props":371,"children":377},{":artifactLinks":372,":authors":373,":venue":374,"thumbnail":375,"title":376,"type":258},"{\"Proceeding\":\"https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660222.pdf\",\"arXiv\":\"https://arxiv.org/pdf/1912.11473.pdf\",\"Code\":\"https://github.com/justimyhxu/Dense-RepPoints\"}","[\"Ze Yang*\",\"Yinghao Xu*\",\"Han Xue*\",\"Zheng Zhang\",\"Raquel Urtasun\",\"Liwei Wang\",\"Stephen Lin\",\"Han Hu (*Equal contribution)\"]","{\"acronym\":\"ECCV\",\"year\":2020,\"name\":\"The European Conference on Computer Vision.\"}","dense_reppoints.jpg","Dense RepPoints: Representing Visual Objects with Dense Point Sets",[],{"type":13,"tag":251,"props":379,"children":386},{":artifactLinks":380,":authors":381,":venue":382,"thumbnail":383,"title":384,"type":258,":hideBottomBorder":385},"{\"Proceeding\":\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf\",\"Code\":\"https://github.com/driving-behavior/DBNet\",\"Website\":\"http://www.dbehavior.net/\"}","[\"Yiping Chen*\",\"Jingkang Wang*\",\"Jonathan Li\",\"Cewu Lu\",\"Zhipeng Luo\",\"Han Xue\",\"Cheng Wang (*Equal contribution)\"]","{\"acronym\":\"CVPR\",\"year\":2018,\"name\":\"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\"}","dbnet.jpg","Lidar-video driving dataset: Learning driving policies effectively","true",[],{"type":13,"tag":26,"props":388,"children":389},{},[390],{"type":13,"tag":32,"props":391,"children":393},{"href":392},"/publication/",[394],{"type":23,"value":395},"Full publication list >>>",{"type":13,"tag":18,"props":397,"children":399},{"id":398},"talks",[400],{"type":23,"value":401},"‚ú® Talks",{"type":13,"tag":403,"props":404,"children":405},"ul",{},[406,429,450],{"type":13,"tag":407,"props":408,"children":409},"li",{},[410,412,419,421,428],{"type":23,"value":411},"[Apr. 2025] Invited talk @ ",{"type":13,"tag":32,"props":413,"children":416},{"href":414,"rel":415},"https://www.techbeat.net/",[36],[417],{"type":23,"value":418},"TechBeat",{"type":23,"value":420}," (Â∞ÜÈó®ÂàõÊäï) on \"Robotic Imitation Learning with Vision-Tactile/Force Sensing\". The link of the recorded video is ",{"type":13,"tag":32,"props":422,"children":425},{"href":423,"rel":424},"https://www.techbeat.net/talk-info?id=963",[36],[426],{"type":23,"value":427},"here",{"type":23,"value":213},{"type":13,"tag":407,"props":430,"children":431},{},[432,434,441,443,449],{"type":23,"value":433},"[Dec. 2024] Invited talk @ ",{"type":13,"tag":32,"props":435,"children":438},{"href":436,"rel":437},"https://www.roscon.cn/2024/index.html",[36],[439],{"type":23,"value":440},"ROSCon China 2024",{"type":23,"value":442}," workshop on \"Development Trends and Challenges in Embodied AI\". The link of the recorded video is ",{"type":13,"tag":32,"props":444,"children":447},{"href":445,"rel":446},"https://www.bilibili.com/video/BV1Z3cteAEnD/?spm_id_from=333.337.search-card.all.click&vd_source=8e062051896958b92b4759e0f4753657",[36],[448],{"type":23,"value":427},{"type":23,"value":213},{"type":13,"tag":407,"props":451,"children":452},{},[453,455,461],{"type":23,"value":454},"[Oct. 2024] Invited talk @ ",{"type":13,"tag":32,"props":456,"children":458},{"href":107,"rel":457},[36],[459],{"type":23,"value":460},"TEA lab",{"type":23,"value":462}," in Tsinghua University, IIIS on \"Efficient Learning for Long-horizon Deformable Object Manipulation\"",{"type":13,"tag":18,"props":464,"children":466},{"id":465},"awards",[467],{"type":23,"value":468},"üèÜ Awards",{"type":13,"tag":403,"props":470,"children":471},{},[472,477,482,487,492,497,502],{"type":13,"tag":407,"props":473,"children":474},{},[475],{"type":23,"value":476},"Outstanding Graduates in Shanghai (Top 3%) [‰∏äÊµ∑Â∏Ç‰ºòÁßÄÊØï‰∏öÁîü] in 2021.",{"type":13,"tag":407,"props":478,"children":479},{},[480],{"type":23,"value":481},"Rongchang Technology Innovation Scholarship (Top 10 students in SJTU) [Ëç£Êò∂ÁßëÊäÄÂàõÊñ∞Â•ñÂ≠¶Èáë] in 2020.",{"type":13,"tag":407,"props":483,"children":484},{},[485],{"type":23,"value":486},"SenseTime Scholarship (Top 21 undergraduates in China) [ÂïÜÊ±§Â•ñÂ≠¶Èáë] in 2020.",{"type":13,"tag":407,"props":488,"children":489},{},[490],{"type":23,"value":491},"National Scholarship (Top 3 students in CS Department) in 2017, 2018 and 2019 (three consecutive years) [ÂõΩÂÆ∂Â•ñÂ≠¶ÈáëÔºàËøûÁª≠‰∏âÂπ¥Ôºâ].",{"type":13,"tag":407,"props":493,"children":494},{},[495],{"type":23,"value":496},"Academic Excellence Scholarship (Class A) of SJTU (Top 1% in SJTU) in 2018.",{"type":13,"tag":407,"props":498,"children":499},{},[500],{"type":23,"value":501},"Meritorious Winner Prize of Mathematical Contest in Modeling in 2018.",{"type":13,"tag":407,"props":503,"children":504},{},[505],{"type":23,"value":506},"1st Prize in China Undergraduate Mathematical Contest in Modeling (Shanghai Division) in 2017.",{"type":13,"tag":18,"props":508,"children":510},{"id":509},"find-me",[511],{"type":23,"value":512},"üìß Find Me",{"type":13,"tag":514,"props":515,"children":518},"contact-item",{"icon":516,"url":517},"email","mailto:xiaoxiaoxh@sjtu.edu.cn",[519],{"type":13,"tag":26,"props":520,"children":521},{},[522],{"type":23,"value":523},"Email",{"type":13,"tag":514,"props":525,"children":528},{"icon":526,"url":527},"github","https://github.com/xiaoxiaoxh",[529],{"type":13,"tag":26,"props":530,"children":531},{},[532],{"type":23,"value":533},"GitHub",{"type":13,"tag":514,"props":535,"children":538},{"icon":536,"url":537},"twitter","https://twitter.com/HanXue012",[539],{"type":13,"tag":26,"props":540,"children":541},{},[542],{"type":23,"value":543},"Twitter",{"title":5,"searchDepth":545,"depth":545,"links":546},2,[547,548,549,550,551,552,553],{"id":20,"depth":545,"text":24},{"id":121,"depth":545,"text":124},{"id":140,"depth":545,"text":143},{"id":246,"depth":545,"text":249},{"id":398,"depth":545,"text":401},{"id":465,"depth":545,"text":468},{"id":509,"depth":545,"text":512},"markdown","content:index.md","content","index.md","md",{"_path":560,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":561,"description":5,"leadingImage":562,"disableFancyImage":8,"body":563,"_type":554,"_id":739,"_source":556,"_file":740,"_extension":558},"/news","News","me-news-google.png",{"type":10,"children":564,"toc":737},[565,570,624,645],{"type":13,"tag":566,"props":567,"children":569},"MarkdownHeader",{"subtitle":568,"title":561},"üì¢ Latest: One paper (DeformPAM) is accepted by ICRA 2025!",[],{"type":13,"tag":403,"props":571,"children":572},{},[573,607],{"type":13,"tag":407,"props":574,"children":575},{},[576,581,583,590,592,597,599,605],{"type":13,"tag":153,"props":577,"children":578},{},[579],{"type":23,"value":580},"05/23/2025",{"type":23,"value":582}," üî• ",{"type":13,"tag":32,"props":584,"children":587},{"href":585,"rel":586},"https://reactive-diffusion-policy.github.io/",[36],[588],{"type":23,"value":589},"Reactive Diffusion Policy",{"type":23,"value":591}," is selected as the ",{"type":13,"tag":153,"props":593,"children":594},{},[595],{"type":23,"value":596},"Best Paper",{"type":23,"value":598}," @ ",{"type":13,"tag":32,"props":600,"children":602},{"href":274,"rel":601},[36],[603],{"type":23,"value":604},"Beyond Pick and Place workshop",{"type":23,"value":606}," @ ICRA 2025!",{"type":13,"tag":407,"props":608,"children":609},{},[610,615,617,622],{"type":13,"tag":153,"props":611,"children":612},{},[613],{"type":23,"value":614},"04/11/2025",{"type":23,"value":616}," üéâ One paper (",{"type":13,"tag":32,"props":618,"children":620},{"href":585,"rel":619},[36],[621],{"type":23,"value":589},{"type":23,"value":623},") is accepted by RSS 2025!",{"type":13,"tag":403,"props":625,"children":626},{},[627],{"type":13,"tag":407,"props":628,"children":629},{},[630,635,636,643],{"type":13,"tag":153,"props":631,"children":632},{},[633],{"type":23,"value":634},"01/29/2025",{"type":23,"value":616},{"type":13,"tag":32,"props":637,"children":640},{"href":638,"rel":639},"https://deform-pam.robotflow.ai/",[36],[641],{"type":23,"value":642},"DeformPAM",{"type":23,"value":644},") is accepted by ICRA 2025!",{"type":13,"tag":403,"props":646,"children":647},{},[648,666,692,710],{"type":13,"tag":407,"props":649,"children":650},{},[651,656,657,664],{"type":13,"tag":153,"props":652,"children":653},{},[654],{"type":23,"value":655},"08/31/2023",{"type":23,"value":616},{"type":13,"tag":32,"props":658,"children":661},{"href":659,"rel":660},"https://unifolding.robotflow.ai/",[36],[662],{"type":23,"value":663},"UniFolding",{"type":23,"value":665},") is accepted by CoRL 2023!",{"type":13,"tag":407,"props":667,"children":668},{},[669,674,676,683,685,690],{"type":13,"tag":153,"props":670,"children":671},{},[672],{"type":23,"value":673},"07/14/2023",{"type":23,"value":675}," üî• One paper (",{"type":13,"tag":32,"props":677,"children":680},{"href":678,"rel":679},"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf",[36],[681],{"type":23,"value":682},"ClothPose",{"type":23,"value":684},") is accepted by ICCV 2023 as an ",{"type":13,"tag":153,"props":686,"children":687},{},[688],{"type":23,"value":689},"oral presentation",{"type":23,"value":691},"!",{"type":13,"tag":407,"props":693,"children":694},{},[695,700,701,708],{"type":13,"tag":153,"props":696,"children":697},{},[698],{"type":23,"value":699},"05/13/2023",{"type":23,"value":616},{"type":13,"tag":32,"props":702,"children":705},{"href":703,"rel":704},"https://sites.google.com/view/rfuniverse",[36],[706],{"type":23,"value":707},"RFUniverse",{"type":23,"value":709},") is accepted by RSS 2023!",{"type":13,"tag":407,"props":711,"children":712},{},[713,718,720,727,728,735],{"type":13,"tag":153,"props":714,"children":715},{},[716],{"type":23,"value":717},"02/28/2023",{"type":23,"value":719}," üéâ Two papers (",{"type":13,"tag":32,"props":721,"children":724},{"href":722,"rel":723},"https://garment-tracking.robotflow.ai/",[36],[725],{"type":23,"value":726},"GarmentTracking",{"type":23,"value":95},{"type":13,"tag":32,"props":729,"children":732},{"href":730,"rel":731},"https://sites.google.com/view/vtaco/",[36],[733],{"type":23,"value":734},"VTaCo",{"type":23,"value":736},") are accepted by CVPR 2023!",{"title":5,"searchDepth":545,"depth":545,"links":738},[],"content:news.md","news.md",1748495865980]