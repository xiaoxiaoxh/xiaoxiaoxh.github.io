[{"data":1,"prerenderedAt":557},["Reactive",2],{"content-query-1DxZ1vYQk5":3,"content-query-B6mqoO8PxR":432},{"_path":4,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":7,"description":5,"hideTitle":8,"disableFancyImage":8,"body":9,"_type":427,"_id":428,"_source":429,"_file":430,"_extension":431},"/","",false,"Home",true,{"type":10,"children":11,"toc":418},"root",[12,17,25,59,64,96,102,106,115,121,162,195,215,221,231,251,259,266,274,283,291,299,308,317,323,381,387,398,408],{"type":13,"tag":14,"props":15,"children":16},"element","IndexHeader",{},[],{"type":13,"tag":18,"props":19,"children":21},"h2",{"id":20},"Ô∏è-about-me",[22],{"type":23,"value":24},"text","ü¶∏üèª‚Äç‚ôÇÔ∏è About Me",{"type":13,"tag":26,"props":27,"children":28},"p",{},[29,31,40,42,49,51,57],{"type":23,"value":30},"I am a first-year Computer Science Ph.D. candidate at ",{"type":13,"tag":32,"props":33,"children":37},"a",{"href":34,"rel":35},"https://en.sjtu.edu.cn/",[36],"nofollow",[38],{"type":23,"value":39},"Shanghai Jiao Tong University",{"type":23,"value":41}," and a member of ",{"type":13,"tag":32,"props":43,"children":46},{"href":44,"rel":45},"https://www.mvig.org/",[36],[47],{"type":23,"value":48},"Machine Intelligence and Vision Group (MVIG)",{"type":23,"value":50}," under the supervision of ",{"type":13,"tag":32,"props":52,"children":54},{"href":44,"rel":53},[36],[55],{"type":23,"value":56},"Prof. Cewu Lu",{"type":23,"value":58},".",{"type":13,"tag":26,"props":60,"children":61},{},[62],{"type":23,"value":63},"I receive my bachelor degree from Shanghai Jiao Tong Universiy in 2021. My research interests lie in Robotics and 3D Vision. Previously, I have been working on deformable object perception and manipulation. Now I am particularly interested in tactile sensing and bimanual manipulation.",{"type":13,"tag":26,"props":65,"children":66},{},[67,69,76,78,85,87,94],{"type":23,"value":68},"In the past, I have interned at ",{"type":13,"tag":32,"props":70,"children":73},{"href":71,"rel":72},"https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/",[36],[74],{"type":23,"value":75},"Microsoft Research Asia",{"type":23,"value":77}," under the supervsion of ",{"type":13,"tag":32,"props":79,"children":82},{"href":80,"rel":81},"https://ancientmooner.github.io/",[36],[83],{"type":23,"value":84},"Han Hu",{"type":23,"value":86}," and ",{"type":13,"tag":32,"props":88,"children":91},{"href":89,"rel":90},"http://yue-cao.me/",[36],[92],{"type":23,"value":93},"Yue Cao",{"type":23,"value":95},". In my spare time, I enjoy watching movies and playing with robotsü§ñ.",{"type":13,"tag":18,"props":97,"children":99},{"id":98},"news",[100],{"type":23,"value":101},"üì∞ News",{"type":13,"tag":103,"props":104,"children":105},"ShortNews",{},[],{"type":13,"tag":26,"props":107,"children":108},{},[109],{"type":13,"tag":32,"props":110,"children":112},{"href":111},"/news/",[113],{"type":23,"value":114},"More news >>>",{"type":13,"tag":18,"props":116,"children":118},{"id":117},"experiences",[119],{"type":23,"value":120},"üè´ Experiences",{"type":13,"tag":122,"props":123,"children":125},"ExperienceRow",{"icon":124},"sjtu.png",[126],{"type":13,"tag":26,"props":127,"children":128},{},[129,134,138,140,143,145,150,152,157,160],{"type":13,"tag":130,"props":131,"children":132},"strong",{},[133],{"type":23,"value":39},{"type":13,"tag":135,"props":136,"children":137},"br",{},[],{"type":23,"value":139},"\nPh.D. Student / Master Student.",{"type":13,"tag":135,"props":141,"children":142},{},[],{"type":23,"value":144},"\nResearch assistant in ",{"type":13,"tag":32,"props":146,"children":148},{"href":44,"rel":147},[36],[149],{"type":23,"value":48},{"type":23,"value":151},", advised by ",{"type":13,"tag":32,"props":153,"children":155},{"href":44,"rel":154},[36],[156],{"type":23,"value":56},{"type":13,"tag":135,"props":158,"children":159},{},[],{"type":23,"value":161},"\nSep. 2021 - Present",{"type":13,"tag":122,"props":163,"children":165},{"icon":164},"microsoft.png",[166],{"type":13,"tag":26,"props":167,"children":168},{},[169,173,176,178,183,184,189,190,193],{"type":13,"tag":130,"props":170,"children":171},{},[172],{"type":23,"value":75},{"type":13,"tag":135,"props":174,"children":175},{},[],{"type":23,"value":177},"\nResearch Intern, advised by ",{"type":13,"tag":32,"props":179,"children":181},{"href":80,"rel":180},[36],[182],{"type":23,"value":84},{"type":23,"value":86},{"type":13,"tag":32,"props":185,"children":187},{"href":89,"rel":186},[36],[188],{"type":23,"value":93},{"type":23,"value":58},{"type":13,"tag":135,"props":191,"children":192},{},[],{"type":23,"value":194},"\nJul. 2019 - Mar. 2020",{"type":13,"tag":122,"props":196,"children":197},{"icon":124},[198],{"type":13,"tag":26,"props":199,"children":200},{},[201,205,208,210,213],{"type":13,"tag":130,"props":202,"children":203},{},[204],{"type":23,"value":39},{"type":13,"tag":135,"props":206,"children":207},{},[],{"type":23,"value":209},"\nBachelor of Engineering in Computer Science. ",{"type":13,"tag":135,"props":211,"children":212},{},[],{"type":23,"value":214},"\nSep 2016 - Jun. 2021",{"type":13,"tag":18,"props":216,"children":218},{"id":217},"selected-publications",[219],{"type":23,"value":220},"üìÑ Selected Publications",{"type":13,"tag":222,"props":223,"children":230},"PublicationRow",{":artifactLinks":224,":authors":225,":venue":226,"thumbnail":227,"title":228,"type":229},"{\"Proceeding\":\"https://openreview.net/pdf?id=ANJuNDFdvP\",\"arXiv\":\"https://arxiv.org/abs/2311.01267\",\"Code\":\"https://github.com/xiaoxiaoxh/UniFolding\",\"Website\":\"https://unifolding.robotflow.ai/\"}","[\"Han Xue*\",\"Yutong Li*\",\"Wenqiang Xu\",\"Huanyu Li\",\"Dongzhe Zheng\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"CoRL\",\"year\":2023,\"name\":\"7th Annual Conference on Robot Learning.\"}","unifolding.gif","UniFolding: Towards Sample-efficient, Scalable, and Generalizable Robotic Garment Folding","conference",[],{"type":13,"tag":222,"props":232,"children":238},{":artifactLinks":233,":authors":234,":venue":235,"thumbnail":236,"title":237,"type":229},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf\"}","[\"Wenqiang Xu*\",\"Wenxin Du*\",\"Han Xue\",\"Yutong Li\",\"Ruolin Ye\",\"Yan-Feng Wang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICCV\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF International Conference on Computer Vision\"}","clothpose.png","ClothPose: A Real-world Benchmark for Visual Analysis of Garment Pose via An Indirect Recording Solution",[239],{"type":13,"tag":26,"props":240,"children":241},{},[242],{"type":13,"tag":243,"props":244,"children":248},"span",{"className":245},[246,247],"text-red-600","font-bold",[249],{"type":23,"value":250},"üî• Oral Presentation.",{"type":13,"tag":222,"props":252,"children":258},{":artifactLinks":253,":authors":254,":venue":255,"thumbnail":256,"title":257,"type":229},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_GarmentTracking_Category-Level_Garment_Pose_Tracking_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.13913.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/GarmentTracking\",\"Website\":\"https://garment-tracking.robotflow.ai/\"}","[\"Han Xue\",\"Wenqiang Xu\",\"Jieyi Zhang\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Ruolin Ye\",\"Cewu Lu\"]","{\"acronym\":\"CVPR\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\"}","garment-tracking.gif","GarmentTracking: Category-Level Garment Pose Tracking",[],{"type":13,"tag":222,"props":260,"children":265},{":artifactLinks":261,":authors":262,":venue":255,"thumbnail":263,"title":264,"type":229},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Visual-Tactile_Sensing_for_In-Hand_Object_Reconstruction_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.14498.pdf\",\"Code\":\"https://github.com/jeffsonyu/VTacO\",\"Website\":\"https://sites.google.com/view/vtaco/\"}","[\"Wenqiang Xu*\",\"Zhenjun Yu*\",\"Han Xue\",\"Ruolin Ye\",\"Siqiong Yao\",\"Cewu Lu (*Equal contribution)\"]","vtaco.png","Visual-Tactile Sensing for In-Hand Object Reconstruction",[],{"type":13,"tag":222,"props":267,"children":273},{":artifactLinks":268,":authors":269,":venue":270,"thumbnail":271,"title":272,"type":229},"{\"Proceeding\":\"https://www.roboticsproceedings.org/rss19/p087.pdf\",\"Code\":\"https://github.com/mvig-robotflow/pyrfuniverse\",\"Website\":\"https://sites.google.com/view/rfuniverse\"}","[\"Haoyuan Fu*\",\"Wenqiang Xu*\",\"Ruolin Ye*\",\"Han Xue\",\"Zhenjun Yu\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Jieyi Zhang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"RSS\",\"year\":2023,\"name\":\"Robotics: Science and Systems.\"}","rfuniverse.png","Demonstrating RFUniverse: A Multiphysics Simulation Platform for Embodied AI",[],{"type":13,"tag":222,"props":275,"children":282},{":artifactLinks":276,":authors":277,":venue":278,"thumbnail":279,"title":280,"type":281},"{\"arXiv\":\"https://arxiv.org/pdf/2105.03260\"}","[\"Liu Liu*\",\"Han Xue*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"TIP\",\"year\":2022,\"name\":\"IEEE Transactions on Image Processing.\"}","articulation_real.png","Toward Real-World Category-Level Articulation Pose Estimation","journal",[],{"type":13,"tag":222,"props":284,"children":290},{":artifactLinks":285,":authors":286,":venue":287,"thumbnail":288,"title":289,"type":229},"{\"Proceeding\":\"https://www.bmvc2021-virtualconference.com/assets/papers/0544.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2112.07334.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/OMAD\"}","[\"Han Xue*\",\"Liu Liu*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"BMVC\",\"year\":2021,\"name\":\"The 32nd British Machine Vision Conference.\"}","omad.png","OMAD: Object Model with Articulated Deformations for Pose Estimation and Retrieval",[],{"type":13,"tag":222,"props":292,"children":298},{":artifactLinks":293,":authors":294,":venue":295,"thumbnail":296,"title":297,"type":229},"{\"Proceeding\":\"https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660222.pdf\",\"arXiv\":\"https://arxiv.org/pdf/1912.11473.pdf\",\"Code\":\"https://github.com/justimyhxu/Dense-RepPoints\"}","[\"Ze Yang*\",\"Yinghao Xu*\",\"Han Xue*\",\"Zheng Zhang\",\"Raquel Urtasun\",\"Liwei Wang\",\"Stephen Lin\",\"Han Hu (*Equal contribution)\"]","{\"acronym\":\"ECCV\",\"year\":2020,\"name\":\"The European Conference on Computer Vision.\"}","dense_reppoints.jpg","Dense RepPoints: Representing Visual Objects with Dense Point Sets",[],{"type":13,"tag":222,"props":300,"children":307},{":artifactLinks":301,":authors":302,":venue":303,"thumbnail":304,"title":305,"type":229,":hideBottomBorder":306},"{\"Proceeding\":\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf\",\"Code\":\"https://github.com/driving-behavior/DBNet\",\"Website\":\"http://www.dbehavior.net/\"}","[\"Yiping Chen*\",\"Jingkang Wang*\",\"Jonathan Li\",\"Cewu Lu\",\"Zhipeng Luo\",\"Han Xue\",\"Cheng Wang (*Equal contribution)\"]","{\"acronym\":\"CVPR\",\"year\":2018,\"name\":\"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\"}","dbnet.jpg","Lidar-video driving dataset: Learning driving policies effectively","true",[],{"type":13,"tag":26,"props":309,"children":310},{},[311],{"type":13,"tag":32,"props":312,"children":314},{"href":313},"/publication/",[315],{"type":23,"value":316},"Full publication list >>>",{"type":13,"tag":18,"props":318,"children":320},{"id":319},"awards",[321],{"type":23,"value":322},"üèÜ Awards",{"type":13,"tag":324,"props":325,"children":326},"ul",{},[327,333,338,343,366,371,376],{"type":13,"tag":328,"props":329,"children":330},"li",{},[331],{"type":23,"value":332},"Outstanding Graduates in Shanghai (Top 3%) in 2021.",{"type":13,"tag":328,"props":334,"children":335},{},[336],{"type":23,"value":337},"Rongchang Technology Innovation Scholarship (Top 10 students in SJTU) in 2020.",{"type":13,"tag":328,"props":339,"children":340},{},[341],{"type":23,"value":342},"SenseTime Scholarship (Top 21 undergraduates in China) in 2020.",{"type":13,"tag":328,"props":344,"children":345},{},[346,348],{"type":23,"value":347},"National Scholarship (Top 3 students in CS Department).\n",{"type":13,"tag":324,"props":349,"children":350},{},[351,356,361],{"type":13,"tag":328,"props":352,"children":353},{},[354],{"type":23,"value":355},"National Scholarship in 2019",{"type":13,"tag":328,"props":357,"children":358},{},[359],{"type":23,"value":360},"National Scholarship in 2018",{"type":13,"tag":328,"props":362,"children":363},{},[364],{"type":23,"value":365},"National Scholarship in 2017",{"type":13,"tag":328,"props":367,"children":368},{},[369],{"type":23,"value":370},"Academic Excellence Scholarship (Class A) of SJTU (Top 1% in SJTU) in 2018.",{"type":13,"tag":328,"props":372,"children":373},{},[374],{"type":23,"value":375},"Meritorious Winner Prize of Mathematical Contest in Modeling in 2018.",{"type":13,"tag":328,"props":377,"children":378},{},[379],{"type":23,"value":380},"1st Prize in China Undergraduate Mathematical Contest in Modeling (Shanghai Division) in 2017.",{"type":13,"tag":18,"props":382,"children":384},{"id":383},"find-me",[385],{"type":23,"value":386},"üìß Find Me",{"type":13,"tag":388,"props":389,"children":392},"contact-item",{"icon":390,"url":391},"email","mailto:xiaoxiaoxh@sjtu.edu.cn",[393],{"type":13,"tag":26,"props":394,"children":395},{},[396],{"type":23,"value":397},"Email",{"type":13,"tag":388,"props":399,"children":402},{"icon":400,"url":401},"github","https://github.com/xiaoxiaoxh",[403],{"type":13,"tag":26,"props":404,"children":405},{},[406],{"type":23,"value":407},"GitHub",{"type":13,"tag":388,"props":409,"children":412},{"icon":410,"url":411},"twitter","https://twitter.com/xiaoxiaoxh",[413],{"type":13,"tag":26,"props":414,"children":415},{},[416],{"type":23,"value":417},"Twitter",{"title":5,"searchDepth":419,"depth":419,"links":420},2,[421,422,423,424,425,426],{"id":20,"depth":419,"text":24},{"id":98,"depth":419,"text":101},{"id":117,"depth":419,"text":120},{"id":217,"depth":419,"text":220},{"id":319,"depth":419,"text":322},{"id":383,"depth":419,"text":386},"markdown","content:index.md","content","index.md","md",{"_path":433,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":434,"description":5,"leadingImage":435,"disableFancyImage":8,"body":436,"_type":427,"_id":555,"_source":429,"_file":556,"_extension":431},"/news","News","me-news-google.png",{"type":10,"children":437,"toc":553},[438,443],{"type":13,"tag":439,"props":440,"children":442},"MarkdownHeader",{"subtitle":441,"title":434},"üì¢ Latest: I'm a Ph.D. candidate now!",[],{"type":13,"tag":324,"props":444,"children":445},{},[446,473,483,500,526],{"type":13,"tag":328,"props":447,"children":448},{},[449,454,456,463,465,472],{"type":13,"tag":130,"props":450,"children":451},{},[452],{"type":23,"value":453},"11/05/2023",{"type":23,"value":455}," üéâ We have released the ",{"type":13,"tag":32,"props":457,"children":460},{"href":458,"rel":459},"https://github.com/xiaoxiaoxh/UniFolding",[36],[461],{"type":23,"value":462},"code",{"type":23,"value":464}," of our CoRL 2023 paper ",{"type":13,"tag":32,"props":466,"children":469},{"href":467,"rel":468},"https://unifolding.robotflow.ai/",[36],[470],{"type":23,"value":471},"UniFolding",{"type":23,"value":58},{"type":13,"tag":328,"props":474,"children":475},{},[476,481],{"type":13,"tag":130,"props":477,"children":478},{},[479],{"type":23,"value":480},"10/31/2023",{"type":23,"value":482}," üéì Passed my classes and research qualifications, I'm a Ph.D. candidate now!",{"type":13,"tag":328,"props":484,"children":485},{},[486,491,493,498],{"type":13,"tag":130,"props":487,"children":488},{},[489],{"type":23,"value":490},"08/31/2023",{"type":23,"value":492}," üéâ One paper (",{"type":13,"tag":32,"props":494,"children":496},{"href":467,"rel":495},[36],[497],{"type":23,"value":471},{"type":23,"value":499},") is accepted by CoRL 2023!",{"type":13,"tag":328,"props":501,"children":502},{},[503,508,510,517,519,524],{"type":13,"tag":130,"props":504,"children":505},{},[506],{"type":23,"value":507},"07/14/2023",{"type":23,"value":509}," üî• One paper (",{"type":13,"tag":32,"props":511,"children":514},{"href":512,"rel":513},"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf",[36],[515],{"type":23,"value":516},"ClothPose",{"type":23,"value":518},") is accepted by ICCV 2023 as an ",{"type":13,"tag":130,"props":520,"children":521},{},[522],{"type":23,"value":523},"oral presentation",{"type":23,"value":525},"!",{"type":13,"tag":328,"props":527,"children":528},{},[529,534,536,543,544,551],{"type":13,"tag":130,"props":530,"children":531},{},[532],{"type":23,"value":533},"02/28/2023",{"type":23,"value":535}," üéâ Two papers (",{"type":13,"tag":32,"props":537,"children":540},{"href":538,"rel":539},"https://garment-tracking.robotflow.ai/",[36],[541],{"type":23,"value":542},"GarmentTracking",{"type":23,"value":86},{"type":13,"tag":32,"props":545,"children":548},{"href":546,"rel":547},"https://sites.google.com/view/vtaco/",[36],[549],{"type":23,"value":550},"VTaCo",{"type":23,"value":552},") are accepted by CVPR 2023!",{"title":5,"searchDepth":419,"depth":419,"links":554},[],"content:news.md","news.md",1699139465265]