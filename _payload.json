[{"data":1,"prerenderedAt":808},["Reactive",2],{"content-query-1DxZ1vYQk5":3,"content-query-B6mqoO8PxR":602},{"_path":4,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":7,"description":5,"hideTitle":8,"disableFancyImage":8,"body":9,"_type":597,"_id":598,"_source":599,"_file":600,"_extension":601},"/","",false,"Home",true,{"type":10,"children":11,"toc":587},"root",[12,17,25,68,73,114,120,124,133,139,180,214,239,245,255,262,269,289,314,322,339,347,354,362,371,379,387,396,405,411,496,502,550,556,567,577],{"type":13,"tag":14,"props":15,"children":16},"element","IndexHeader",{},[],{"type":13,"tag":18,"props":19,"children":21},"h2",{"id":20},"Ô∏è-about-me",[22],{"type":23,"value":24},"text","ü¶∏üèª‚Äç‚ôÇÔ∏è About Me",{"type":13,"tag":26,"props":27,"children":28},"p",{},[29,31,40,42,49,51,57,59,66],{"type":23,"value":30},"I am a fourth-year Computer Science Ph.D. candidate at ",{"type":13,"tag":32,"props":33,"children":37},"a",{"href":34,"rel":35},"https://en.sjtu.edu.cn/",[36],"nofollow",[38],{"type":23,"value":39},"Shanghai Jiao Tong University",{"type":23,"value":41}," and a member of ",{"type":13,"tag":32,"props":43,"children":46},{"href":44,"rel":45},"https://www.mvig.org/",[36],[47],{"type":23,"value":48},"Machine Intelligence and Vision Group (MVIG)",{"type":23,"value":50}," under the supervision of ",{"type":13,"tag":32,"props":52,"children":54},{"href":44,"rel":53},[36],[55],{"type":23,"value":56},"Prof. Cewu Lu",{"type":23,"value":58}," (Âç¢Á≠ñÂêæ). I am also a member of ",{"type":13,"tag":32,"props":60,"children":63},{"href":61,"rel":62},"https://ai.sjtu.edu.cn/info/announcements/204",[36],[64],{"type":23,"value":65},"Wu Wen Jun Honorary Doctoral Program",{"type":23,"value":67}," (Âê¥Êñá‰øäËç£Ë™âÂçöÂ£´Áè≠).",{"type":13,"tag":26,"props":69,"children":70},{},[71],{"type":23,"value":72},"I receive my bachelor degree from Shanghai Jiao Tong Universiy in 2021. My research interests lie in Robotics and Computer Vision. Previously, I have been working on deformable object perception and manipulation. Now I am particularly interested in imitation learning with tactile/force sensing and low-cost data collection system.",{"type":13,"tag":26,"props":74,"children":75},{},[76,78,85,87,94,96,103,105,112],{"type":23,"value":77},"In the past, I have interned at ",{"type":13,"tag":32,"props":79,"children":82},{"href":80,"rel":81},"https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/",[36],[83],{"type":23,"value":84},"Microsoft Research Asia",{"type":23,"value":86}," under the supervsion of ",{"type":13,"tag":32,"props":88,"children":91},{"href":89,"rel":90},"https://ancientmooner.github.io/",[36],[92],{"type":23,"value":93},"Han Hu",{"type":23,"value":95}," and ",{"type":13,"tag":32,"props":97,"children":100},{"href":98,"rel":99},"http://yue-cao.me/",[36],[101],{"type":23,"value":102},"Yue Cao",{"type":23,"value":104},". I also spent time with Prof. ",{"type":13,"tag":32,"props":106,"children":109},{"href":107,"rel":108},"http://hxu.rocks/index.html",[36],[110],{"type":23,"value":111},"Huazhe Xu",{"type":23,"value":113}," at Tsinghua University on my projects. In my spare time, I enjoy watching movies and playing with robotsü§ñ.",{"type":13,"tag":18,"props":115,"children":117},{"id":116},"news",[118],{"type":23,"value":119},"üì∞ News",{"type":13,"tag":121,"props":122,"children":123},"ShortNews",{},[],{"type":13,"tag":26,"props":125,"children":126},{},[127],{"type":13,"tag":32,"props":128,"children":130},{"href":129},"/news/",[131],{"type":23,"value":132},"More news >>>",{"type":13,"tag":18,"props":134,"children":136},{"id":135},"experiences",[137],{"type":23,"value":138},"üè´ Experiences",{"type":13,"tag":140,"props":141,"children":143},"ExperienceRow",{"icon":142},"sjtu.png",[144],{"type":13,"tag":26,"props":145,"children":146},{},[147,152,156,158,161,163,168,170,175,178],{"type":13,"tag":148,"props":149,"children":150},"strong",{},[151],{"type":23,"value":39},{"type":13,"tag":153,"props":154,"children":155},"br",{},[],{"type":23,"value":157},"\nPh.D. Student ",{"type":13,"tag":153,"props":159,"children":160},{},[],{"type":23,"value":162},"\nResearch assistant in ",{"type":13,"tag":32,"props":164,"children":166},{"href":44,"rel":165},[36],[167],{"type":23,"value":48},{"type":23,"value":169},", advised by ",{"type":13,"tag":32,"props":171,"children":173},{"href":44,"rel":172},[36],[174],{"type":23,"value":56},{"type":13,"tag":153,"props":176,"children":177},{},[],{"type":23,"value":179},"\nSep. 2021 - Present",{"type":13,"tag":140,"props":181,"children":183},{"icon":182},"microsoft.png",[184],{"type":13,"tag":26,"props":185,"children":186},{},[187,191,194,196,201,202,207,209,212],{"type":13,"tag":148,"props":188,"children":189},{},[190],{"type":23,"value":84},{"type":13,"tag":153,"props":192,"children":193},{},[],{"type":23,"value":195},"\nResearch Intern, advised by ",{"type":13,"tag":32,"props":197,"children":199},{"href":89,"rel":198},[36],[200],{"type":23,"value":93},{"type":23,"value":95},{"type":13,"tag":32,"props":203,"children":205},{"href":98,"rel":204},[36],[206],{"type":23,"value":102},{"type":23,"value":208},".",{"type":13,"tag":153,"props":210,"children":211},{},[],{"type":23,"value":213},"\nJul. 2019 - Mar. 2020",{"type":13,"tag":140,"props":215,"children":216},{"icon":142},[217],{"type":13,"tag":26,"props":218,"children":219},{},[220,224,227,229,232,234,237],{"type":13,"tag":148,"props":221,"children":222},{},[223],{"type":23,"value":39},{"type":13,"tag":153,"props":225,"children":226},{},[],{"type":23,"value":228},"\nBachelor of Engineering in Computer Science. ",{"type":13,"tag":153,"props":230,"children":231},{},[],{"type":23,"value":233},"\nGPA 4.04/4.3, Rank 3/150 (Top 2%)",{"type":13,"tag":153,"props":235,"children":236},{},[],{"type":23,"value":238},"\nSep 2016 - Jun. 2021",{"type":13,"tag":18,"props":240,"children":242},{"id":241},"selected-publications",[243],{"type":23,"value":244},"üìÑ Selected Publications",{"type":13,"tag":246,"props":247,"children":254},"PublicationRow",{":artifactLinks":248,":authors":249,":venue":250,"thumbnail":251,"title":252,"type":253},"{\"Website\":\"https://implicit-rdp.github.io/\",\"arXiv\":\"https://arxiv.org/abs/2512.10946\"}","[\"Wendi Chen\",\"Han Xue\",\"Yi Wang\",\"Fangyuan Zhou\",\"Jun Lv\",\"Yang Jin\",\"Shirun Tang\",\"Chuan Wen‚Ä†\",\"Cewu Lu‚Ä† (‚Ä†equal advising)\"]","{\"acronym\":\"arXiv\",\"year\":2025,\"name\":\"arXiv\"}","ImplicitRDP.gif","ImplicitRDP: An End-to-End Visual-Force Diffusion Policy with Structural Slow-Fast Learning","conference",[],{"type":13,"tag":246,"props":256,"children":261},{":artifactLinks":257,":authors":258,":venue":250,"thumbnail":259,"title":260,"type":253},"{\"Website\":\"https://ericjin2002.github.io/SOE\",\"arXiv\":\"https://arxiv.org/abs/2509.19292\"}","[\"Yang Jin\",\"Jun Lv\",\"Han Xue\",\"Wendi Chen\",\"Chuan Wen‚Ä†\",\"Cewu Lu‚Ä† (‚Ä†equal advising)\"]","SOE.gif","SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration",[],{"type":13,"tag":246,"props":263,"children":268},{":artifactLinks":264,":authors":265,":venue":250,"thumbnail":266,"title":267,"type":253},"{\"Website\":\"https://right-side-out.github.io/\",\"arXiv\":\"https://arxiv.org/abs/2509.15953\"}","[\"Chang Yu*\",\"Siyu Ma*\",\"Wenxin Du\",\"Zeshun Zong\",\"Han Xue\",\"Wendi Chen\",\"Cewu Lu\",\"Yin Yang\",\"Xuchen Han\",\"Joseph Masterjohn\",\"Alejandro Castro\",\"Chenfanfu Jiang (*Equal contribution)\"]","Right-Side-Out.gif","Right-Side-Out: Learning Zero-Shot Sim-to-Real Garment Reversal",[],{"type":13,"tag":246,"props":270,"children":276},{":artifactLinks":271,":authors":272,":venue":273,"thumbnail":274,"title":275,"type":253},"{\"Website\":\"https://reactive-diffusion-policy.github.io/\",\"arXiv\":\"https://arxiv.org/abs/2503.02881\",\"Code\":\"https://github.com/xiaoxiaoxh/reactive_diffusion_policy\"}","[\"Han Xue*\",\"Jieji Ren*\",\"Wendi Chen*\",\"Gu Zhang\",\"Yuan Fang\",\"Guoying Gu\",\"Huazhe Xu‚Ä†\",\"Cewu Lu‚Ä† (‚Ä†Equal advising)\"]","{\"acronym\":\"RSS\",\"year\":2025,\"name\":\"Robotics: Science and Systems (RSS)\"}","rdp.gif","Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation",[277],{"type":13,"tag":26,"props":278,"children":279},{},[280],{"type":13,"tag":281,"props":282,"children":286},"span",{"className":283},[284,285],"text-red-600","font-bold",[287],{"type":23,"value":288},"üî• Best Student Paper Finalist.",{"type":13,"tag":246,"props":290,"children":296},{":artifactLinks":291,":authors":292,":venue":293,"thumbnail":294,"title":295,"type":253},"{\"Website\":\"https://deform-pam.robotflow.ai/\",\"arXiv\":\"https://arxiv.org/abs/2410.11584\",\"Code\":\"https://github.com/xiaoxiaoxh/DeformPAM\"}","[\"Wendi Chen*\",\"Han Xue*\",\"Fangyuan Zhou\",\"Yuan Fang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICRA\",\"year\":2025,\"name\":\"IEEE International Conference on Robotics and Automation (ICRA)\"}","deform-pam.gif","DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object Manipulation via Preference-based Action Alignment",[297],{"type":13,"tag":26,"props":298,"children":299},{},[300],{"type":13,"tag":281,"props":301,"children":303},{"className":302},[284,285],[304,306,313],{"type":23,"value":305},"üî• Best Paper Finalist ",{"type":13,"tag":32,"props":307,"children":310},{"href":308,"rel":309},"https://deformable-workshop.github.io/icra2025/",[36],[311],{"type":23,"value":312},"@ RMDO Workshop in ICRA 2025",{"type":23,"value":208},{"type":13,"tag":246,"props":315,"children":321},{":artifactLinks":316,":authors":317,":venue":318,"thumbnail":319,"title":320,"type":253},"{\"Proceeding\":\"https://openreview.net/pdf?id=ANJuNDFdvP\",\"arXiv\":\"https://arxiv.org/abs/2311.01267\",\"Code\":\"https://github.com/xiaoxiaoxh/UniFolding\",\"Website\":\"https://unifolding.robotflow.ai/\"}","[\"Han Xue*\",\"Yutong Li*\",\"Wenqiang Xu\",\"Huanyu Li\",\"Dongzhe Zheng\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"CoRL\",\"year\":2023,\"name\":\"7th Annual Conference on Robot Learning.\"}","unifolding.gif","UniFolding: Towards Sample-efficient, Scalable, and Generalizable Robotic Garment Folding",[],{"type":13,"tag":246,"props":323,"children":329},{":artifactLinks":324,":authors":325,":venue":326,"thumbnail":327,"title":328,"type":253},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf\"}","[\"Wenqiang Xu*\",\"Wenxin Du*\",\"Han Xue\",\"Yutong Li\",\"Ruolin Ye\",\"Yan-Feng Wang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICCV\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF International Conference on Computer Vision\"}","clothpose.png","ClothPose: A Real-world Benchmark for Visual Analysis of Garment Pose via An Indirect Recording Solution",[330],{"type":13,"tag":26,"props":331,"children":332},{},[333],{"type":13,"tag":281,"props":334,"children":336},{"className":335},[284,285],[337],{"type":23,"value":338},"üî• Oral Presentation.",{"type":13,"tag":246,"props":340,"children":346},{":artifactLinks":341,":authors":342,":venue":343,"thumbnail":344,"title":345,"type":253},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_GarmentTracking_Category-Level_Garment_Pose_Tracking_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.13913.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/GarmentTracking\",\"Website\":\"https://garment-tracking.robotflow.ai/\"}","[\"Han Xue\",\"Wenqiang Xu\",\"Jieyi Zhang\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Ruolin Ye\",\"Cewu Lu\"]","{\"acronym\":\"CVPR\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\"}","garment-tracking.gif","GarmentTracking: Category-Level Garment Pose Tracking",[],{"type":13,"tag":246,"props":348,"children":353},{":artifactLinks":349,":authors":350,":venue":343,"thumbnail":351,"title":352,"type":253},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Visual-Tactile_Sensing_for_In-Hand_Object_Reconstruction_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.14498.pdf\",\"Code\":\"https://github.com/jeffsonyu/VTacO\",\"Website\":\"https://sites.google.com/view/vtaco/\"}","[\"Wenqiang Xu*\",\"Zhenjun Yu*\",\"Han Xue\",\"Ruolin Ye\",\"Siqiong Yao\",\"Cewu Lu (*Equal contribution)\"]","vtaco.png","Visual-Tactile Sensing for In-Hand Object Reconstruction",[],{"type":13,"tag":246,"props":355,"children":361},{":artifactLinks":356,":authors":357,":venue":358,"thumbnail":359,"title":360,"type":253},"{\"Proceeding\":\"https://www.roboticsproceedings.org/rss19/p087.pdf\",\"Code\":\"https://github.com/mvig-robotflow/pyrfuniverse\",\"Website\":\"https://sites.google.com/view/rfuniverse\"}","[\"Haoyuan Fu*\",\"Wenqiang Xu*\",\"Ruolin Ye*\",\"Han Xue\",\"Zhenjun Yu\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Jieyi Zhang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"RSS\",\"year\":2023,\"name\":\"Robotics: Science and Systems.\"}","rfuniverse.png","Demonstrating RFUniverse: A Multiphysics Simulation Platform for Embodied AI",[],{"type":13,"tag":246,"props":363,"children":370},{":artifactLinks":364,":authors":365,":venue":366,"thumbnail":367,"title":368,"type":369},"{\"arXiv\":\"https://arxiv.org/pdf/2105.03260\"}","[\"Liu Liu*\",\"Han Xue*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"TIP\",\"year\":2022,\"name\":\"IEEE Transactions on Image Processing.\"}","articulation_real.png","Toward Real-World Category-Level Articulation Pose Estimation","journal",[],{"type":13,"tag":246,"props":372,"children":378},{":artifactLinks":373,":authors":374,":venue":375,"thumbnail":376,"title":377,"type":253},"{\"Proceeding\":\"https://www.bmvc2021-virtualconference.com/assets/papers/0544.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2112.07334.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/OMAD\"}","[\"Han Xue*\",\"Liu Liu*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"BMVC\",\"year\":2021,\"name\":\"The 32nd British Machine Vision Conference.\"}","omad.png","OMAD: Object Model with Articulated Deformations for Pose Estimation and Retrieval",[],{"type":13,"tag":246,"props":380,"children":386},{":artifactLinks":381,":authors":382,":venue":383,"thumbnail":384,"title":385,"type":253},"{\"Proceeding\":\"https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660222.pdf\",\"arXiv\":\"https://arxiv.org/pdf/1912.11473.pdf\",\"Code\":\"https://github.com/justimyhxu/Dense-RepPoints\"}","[\"Ze Yang*\",\"Yinghao Xu*\",\"Han Xue*\",\"Zheng Zhang\",\"Raquel Urtasun\",\"Liwei Wang\",\"Stephen Lin\",\"Han Hu (*Equal contribution)\"]","{\"acronym\":\"ECCV\",\"year\":2020,\"name\":\"The European Conference on Computer Vision.\"}","dense_reppoints.jpg","Dense RepPoints: Representing Visual Objects with Dense Point Sets",[],{"type":13,"tag":246,"props":388,"children":395},{":artifactLinks":389,":authors":390,":venue":391,"thumbnail":392,"title":393,"type":253,":hideBottomBorder":394},"{\"Proceeding\":\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf\",\"Code\":\"https://github.com/driving-behavior/DBNet\",\"Website\":\"http://www.dbehavior.net/\"}","[\"Yiping Chen*\",\"Jingkang Wang*\",\"Jonathan Li\",\"Cewu Lu\",\"Zhipeng Luo\",\"Han Xue\",\"Cheng Wang (*Equal contribution)\"]","{\"acronym\":\"CVPR\",\"year\":2018,\"name\":\"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\"}","dbnet.jpg","Lidar-video driving dataset: Learning driving policies effectively","true",[],{"type":13,"tag":26,"props":397,"children":398},{},[399],{"type":13,"tag":32,"props":400,"children":402},{"href":401},"/publication/",[403],{"type":23,"value":404},"Full publication list >>>",{"type":13,"tag":18,"props":406,"children":408},{"id":407},"talks",[409],{"type":23,"value":410},"‚ú® Talks",{"type":13,"tag":412,"props":413,"children":414},"ul",{},[415,428,440,462,483],{"type":13,"tag":416,"props":417,"children":418},"li",{},[419,421,427],{"type":23,"value":420},"[Jun. 2025] Invited talk @ ",{"type":13,"tag":32,"props":422,"children":424},{"href":80,"rel":423},[36],[425],{"type":23,"value":426},"Microsoft Research Aisa",{"type":23,"value":208},{"type":13,"tag":416,"props":429,"children":430},{},[431,433,438],{"type":23,"value":432},"[May. 2025] Invited talk @ ",{"type":13,"tag":281,"props":434,"children":435},{},[436],{"type":23,"value":437},"ÂÖ∑Ë∫´Êô∫ËÉΩ‰πãÂøÉ",{"type":23,"value":439}," on \"Robotic Imitation Learning with Vision-Tactile/Force Sensing\".",{"type":13,"tag":416,"props":441,"children":442},{},[443,445,452,454,461],{"type":23,"value":444},"[Apr. 2025] Invited talk @ ",{"type":13,"tag":32,"props":446,"children":449},{"href":447,"rel":448},"https://www.techbeat.net/",[36],[450],{"type":23,"value":451},"TechBeat",{"type":23,"value":453}," (Â∞ÜÈó®ÂàõÊäï) on \"Robotic Imitation Learning with Vision-Tactile/Force Sensing\". The link of the recorded video is ",{"type":13,"tag":32,"props":455,"children":458},{"href":456,"rel":457},"https://www.techbeat.net/talk-info?id=963",[36],[459],{"type":23,"value":460},"here",{"type":23,"value":208},{"type":13,"tag":416,"props":463,"children":464},{},[465,467,474,476,482],{"type":23,"value":466},"[Dec. 2024] Invited talk @ ",{"type":13,"tag":32,"props":468,"children":471},{"href":469,"rel":470},"https://www.roscon.cn/2024/index.html",[36],[472],{"type":23,"value":473},"ROSCon China 2024",{"type":23,"value":475}," workshop on \"Development Trends and Challenges in Embodied AI\". The link of the recorded video is ",{"type":13,"tag":32,"props":477,"children":480},{"href":478,"rel":479},"https://www.bilibili.com/video/BV1Z3cteAEnD/?spm_id_from=333.337.search-card.all.click&vd_source=8e062051896958b92b4759e0f4753657",[36],[481],{"type":23,"value":460},{"type":23,"value":208},{"type":13,"tag":416,"props":484,"children":485},{},[486,488,494],{"type":23,"value":487},"[Oct. 2024] Invited talk @ ",{"type":13,"tag":32,"props":489,"children":491},{"href":107,"rel":490},[36],[492],{"type":23,"value":493},"TEA lab",{"type":23,"value":495}," in Tsinghua University, IIIS on \"Efficient Learning for Long-horizon Deformable Object Manipulation\"",{"type":13,"tag":18,"props":497,"children":499},{"id":498},"awards",[500],{"type":23,"value":501},"üèÜ Awards",{"type":13,"tag":412,"props":503,"children":504},{},[505,510,515,520,525,530,535,540,545],{"type":13,"tag":416,"props":506,"children":507},{},[508],{"type":23,"value":509},"Best Student Paper Award Finalist [ÊúÄ‰Ω≥Â≠¶ÁîüËÆ∫ÊñáÂ•ñÊèêÂêç] in RSS 2025.",{"type":13,"tag":416,"props":511,"children":512},{},[513],{"type":23,"value":514},"Wu Wen Jun Scholarship [Âê¥Êñá‰øäÂ•ñÂ≠¶Èáë] in 2024 and 2025.",{"type":13,"tag":416,"props":516,"children":517},{},[518],{"type":23,"value":519},"Outstanding Graduates in Shanghai (Top 3%) [‰∏äÊµ∑Â∏Ç‰ºòÁßÄÊØï‰∏öÁîü] in 2021.",{"type":13,"tag":416,"props":521,"children":522},{},[523],{"type":23,"value":524},"Rongchang Technology Innovation Scholarship (Top 10 students in SJTU) [Ëç£Êò∂ÁßëÊäÄÂàõÊñ∞Â•ñÂ≠¶Èáë] in 2020.",{"type":13,"tag":416,"props":526,"children":527},{},[528],{"type":23,"value":529},"SenseTime Scholarship (Top 21 undergraduates in China) [ÂïÜÊ±§Â•ñÂ≠¶Èáë] in 2020.",{"type":13,"tag":416,"props":531,"children":532},{},[533],{"type":23,"value":534},"National Scholarship (Top 3 students in CS Department) in 2017, 2018 and 2019 (three consecutive years) [ÂõΩÂÆ∂Â•ñÂ≠¶ÈáëÔºàËøûÁª≠‰∏âÂπ¥Ôºâ].",{"type":13,"tag":416,"props":536,"children":537},{},[538],{"type":23,"value":539},"Academic Excellence Scholarship (Class A) of SJTU (Top 1% in SJTU) in 2018.",{"type":13,"tag":416,"props":541,"children":542},{},[543],{"type":23,"value":544},"Meritorious Winner Prize of Mathematical Contest in Modeling in 2018.",{"type":13,"tag":416,"props":546,"children":547},{},[548],{"type":23,"value":549},"1st Prize in China Undergraduate Mathematical Contest in Modeling (Shanghai Division) in 2017.",{"type":13,"tag":18,"props":551,"children":553},{"id":552},"find-me",[554],{"type":23,"value":555},"üìß Find Me",{"type":13,"tag":557,"props":558,"children":561},"contact-item",{"icon":559,"url":560},"email","mailto:xiaoxiaoxh@sjtu.edu.cn",[562],{"type":13,"tag":26,"props":563,"children":564},{},[565],{"type":23,"value":566},"Email",{"type":13,"tag":557,"props":568,"children":571},{"icon":569,"url":570},"github","https://github.com/xiaoxiaoxh",[572],{"type":13,"tag":26,"props":573,"children":574},{},[575],{"type":23,"value":576},"GitHub",{"type":13,"tag":557,"props":578,"children":581},{"icon":579,"url":580},"twitter","https://twitter.com/HanXue012",[582],{"type":13,"tag":26,"props":583,"children":584},{},[585],{"type":23,"value":586},"Twitter",{"title":5,"searchDepth":588,"depth":588,"links":589},2,[590,591,592,593,594,595,596],{"id":20,"depth":588,"text":24},{"id":116,"depth":588,"text":119},{"id":135,"depth":588,"text":138},{"id":241,"depth":588,"text":244},{"id":407,"depth":588,"text":410},{"id":498,"depth":588,"text":501},{"id":552,"depth":588,"text":555},"markdown","content:index.md","content","index.md","md",{"_path":603,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":604,"description":5,"leadingImage":605,"disableFancyImage":8,"body":606,"_type":597,"_id":806,"_source":599,"_file":807,"_extension":601},"/news","News","me-news-google.png",{"type":10,"children":607,"toc":804},[608,613,691,712],{"type":13,"tag":609,"props":610,"children":612},"MarkdownHeader",{"subtitle":611,"title":604},"üì¢ Latest: One paper (DeformPAM) is accepted by ICRA 2025!",[],{"type":13,"tag":412,"props":614,"children":615},{},[616,642,673],{"type":13,"tag":416,"props":617,"children":618},{},[619,624,626,633,635,640],{"type":13,"tag":148,"props":620,"children":621},{},[622],{"type":23,"value":623},"06/18/2025",{"type":23,"value":625}," üî• ",{"type":13,"tag":32,"props":627,"children":630},{"href":628,"rel":629},"https://reactive-diffusion-policy.github.io/",[36],[631],{"type":23,"value":632},"RDP",{"type":23,"value":634}," is selected as the ",{"type":13,"tag":148,"props":636,"children":637},{},[638],{"type":23,"value":639},"Best Student Paper Award Finalist",{"type":23,"value":641}," @ RSS 2025!",{"type":13,"tag":416,"props":643,"children":644},{},[645,650,651,656,657,662,664,671],{"type":13,"tag":148,"props":646,"children":647},{},[648],{"type":23,"value":649},"05/23/2025",{"type":23,"value":625},{"type":13,"tag":32,"props":652,"children":654},{"href":628,"rel":653},[36],[655],{"type":23,"value":632},{"type":23,"value":634},{"type":13,"tag":148,"props":658,"children":659},{},[660],{"type":23,"value":661},"Best Paper",{"type":23,"value":663}," in ",{"type":13,"tag":32,"props":665,"children":668},{"href":666,"rel":667},"https://sites.google.com/view/icra-2025-beyond-pick-place/home",[36],[669],{"type":23,"value":670},"Beyond Pick and Place workshop",{"type":23,"value":672}," @ ICRA 2025!",{"type":13,"tag":416,"props":674,"children":675},{},[676,681,683,689],{"type":13,"tag":148,"props":677,"children":678},{},[679],{"type":23,"value":680},"04/11/2025",{"type":23,"value":682}," üéâ One paper (",{"type":13,"tag":32,"props":684,"children":686},{"href":628,"rel":685},[36],[687],{"type":23,"value":688},"Reactive Diffusion Policy",{"type":23,"value":690},") is accepted by RSS 2025!",{"type":13,"tag":412,"props":692,"children":693},{},[694],{"type":13,"tag":416,"props":695,"children":696},{},[697,702,703,710],{"type":13,"tag":148,"props":698,"children":699},{},[700],{"type":23,"value":701},"01/29/2025",{"type":23,"value":682},{"type":13,"tag":32,"props":704,"children":707},{"href":705,"rel":706},"https://deform-pam.robotflow.ai/",[36],[708],{"type":23,"value":709},"DeformPAM",{"type":23,"value":711},") is accepted by ICRA 2025!",{"type":13,"tag":412,"props":713,"children":714},{},[715,733,759,777],{"type":13,"tag":416,"props":716,"children":717},{},[718,723,724,731],{"type":13,"tag":148,"props":719,"children":720},{},[721],{"type":23,"value":722},"08/31/2023",{"type":23,"value":682},{"type":13,"tag":32,"props":725,"children":728},{"href":726,"rel":727},"https://unifolding.robotflow.ai/",[36],[729],{"type":23,"value":730},"UniFolding",{"type":23,"value":732},") is accepted by CoRL 2023!",{"type":13,"tag":416,"props":734,"children":735},{},[736,741,743,750,752,757],{"type":13,"tag":148,"props":737,"children":738},{},[739],{"type":23,"value":740},"07/14/2023",{"type":23,"value":742}," üî• One paper (",{"type":13,"tag":32,"props":744,"children":747},{"href":745,"rel":746},"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf",[36],[748],{"type":23,"value":749},"ClothPose",{"type":23,"value":751},") is accepted by ICCV 2023 as an ",{"type":13,"tag":148,"props":753,"children":754},{},[755],{"type":23,"value":756},"oral presentation",{"type":23,"value":758},"!",{"type":13,"tag":416,"props":760,"children":761},{},[762,767,768,775],{"type":13,"tag":148,"props":763,"children":764},{},[765],{"type":23,"value":766},"05/13/2023",{"type":23,"value":682},{"type":13,"tag":32,"props":769,"children":772},{"href":770,"rel":771},"https://sites.google.com/view/rfuniverse",[36],[773],{"type":23,"value":774},"RFUniverse",{"type":23,"value":776},") is accepted by RSS 2023!",{"type":13,"tag":416,"props":778,"children":779},{},[780,785,787,794,795,802],{"type":13,"tag":148,"props":781,"children":782},{},[783],{"type":23,"value":784},"02/28/2023",{"type":23,"value":786}," üéâ Two papers (",{"type":13,"tag":32,"props":788,"children":791},{"href":789,"rel":790},"https://garment-tracking.robotflow.ai/",[36],[792],{"type":23,"value":793},"GarmentTracking",{"type":23,"value":95},{"type":13,"tag":32,"props":796,"children":799},{"href":797,"rel":798},"https://sites.google.com/view/vtaco/",[36],[800],{"type":23,"value":801},"VTaCo",{"type":23,"value":803},") are accepted by CVPR 2023!",{"title":5,"searchDepth":588,"depth":588,"links":805},[],"content:news.md","news.md",1766458175969]