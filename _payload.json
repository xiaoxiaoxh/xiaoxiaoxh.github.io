[{"data":1,"prerenderedAt":757},["Reactive",2],{"content-query-1DxZ1vYQk5":3,"content-query-B6mqoO8PxR":551},{"_path":4,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":7,"description":5,"hideTitle":8,"disableFancyImage":8,"body":9,"_type":546,"_id":547,"_source":548,"_file":549,"_extension":550},"/","",false,"Home",true,{"type":10,"children":11,"toc":536},"root",[12,17,25,68,73,114,119,125,129,138,144,185,219,244,250,272,297,305,322,330,337,345,354,362,370,379,388,394,455,461,499,505,516,526],{"type":13,"tag":14,"props":15,"children":16},"element","IndexHeader",{},[],{"type":13,"tag":18,"props":19,"children":21},"h2",{"id":20},"Ô∏è-about-me",[22],{"type":23,"value":24},"text","ü¶∏üèª‚Äç‚ôÇÔ∏è About Me",{"type":13,"tag":26,"props":27,"children":28},"p",{},[29,31,40,42,49,51,57,59,66],{"type":23,"value":30},"I am a fourth-year Computer Science Ph.D. candidate at ",{"type":13,"tag":32,"props":33,"children":37},"a",{"href":34,"rel":35},"https://en.sjtu.edu.cn/",[36],"nofollow",[38],{"type":23,"value":39},"Shanghai Jiao Tong University",{"type":23,"value":41}," and a member of ",{"type":13,"tag":32,"props":43,"children":46},{"href":44,"rel":45},"https://www.mvig.org/",[36],[47],{"type":23,"value":48},"Machine Intelligence and Vision Group (MVIG)",{"type":23,"value":50}," under the supervision of ",{"type":13,"tag":32,"props":52,"children":54},{"href":44,"rel":53},[36],[55],{"type":23,"value":56},"Prof. Cewu Lu",{"type":23,"value":58}," (Âç¢Á≠ñÂêæ). I am also a member of ",{"type":13,"tag":32,"props":60,"children":63},{"href":61,"rel":62},"https://ai.sjtu.edu.cn/info/announcements/204",[36],[64],{"type":23,"value":65},"Wu Wen Jun Honorary Doctoral Program",{"type":23,"value":67}," (Âê¥Êñá‰øäËç£Ë™âÂçöÂ£´Áè≠).",{"type":13,"tag":26,"props":69,"children":70},{},[71],{"type":23,"value":72},"I receive my bachelor degree from Shanghai Jiao Tong Universiy in 2021. My research interests lie in Robotics and Computer Vision. Previously, I have been working on deformable object perception and manipulation. Now I am particularly interested in imitation learning with tactile/force sensing and low-cost data collection system.",{"type":13,"tag":26,"props":74,"children":75},{},[76,78,85,87,94,96,103,105,112],{"type":23,"value":77},"In the past, I have interned at ",{"type":13,"tag":32,"props":79,"children":82},{"href":80,"rel":81},"https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/",[36],[83],{"type":23,"value":84},"Microsoft Research Asia",{"type":23,"value":86}," under the supervsion of ",{"type":13,"tag":32,"props":88,"children":91},{"href":89,"rel":90},"https://ancientmooner.github.io/",[36],[92],{"type":23,"value":93},"Han Hu",{"type":23,"value":95}," and ",{"type":13,"tag":32,"props":97,"children":100},{"href":98,"rel":99},"http://yue-cao.me/",[36],[101],{"type":23,"value":102},"Yue Cao",{"type":23,"value":104},". I also spent time with Prof. ",{"type":13,"tag":32,"props":106,"children":109},{"href":107,"rel":108},"http://hxu.rocks/index.html",[36],[110],{"type":23,"value":111},"Huazhe Xu",{"type":23,"value":113}," at Tsinghua University on my projects. In my spare time, I enjoy watching movies and playing with robotsü§ñ.",{"type":13,"tag":26,"props":115,"children":116},{},[117],{"type":23,"value":118},"I'm on the job market now!",{"type":13,"tag":18,"props":120,"children":122},{"id":121},"news",[123],{"type":23,"value":124},"üì∞ News",{"type":13,"tag":126,"props":127,"children":128},"ShortNews",{},[],{"type":13,"tag":26,"props":130,"children":131},{},[132],{"type":13,"tag":32,"props":133,"children":135},{"href":134},"/news/",[136],{"type":23,"value":137},"More news >>>",{"type":13,"tag":18,"props":139,"children":141},{"id":140},"experiences",[142],{"type":23,"value":143},"üè´ Experiences",{"type":13,"tag":145,"props":146,"children":148},"ExperienceRow",{"icon":147},"sjtu.png",[149],{"type":13,"tag":26,"props":150,"children":151},{},[152,157,161,163,166,168,173,175,180,183],{"type":13,"tag":153,"props":154,"children":155},"strong",{},[156],{"type":23,"value":39},{"type":13,"tag":158,"props":159,"children":160},"br",{},[],{"type":23,"value":162},"\nPh.D. Student ",{"type":13,"tag":158,"props":164,"children":165},{},[],{"type":23,"value":167},"\nResearch assistant in ",{"type":13,"tag":32,"props":169,"children":171},{"href":44,"rel":170},[36],[172],{"type":23,"value":48},{"type":23,"value":174},", advised by ",{"type":13,"tag":32,"props":176,"children":178},{"href":44,"rel":177},[36],[179],{"type":23,"value":56},{"type":13,"tag":158,"props":181,"children":182},{},[],{"type":23,"value":184},"\nSep. 2021 - Present",{"type":13,"tag":145,"props":186,"children":188},{"icon":187},"microsoft.png",[189],{"type":13,"tag":26,"props":190,"children":191},{},[192,196,199,201,206,207,212,214,217],{"type":13,"tag":153,"props":193,"children":194},{},[195],{"type":23,"value":84},{"type":13,"tag":158,"props":197,"children":198},{},[],{"type":23,"value":200},"\nResearch Intern, advised by ",{"type":13,"tag":32,"props":202,"children":204},{"href":89,"rel":203},[36],[205],{"type":23,"value":93},{"type":23,"value":95},{"type":13,"tag":32,"props":208,"children":210},{"href":98,"rel":209},[36],[211],{"type":23,"value":102},{"type":23,"value":213},".",{"type":13,"tag":158,"props":215,"children":216},{},[],{"type":23,"value":218},"\nJul. 2019 - Mar. 2020",{"type":13,"tag":145,"props":220,"children":221},{"icon":147},[222],{"type":13,"tag":26,"props":223,"children":224},{},[225,229,232,234,237,239,242],{"type":13,"tag":153,"props":226,"children":227},{},[228],{"type":23,"value":39},{"type":13,"tag":158,"props":230,"children":231},{},[],{"type":23,"value":233},"\nBachelor of Engineering in Computer Science. ",{"type":13,"tag":158,"props":235,"children":236},{},[],{"type":23,"value":238},"\nGPA 4.04/4.3, Rank 3/150 (Top 2%)",{"type":13,"tag":158,"props":240,"children":241},{},[],{"type":23,"value":243},"\nSep 2016 - Jun. 2021",{"type":13,"tag":18,"props":245,"children":247},{"id":246},"selected-publications",[248],{"type":23,"value":249},"üìÑ Selected Publications",{"type":13,"tag":251,"props":252,"children":259},"PublicationRow",{":artifactLinks":253,":authors":254,":venue":255,"thumbnail":256,"title":257,"type":258},"{\"Website\":\"https://reactive-diffusion-policy.github.io/\",\"arXiv\":\"https://arxiv.org/abs/2503.02881\",\"Code\":\"https://github.com/xiaoxiaoxh/reactive_diffusion_policy\"}","[\"Han Xue*\",\"Jieji Ren*\",\"Wendi Chen*\",\"Gu Zhang\",\"Yuan Fang\",\"Guoying Gu\",\"Huazhe Xu‚Ä†\",\"Cewu Lu‚Ä† (‚Ä†Equal advising)\"]","{\"acronym\":\"RSS\",\"year\":2025,\"name\":\"Robotics: Science and Systems (RSS)\"}","rdp.gif","Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation","conference",[260],{"type":13,"tag":26,"props":261,"children":262},{},[263],{"type":13,"tag":264,"props":265,"children":269},"span",{"className":266},[267,268],"text-red-600","font-bold",[270],{"type":23,"value":271},"üî• Best Student Paper Finalist.",{"type":13,"tag":251,"props":273,"children":279},{":artifactLinks":274,":authors":275,":venue":276,"thumbnail":277,"title":278,"type":258},"{\"Website\":\"https://deform-pam.robotflow.ai/\",\"arXiv\":\"https://arxiv.org/abs/2410.11584\",\"Code\":\"https://github.com/xiaoxiaoxh/DeformPAM\"}","[\"Wendi Chen*\",\"Han Xue*\",\"Fangyuan Zhou\",\"Yuan Fang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICRA\",\"year\":2025,\"name\":\"IEEE International Conference on Robotics and Automation (ICRA)\"}","deform-pam.gif","DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object Manipulation via Preference-based Action Alignment",[280],{"type":13,"tag":26,"props":281,"children":282},{},[283],{"type":13,"tag":264,"props":284,"children":286},{"className":285},[267,268],[287,289,296],{"type":23,"value":288},"üî• Best Paper Finalist ",{"type":13,"tag":32,"props":290,"children":293},{"href":291,"rel":292},"https://deformable-workshop.github.io/icra2025/",[36],[294],{"type":23,"value":295},"@ RMDO Workshop in ICRA 2025",{"type":23,"value":213},{"type":13,"tag":251,"props":298,"children":304},{":artifactLinks":299,":authors":300,":venue":301,"thumbnail":302,"title":303,"type":258},"{\"Proceeding\":\"https://openreview.net/pdf?id=ANJuNDFdvP\",\"arXiv\":\"https://arxiv.org/abs/2311.01267\",\"Code\":\"https://github.com/xiaoxiaoxh/UniFolding\",\"Website\":\"https://unifolding.robotflow.ai/\"}","[\"Han Xue*\",\"Yutong Li*\",\"Wenqiang Xu\",\"Huanyu Li\",\"Dongzhe Zheng\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"CoRL\",\"year\":2023,\"name\":\"7th Annual Conference on Robot Learning.\"}","unifolding.gif","UniFolding: Towards Sample-efficient, Scalable, and Generalizable Robotic Garment Folding",[],{"type":13,"tag":251,"props":306,"children":312},{":artifactLinks":307,":authors":308,":venue":309,"thumbnail":310,"title":311,"type":258},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf\"}","[\"Wenqiang Xu*\",\"Wenxin Du*\",\"Han Xue\",\"Yutong Li\",\"Ruolin Ye\",\"Yan-Feng Wang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICCV\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF International Conference on Computer Vision\"}","clothpose.png","ClothPose: A Real-world Benchmark for Visual Analysis of Garment Pose via An Indirect Recording Solution",[313],{"type":13,"tag":26,"props":314,"children":315},{},[316],{"type":13,"tag":264,"props":317,"children":319},{"className":318},[267,268],[320],{"type":23,"value":321},"üî• Oral Presentation.",{"type":13,"tag":251,"props":323,"children":329},{":artifactLinks":324,":authors":325,":venue":326,"thumbnail":327,"title":328,"type":258},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_GarmentTracking_Category-Level_Garment_Pose_Tracking_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.13913.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/GarmentTracking\",\"Website\":\"https://garment-tracking.robotflow.ai/\"}","[\"Han Xue\",\"Wenqiang Xu\",\"Jieyi Zhang\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Ruolin Ye\",\"Cewu Lu\"]","{\"acronym\":\"CVPR\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\"}","garment-tracking.gif","GarmentTracking: Category-Level Garment Pose Tracking",[],{"type":13,"tag":251,"props":331,"children":336},{":artifactLinks":332,":authors":333,":venue":326,"thumbnail":334,"title":335,"type":258},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Visual-Tactile_Sensing_for_In-Hand_Object_Reconstruction_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.14498.pdf\",\"Code\":\"https://github.com/jeffsonyu/VTacO\",\"Website\":\"https://sites.google.com/view/vtaco/\"}","[\"Wenqiang Xu*\",\"Zhenjun Yu*\",\"Han Xue\",\"Ruolin Ye\",\"Siqiong Yao\",\"Cewu Lu (*Equal contribution)\"]","vtaco.png","Visual-Tactile Sensing for In-Hand Object Reconstruction",[],{"type":13,"tag":251,"props":338,"children":344},{":artifactLinks":339,":authors":340,":venue":341,"thumbnail":342,"title":343,"type":258},"{\"Proceeding\":\"https://www.roboticsproceedings.org/rss19/p087.pdf\",\"Code\":\"https://github.com/mvig-robotflow/pyrfuniverse\",\"Website\":\"https://sites.google.com/view/rfuniverse\"}","[\"Haoyuan Fu*\",\"Wenqiang Xu*\",\"Ruolin Ye*\",\"Han Xue\",\"Zhenjun Yu\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Jieyi Zhang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"RSS\",\"year\":2023,\"name\":\"Robotics: Science and Systems.\"}","rfuniverse.png","Demonstrating RFUniverse: A Multiphysics Simulation Platform for Embodied AI",[],{"type":13,"tag":251,"props":346,"children":353},{":artifactLinks":347,":authors":348,":venue":349,"thumbnail":350,"title":351,"type":352},"{\"arXiv\":\"https://arxiv.org/pdf/2105.03260\"}","[\"Liu Liu*\",\"Han Xue*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"TIP\",\"year\":2022,\"name\":\"IEEE Transactions on Image Processing.\"}","articulation_real.png","Toward Real-World Category-Level Articulation Pose Estimation","journal",[],{"type":13,"tag":251,"props":355,"children":361},{":artifactLinks":356,":authors":357,":venue":358,"thumbnail":359,"title":360,"type":258},"{\"Proceeding\":\"https://www.bmvc2021-virtualconference.com/assets/papers/0544.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2112.07334.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/OMAD\"}","[\"Han Xue*\",\"Liu Liu*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"BMVC\",\"year\":2021,\"name\":\"The 32nd British Machine Vision Conference.\"}","omad.png","OMAD: Object Model with Articulated Deformations for Pose Estimation and Retrieval",[],{"type":13,"tag":251,"props":363,"children":369},{":artifactLinks":364,":authors":365,":venue":366,"thumbnail":367,"title":368,"type":258},"{\"Proceeding\":\"https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660222.pdf\",\"arXiv\":\"https://arxiv.org/pdf/1912.11473.pdf\",\"Code\":\"https://github.com/justimyhxu/Dense-RepPoints\"}","[\"Ze Yang*\",\"Yinghao Xu*\",\"Han Xue*\",\"Zheng Zhang\",\"Raquel Urtasun\",\"Liwei Wang\",\"Stephen Lin\",\"Han Hu (*Equal contribution)\"]","{\"acronym\":\"ECCV\",\"year\":2020,\"name\":\"The European Conference on Computer Vision.\"}","dense_reppoints.jpg","Dense RepPoints: Representing Visual Objects with Dense Point Sets",[],{"type":13,"tag":251,"props":371,"children":378},{":artifactLinks":372,":authors":373,":venue":374,"thumbnail":375,"title":376,"type":258,":hideBottomBorder":377},"{\"Proceeding\":\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf\",\"Code\":\"https://github.com/driving-behavior/DBNet\",\"Website\":\"http://www.dbehavior.net/\"}","[\"Yiping Chen*\",\"Jingkang Wang*\",\"Jonathan Li\",\"Cewu Lu\",\"Zhipeng Luo\",\"Han Xue\",\"Cheng Wang (*Equal contribution)\"]","{\"acronym\":\"CVPR\",\"year\":2018,\"name\":\"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\"}","dbnet.jpg","Lidar-video driving dataset: Learning driving policies effectively","true",[],{"type":13,"tag":26,"props":380,"children":381},{},[382],{"type":13,"tag":32,"props":383,"children":385},{"href":384},"/publication/",[386],{"type":23,"value":387},"Full publication list >>>",{"type":13,"tag":18,"props":389,"children":391},{"id":390},"talks",[392],{"type":23,"value":393},"‚ú® Talks",{"type":13,"tag":395,"props":396,"children":397},"ul",{},[398,421,442],{"type":13,"tag":399,"props":400,"children":401},"li",{},[402,404,411,413,420],{"type":23,"value":403},"[Apr. 2025] Invited talk @ ",{"type":13,"tag":32,"props":405,"children":408},{"href":406,"rel":407},"https://www.techbeat.net/",[36],[409],{"type":23,"value":410},"TechBeat",{"type":23,"value":412}," (Â∞ÜÈó®ÂàõÊäï) on \"Robotic Imitation Learning with Vision-Tactile/Force Sensing\". The link of the recorded video is ",{"type":13,"tag":32,"props":414,"children":417},{"href":415,"rel":416},"https://www.techbeat.net/talk-info?id=963",[36],[418],{"type":23,"value":419},"here",{"type":23,"value":213},{"type":13,"tag":399,"props":422,"children":423},{},[424,426,433,435,441],{"type":23,"value":425},"[Dec. 2024] Invited talk @ ",{"type":13,"tag":32,"props":427,"children":430},{"href":428,"rel":429},"https://www.roscon.cn/2024/index.html",[36],[431],{"type":23,"value":432},"ROSCon China 2024",{"type":23,"value":434}," workshop on \"Development Trends and Challenges in Embodied AI\". The link of the recorded video is ",{"type":13,"tag":32,"props":436,"children":439},{"href":437,"rel":438},"https://www.bilibili.com/video/BV1Z3cteAEnD/?spm_id_from=333.337.search-card.all.click&vd_source=8e062051896958b92b4759e0f4753657",[36],[440],{"type":23,"value":419},{"type":23,"value":213},{"type":13,"tag":399,"props":443,"children":444},{},[445,447,453],{"type":23,"value":446},"[Oct. 2024] Invited talk @ ",{"type":13,"tag":32,"props":448,"children":450},{"href":107,"rel":449},[36],[451],{"type":23,"value":452},"TEA lab",{"type":23,"value":454}," in Tsinghua University, IIIS on \"Efficient Learning for Long-horizon Deformable Object Manipulation\"",{"type":13,"tag":18,"props":456,"children":458},{"id":457},"awards",[459],{"type":23,"value":460},"üèÜ Awards",{"type":13,"tag":395,"props":462,"children":463},{},[464,469,474,479,484,489,494],{"type":13,"tag":399,"props":465,"children":466},{},[467],{"type":23,"value":468},"Outstanding Graduates in Shanghai (Top 3%) [‰∏äÊµ∑Â∏Ç‰ºòÁßÄÊØï‰∏öÁîü] in 2021.",{"type":13,"tag":399,"props":470,"children":471},{},[472],{"type":23,"value":473},"Rongchang Technology Innovation Scholarship (Top 10 students in SJTU) [Ëç£Êò∂ÁßëÊäÄÂàõÊñ∞Â•ñÂ≠¶Èáë] in 2020.",{"type":13,"tag":399,"props":475,"children":476},{},[477],{"type":23,"value":478},"SenseTime Scholarship (Top 21 undergraduates in China) [ÂïÜÊ±§Â•ñÂ≠¶Èáë] in 2020.",{"type":13,"tag":399,"props":480,"children":481},{},[482],{"type":23,"value":483},"National Scholarship (Top 3 students in CS Department) in 2017, 2018 and 2019 (three consecutive years) [ÂõΩÂÆ∂Â•ñÂ≠¶ÈáëÔºàËøûÁª≠‰∏âÂπ¥Ôºâ].",{"type":13,"tag":399,"props":485,"children":486},{},[487],{"type":23,"value":488},"Academic Excellence Scholarship (Class A) of SJTU (Top 1% in SJTU) in 2018.",{"type":13,"tag":399,"props":490,"children":491},{},[492],{"type":23,"value":493},"Meritorious Winner Prize of Mathematical Contest in Modeling in 2018.",{"type":13,"tag":399,"props":495,"children":496},{},[497],{"type":23,"value":498},"1st Prize in China Undergraduate Mathematical Contest in Modeling (Shanghai Division) in 2017.",{"type":13,"tag":18,"props":500,"children":502},{"id":501},"find-me",[503],{"type":23,"value":504},"üìß Find Me",{"type":13,"tag":506,"props":507,"children":510},"contact-item",{"icon":508,"url":509},"email","mailto:xiaoxiaoxh@sjtu.edu.cn",[511],{"type":13,"tag":26,"props":512,"children":513},{},[514],{"type":23,"value":515},"Email",{"type":13,"tag":506,"props":517,"children":520},{"icon":518,"url":519},"github","https://github.com/xiaoxiaoxh",[521],{"type":13,"tag":26,"props":522,"children":523},{},[524],{"type":23,"value":525},"GitHub",{"type":13,"tag":506,"props":527,"children":530},{"icon":528,"url":529},"twitter","https://twitter.com/HanXue012",[531],{"type":13,"tag":26,"props":532,"children":533},{},[534],{"type":23,"value":535},"Twitter",{"title":5,"searchDepth":537,"depth":537,"links":538},2,[539,540,541,542,543,544,545],{"id":20,"depth":537,"text":24},{"id":121,"depth":537,"text":124},{"id":140,"depth":537,"text":143},{"id":246,"depth":537,"text":249},{"id":390,"depth":537,"text":393},{"id":457,"depth":537,"text":460},{"id":501,"depth":537,"text":504},"markdown","content:index.md","content","index.md","md",{"_path":552,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":553,"description":5,"leadingImage":554,"disableFancyImage":8,"body":555,"_type":546,"_id":755,"_source":548,"_file":756,"_extension":550},"/news","News","me-news-google.png",{"type":10,"children":556,"toc":753},[557,562,640,661],{"type":13,"tag":558,"props":559,"children":561},"MarkdownHeader",{"subtitle":560,"title":553},"üì¢ Latest: One paper (DeformPAM) is accepted by ICRA 2025!",[],{"type":13,"tag":395,"props":563,"children":564},{},[565,591,622],{"type":13,"tag":399,"props":566,"children":567},{},[568,573,575,582,584,589],{"type":13,"tag":153,"props":569,"children":570},{},[571],{"type":23,"value":572},"06/18/2025",{"type":23,"value":574}," üî• ",{"type":13,"tag":32,"props":576,"children":579},{"href":577,"rel":578},"https://reactive-diffusion-policy.github.io/",[36],[580],{"type":23,"value":581},"RDP",{"type":23,"value":583}," is selected as the ",{"type":13,"tag":153,"props":585,"children":586},{},[587],{"type":23,"value":588},"Best Student Paper Award Finalist",{"type":23,"value":590}," @ RSS 2025!",{"type":13,"tag":399,"props":592,"children":593},{},[594,599,600,605,606,611,613,620],{"type":13,"tag":153,"props":595,"children":596},{},[597],{"type":23,"value":598},"05/23/2025",{"type":23,"value":574},{"type":13,"tag":32,"props":601,"children":603},{"href":577,"rel":602},[36],[604],{"type":23,"value":581},{"type":23,"value":583},{"type":13,"tag":153,"props":607,"children":608},{},[609],{"type":23,"value":610},"Best Paper",{"type":23,"value":612}," in ",{"type":13,"tag":32,"props":614,"children":617},{"href":615,"rel":616},"https://sites.google.com/view/icra-2025-beyond-pick-place/home",[36],[618],{"type":23,"value":619},"Beyond Pick and Place workshop",{"type":23,"value":621}," @ ICRA 2025!",{"type":13,"tag":399,"props":623,"children":624},{},[625,630,632,638],{"type":13,"tag":153,"props":626,"children":627},{},[628],{"type":23,"value":629},"04/11/2025",{"type":23,"value":631}," üéâ One paper (",{"type":13,"tag":32,"props":633,"children":635},{"href":577,"rel":634},[36],[636],{"type":23,"value":637},"Reactive Diffusion Policy",{"type":23,"value":639},") is accepted by RSS 2025!",{"type":13,"tag":395,"props":641,"children":642},{},[643],{"type":13,"tag":399,"props":644,"children":645},{},[646,651,652,659],{"type":13,"tag":153,"props":647,"children":648},{},[649],{"type":23,"value":650},"01/29/2025",{"type":23,"value":631},{"type":13,"tag":32,"props":653,"children":656},{"href":654,"rel":655},"https://deform-pam.robotflow.ai/",[36],[657],{"type":23,"value":658},"DeformPAM",{"type":23,"value":660},") is accepted by ICRA 2025!",{"type":13,"tag":395,"props":662,"children":663},{},[664,682,708,726],{"type":13,"tag":399,"props":665,"children":666},{},[667,672,673,680],{"type":13,"tag":153,"props":668,"children":669},{},[670],{"type":23,"value":671},"08/31/2023",{"type":23,"value":631},{"type":13,"tag":32,"props":674,"children":677},{"href":675,"rel":676},"https://unifolding.robotflow.ai/",[36],[678],{"type":23,"value":679},"UniFolding",{"type":23,"value":681},") is accepted by CoRL 2023!",{"type":13,"tag":399,"props":683,"children":684},{},[685,690,692,699,701,706],{"type":13,"tag":153,"props":686,"children":687},{},[688],{"type":23,"value":689},"07/14/2023",{"type":23,"value":691}," üî• One paper (",{"type":13,"tag":32,"props":693,"children":696},{"href":694,"rel":695},"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf",[36],[697],{"type":23,"value":698},"ClothPose",{"type":23,"value":700},") is accepted by ICCV 2023 as an ",{"type":13,"tag":153,"props":702,"children":703},{},[704],{"type":23,"value":705},"oral presentation",{"type":23,"value":707},"!",{"type":13,"tag":399,"props":709,"children":710},{},[711,716,717,724],{"type":13,"tag":153,"props":712,"children":713},{},[714],{"type":23,"value":715},"05/13/2023",{"type":23,"value":631},{"type":13,"tag":32,"props":718,"children":721},{"href":719,"rel":720},"https://sites.google.com/view/rfuniverse",[36],[722],{"type":23,"value":723},"RFUniverse",{"type":23,"value":725},") is accepted by RSS 2023!",{"type":13,"tag":399,"props":727,"children":728},{},[729,734,736,743,744,751],{"type":13,"tag":153,"props":730,"children":731},{},[732],{"type":23,"value":733},"02/28/2023",{"type":23,"value":735}," üéâ Two papers (",{"type":13,"tag":32,"props":737,"children":740},{"href":738,"rel":739},"https://garment-tracking.robotflow.ai/",[36],[741],{"type":23,"value":742},"GarmentTracking",{"type":23,"value":95},{"type":13,"tag":32,"props":745,"children":748},{"href":746,"rel":747},"https://sites.google.com/view/vtaco/",[36],[749],{"type":23,"value":750},"VTaCo",{"type":23,"value":752},") are accepted by CVPR 2023!",{"title":5,"searchDepth":537,"depth":537,"links":754},[],"content:news.md","news.md",1750309680598]