[{"data":1,"prerenderedAt":786},["Reactive",2],{"content-query-1DxZ1vYQk5":3,"content-query-B6mqoO8PxR":580},{"_path":4,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":7,"description":5,"hideTitle":8,"disableFancyImage":8,"body":9,"_type":575,"_id":576,"_source":577,"_file":578,"_extension":579},"/","",false,"Home",true,{"type":10,"children":11,"toc":565},"root",[12,17,25,68,73,114,120,124,133,139,180,214,239,245,267,292,300,317,325,332,340,349,357,365,374,383,389,474,480,528,534,545,555],{"type":13,"tag":14,"props":15,"children":16},"element","IndexHeader",{},[],{"type":13,"tag":18,"props":19,"children":21},"h2",{"id":20},"Ô∏è-about-me",[22],{"type":23,"value":24},"text","ü¶∏üèª‚Äç‚ôÇÔ∏è About Me",{"type":13,"tag":26,"props":27,"children":28},"p",{},[29,31,40,42,49,51,57,59,66],{"type":23,"value":30},"I am a fourth-year Computer Science Ph.D. candidate at ",{"type":13,"tag":32,"props":33,"children":37},"a",{"href":34,"rel":35},"https://en.sjtu.edu.cn/",[36],"nofollow",[38],{"type":23,"value":39},"Shanghai Jiao Tong University",{"type":23,"value":41}," and a member of ",{"type":13,"tag":32,"props":43,"children":46},{"href":44,"rel":45},"https://www.mvig.org/",[36],[47],{"type":23,"value":48},"Machine Intelligence and Vision Group (MVIG)",{"type":23,"value":50}," under the supervision of ",{"type":13,"tag":32,"props":52,"children":54},{"href":44,"rel":53},[36],[55],{"type":23,"value":56},"Prof. Cewu Lu",{"type":23,"value":58}," (Âç¢Á≠ñÂêæ). I am also a member of ",{"type":13,"tag":32,"props":60,"children":63},{"href":61,"rel":62},"https://ai.sjtu.edu.cn/info/announcements/204",[36],[64],{"type":23,"value":65},"Wu Wen Jun Honorary Doctoral Program",{"type":23,"value":67}," (Âê¥Êñá‰øäËç£Ë™âÂçöÂ£´Áè≠).",{"type":13,"tag":26,"props":69,"children":70},{},[71],{"type":23,"value":72},"I receive my bachelor degree from Shanghai Jiao Tong Universiy in 2021. My research interests lie in Robotics and Computer Vision. Previously, I have been working on deformable object perception and manipulation. Now I am particularly interested in imitation learning with tactile/force sensing and low-cost data collection system.",{"type":13,"tag":26,"props":74,"children":75},{},[76,78,85,87,94,96,103,105,112],{"type":23,"value":77},"In the past, I have interned at ",{"type":13,"tag":32,"props":79,"children":82},{"href":80,"rel":81},"https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/",[36],[83],{"type":23,"value":84},"Microsoft Research Asia",{"type":23,"value":86}," under the supervsion of ",{"type":13,"tag":32,"props":88,"children":91},{"href":89,"rel":90},"https://ancientmooner.github.io/",[36],[92],{"type":23,"value":93},"Han Hu",{"type":23,"value":95}," and ",{"type":13,"tag":32,"props":97,"children":100},{"href":98,"rel":99},"http://yue-cao.me/",[36],[101],{"type":23,"value":102},"Yue Cao",{"type":23,"value":104},". I also spent time with Prof. ",{"type":13,"tag":32,"props":106,"children":109},{"href":107,"rel":108},"http://hxu.rocks/index.html",[36],[110],{"type":23,"value":111},"Huazhe Xu",{"type":23,"value":113}," at Tsinghua University on my projects. In my spare time, I enjoy watching movies and playing with robotsü§ñ.",{"type":13,"tag":18,"props":115,"children":117},{"id":116},"news",[118],{"type":23,"value":119},"üì∞ News",{"type":13,"tag":121,"props":122,"children":123},"ShortNews",{},[],{"type":13,"tag":26,"props":125,"children":126},{},[127],{"type":13,"tag":32,"props":128,"children":130},{"href":129},"/news/",[131],{"type":23,"value":132},"More news >>>",{"type":13,"tag":18,"props":134,"children":136},{"id":135},"experiences",[137],{"type":23,"value":138},"üè´ Experiences",{"type":13,"tag":140,"props":141,"children":143},"ExperienceRow",{"icon":142},"sjtu.png",[144],{"type":13,"tag":26,"props":145,"children":146},{},[147,152,156,158,161,163,168,170,175,178],{"type":13,"tag":148,"props":149,"children":150},"strong",{},[151],{"type":23,"value":39},{"type":13,"tag":153,"props":154,"children":155},"br",{},[],{"type":23,"value":157},"\nPh.D. Student ",{"type":13,"tag":153,"props":159,"children":160},{},[],{"type":23,"value":162},"\nResearch assistant in ",{"type":13,"tag":32,"props":164,"children":166},{"href":44,"rel":165},[36],[167],{"type":23,"value":48},{"type":23,"value":169},", advised by ",{"type":13,"tag":32,"props":171,"children":173},{"href":44,"rel":172},[36],[174],{"type":23,"value":56},{"type":13,"tag":153,"props":176,"children":177},{},[],{"type":23,"value":179},"\nSep. 2021 - Present",{"type":13,"tag":140,"props":181,"children":183},{"icon":182},"microsoft.png",[184],{"type":13,"tag":26,"props":185,"children":186},{},[187,191,194,196,201,202,207,209,212],{"type":13,"tag":148,"props":188,"children":189},{},[190],{"type":23,"value":84},{"type":13,"tag":153,"props":192,"children":193},{},[],{"type":23,"value":195},"\nResearch Intern, advised by ",{"type":13,"tag":32,"props":197,"children":199},{"href":89,"rel":198},[36],[200],{"type":23,"value":93},{"type":23,"value":95},{"type":13,"tag":32,"props":203,"children":205},{"href":98,"rel":204},[36],[206],{"type":23,"value":102},{"type":23,"value":208},".",{"type":13,"tag":153,"props":210,"children":211},{},[],{"type":23,"value":213},"\nJul. 2019 - Mar. 2020",{"type":13,"tag":140,"props":215,"children":216},{"icon":142},[217],{"type":13,"tag":26,"props":218,"children":219},{},[220,224,227,229,232,234,237],{"type":13,"tag":148,"props":221,"children":222},{},[223],{"type":23,"value":39},{"type":13,"tag":153,"props":225,"children":226},{},[],{"type":23,"value":228},"\nBachelor of Engineering in Computer Science. ",{"type":13,"tag":153,"props":230,"children":231},{},[],{"type":23,"value":233},"\nGPA 4.04/4.3, Rank 3/150 (Top 2%)",{"type":13,"tag":153,"props":235,"children":236},{},[],{"type":23,"value":238},"\nSep 2016 - Jun. 2021",{"type":13,"tag":18,"props":240,"children":242},{"id":241},"selected-publications",[243],{"type":23,"value":244},"üìÑ Selected Publications",{"type":13,"tag":246,"props":247,"children":254},"PublicationRow",{":artifactLinks":248,":authors":249,":venue":250,"thumbnail":251,"title":252,"type":253},"{\"Website\":\"https://reactive-diffusion-policy.github.io/\",\"arXiv\":\"https://arxiv.org/abs/2503.02881\",\"Code\":\"https://github.com/xiaoxiaoxh/reactive_diffusion_policy\"}","[\"Han Xue*\",\"Jieji Ren*\",\"Wendi Chen*\",\"Gu Zhang\",\"Yuan Fang\",\"Guoying Gu\",\"Huazhe Xu‚Ä†\",\"Cewu Lu‚Ä† (‚Ä†Equal advising)\"]","{\"acronym\":\"RSS\",\"year\":2025,\"name\":\"Robotics: Science and Systems (RSS)\"}","rdp.gif","Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation","conference",[255],{"type":13,"tag":26,"props":256,"children":257},{},[258],{"type":13,"tag":259,"props":260,"children":264},"span",{"className":261},[262,263],"text-red-600","font-bold",[265],{"type":23,"value":266},"üî• Best Student Paper Finalist.",{"type":13,"tag":246,"props":268,"children":274},{":artifactLinks":269,":authors":270,":venue":271,"thumbnail":272,"title":273,"type":253},"{\"Website\":\"https://deform-pam.robotflow.ai/\",\"arXiv\":\"https://arxiv.org/abs/2410.11584\",\"Code\":\"https://github.com/xiaoxiaoxh/DeformPAM\"}","[\"Wendi Chen*\",\"Han Xue*\",\"Fangyuan Zhou\",\"Yuan Fang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICRA\",\"year\":2025,\"name\":\"IEEE International Conference on Robotics and Automation (ICRA)\"}","deform-pam.gif","DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object Manipulation via Preference-based Action Alignment",[275],{"type":13,"tag":26,"props":276,"children":277},{},[278],{"type":13,"tag":259,"props":279,"children":281},{"className":280},[262,263],[282,284,291],{"type":23,"value":283},"üî• Best Paper Finalist ",{"type":13,"tag":32,"props":285,"children":288},{"href":286,"rel":287},"https://deformable-workshop.github.io/icra2025/",[36],[289],{"type":23,"value":290},"@ RMDO Workshop in ICRA 2025",{"type":23,"value":208},{"type":13,"tag":246,"props":293,"children":299},{":artifactLinks":294,":authors":295,":venue":296,"thumbnail":297,"title":298,"type":253},"{\"Proceeding\":\"https://openreview.net/pdf?id=ANJuNDFdvP\",\"arXiv\":\"https://arxiv.org/abs/2311.01267\",\"Code\":\"https://github.com/xiaoxiaoxh/UniFolding\",\"Website\":\"https://unifolding.robotflow.ai/\"}","[\"Han Xue*\",\"Yutong Li*\",\"Wenqiang Xu\",\"Huanyu Li\",\"Dongzhe Zheng\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"CoRL\",\"year\":2023,\"name\":\"7th Annual Conference on Robot Learning.\"}","unifolding.gif","UniFolding: Towards Sample-efficient, Scalable, and Generalizable Robotic Garment Folding",[],{"type":13,"tag":246,"props":301,"children":307},{":artifactLinks":302,":authors":303,":venue":304,"thumbnail":305,"title":306,"type":253},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf\"}","[\"Wenqiang Xu*\",\"Wenxin Du*\",\"Han Xue\",\"Yutong Li\",\"Ruolin Ye\",\"Yan-Feng Wang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICCV\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF International Conference on Computer Vision\"}","clothpose.png","ClothPose: A Real-world Benchmark for Visual Analysis of Garment Pose via An Indirect Recording Solution",[308],{"type":13,"tag":26,"props":309,"children":310},{},[311],{"type":13,"tag":259,"props":312,"children":314},{"className":313},[262,263],[315],{"type":23,"value":316},"üî• Oral Presentation.",{"type":13,"tag":246,"props":318,"children":324},{":artifactLinks":319,":authors":320,":venue":321,"thumbnail":322,"title":323,"type":253},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_GarmentTracking_Category-Level_Garment_Pose_Tracking_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.13913.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/GarmentTracking\",\"Website\":\"https://garment-tracking.robotflow.ai/\"}","[\"Han Xue\",\"Wenqiang Xu\",\"Jieyi Zhang\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Ruolin Ye\",\"Cewu Lu\"]","{\"acronym\":\"CVPR\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\"}","garment-tracking.gif","GarmentTracking: Category-Level Garment Pose Tracking",[],{"type":13,"tag":246,"props":326,"children":331},{":artifactLinks":327,":authors":328,":venue":321,"thumbnail":329,"title":330,"type":253},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Visual-Tactile_Sensing_for_In-Hand_Object_Reconstruction_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.14498.pdf\",\"Code\":\"https://github.com/jeffsonyu/VTacO\",\"Website\":\"https://sites.google.com/view/vtaco/\"}","[\"Wenqiang Xu*\",\"Zhenjun Yu*\",\"Han Xue\",\"Ruolin Ye\",\"Siqiong Yao\",\"Cewu Lu (*Equal contribution)\"]","vtaco.png","Visual-Tactile Sensing for In-Hand Object Reconstruction",[],{"type":13,"tag":246,"props":333,"children":339},{":artifactLinks":334,":authors":335,":venue":336,"thumbnail":337,"title":338,"type":253},"{\"Proceeding\":\"https://www.roboticsproceedings.org/rss19/p087.pdf\",\"Code\":\"https://github.com/mvig-robotflow/pyrfuniverse\",\"Website\":\"https://sites.google.com/view/rfuniverse\"}","[\"Haoyuan Fu*\",\"Wenqiang Xu*\",\"Ruolin Ye*\",\"Han Xue\",\"Zhenjun Yu\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Jieyi Zhang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"RSS\",\"year\":2023,\"name\":\"Robotics: Science and Systems.\"}","rfuniverse.png","Demonstrating RFUniverse: A Multiphysics Simulation Platform for Embodied AI",[],{"type":13,"tag":246,"props":341,"children":348},{":artifactLinks":342,":authors":343,":venue":344,"thumbnail":345,"title":346,"type":347},"{\"arXiv\":\"https://arxiv.org/pdf/2105.03260\"}","[\"Liu Liu*\",\"Han Xue*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"TIP\",\"year\":2022,\"name\":\"IEEE Transactions on Image Processing.\"}","articulation_real.png","Toward Real-World Category-Level Articulation Pose Estimation","journal",[],{"type":13,"tag":246,"props":350,"children":356},{":artifactLinks":351,":authors":352,":venue":353,"thumbnail":354,"title":355,"type":253},"{\"Proceeding\":\"https://www.bmvc2021-virtualconference.com/assets/papers/0544.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2112.07334.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/OMAD\"}","[\"Han Xue*\",\"Liu Liu*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"BMVC\",\"year\":2021,\"name\":\"The 32nd British Machine Vision Conference.\"}","omad.png","OMAD: Object Model with Articulated Deformations for Pose Estimation and Retrieval",[],{"type":13,"tag":246,"props":358,"children":364},{":artifactLinks":359,":authors":360,":venue":361,"thumbnail":362,"title":363,"type":253},"{\"Proceeding\":\"https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660222.pdf\",\"arXiv\":\"https://arxiv.org/pdf/1912.11473.pdf\",\"Code\":\"https://github.com/justimyhxu/Dense-RepPoints\"}","[\"Ze Yang*\",\"Yinghao Xu*\",\"Han Xue*\",\"Zheng Zhang\",\"Raquel Urtasun\",\"Liwei Wang\",\"Stephen Lin\",\"Han Hu (*Equal contribution)\"]","{\"acronym\":\"ECCV\",\"year\":2020,\"name\":\"The European Conference on Computer Vision.\"}","dense_reppoints.jpg","Dense RepPoints: Representing Visual Objects with Dense Point Sets",[],{"type":13,"tag":246,"props":366,"children":373},{":artifactLinks":367,":authors":368,":venue":369,"thumbnail":370,"title":371,"type":253,":hideBottomBorder":372},"{\"Proceeding\":\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf\",\"Code\":\"https://github.com/driving-behavior/DBNet\",\"Website\":\"http://www.dbehavior.net/\"}","[\"Yiping Chen*\",\"Jingkang Wang*\",\"Jonathan Li\",\"Cewu Lu\",\"Zhipeng Luo\",\"Han Xue\",\"Cheng Wang (*Equal contribution)\"]","{\"acronym\":\"CVPR\",\"year\":2018,\"name\":\"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\"}","dbnet.jpg","Lidar-video driving dataset: Learning driving policies effectively","true",[],{"type":13,"tag":26,"props":375,"children":376},{},[377],{"type":13,"tag":32,"props":378,"children":380},{"href":379},"/publication/",[381],{"type":23,"value":382},"Full publication list >>>",{"type":13,"tag":18,"props":384,"children":386},{"id":385},"talks",[387],{"type":23,"value":388},"‚ú® Talks",{"type":13,"tag":390,"props":391,"children":392},"ul",{},[393,406,418,440,461],{"type":13,"tag":394,"props":395,"children":396},"li",{},[397,399,405],{"type":23,"value":398},"[Jun. 2025] Invited talk @ ",{"type":13,"tag":32,"props":400,"children":402},{"href":80,"rel":401},[36],[403],{"type":23,"value":404},"Microsoft Research Aisa",{"type":23,"value":208},{"type":13,"tag":394,"props":407,"children":408},{},[409,411,416],{"type":23,"value":410},"[May. 2025] Invited talk @ ",{"type":13,"tag":259,"props":412,"children":413},{},[414],{"type":23,"value":415},"ÂÖ∑Ë∫´Êô∫ËÉΩ‰πãÂøÉ",{"type":23,"value":417}," on \"Robotic Imitation Learning with Vision-Tactile/Force Sensing\".",{"type":13,"tag":394,"props":419,"children":420},{},[421,423,430,432,439],{"type":23,"value":422},"[Apr. 2025] Invited talk @ ",{"type":13,"tag":32,"props":424,"children":427},{"href":425,"rel":426},"https://www.techbeat.net/",[36],[428],{"type":23,"value":429},"TechBeat",{"type":23,"value":431}," (Â∞ÜÈó®ÂàõÊäï) on \"Robotic Imitation Learning with Vision-Tactile/Force Sensing\". The link of the recorded video is ",{"type":13,"tag":32,"props":433,"children":436},{"href":434,"rel":435},"https://www.techbeat.net/talk-info?id=963",[36],[437],{"type":23,"value":438},"here",{"type":23,"value":208},{"type":13,"tag":394,"props":441,"children":442},{},[443,445,452,454,460],{"type":23,"value":444},"[Dec. 2024] Invited talk @ ",{"type":13,"tag":32,"props":446,"children":449},{"href":447,"rel":448},"https://www.roscon.cn/2024/index.html",[36],[450],{"type":23,"value":451},"ROSCon China 2024",{"type":23,"value":453}," workshop on \"Development Trends and Challenges in Embodied AI\". The link of the recorded video is ",{"type":13,"tag":32,"props":455,"children":458},{"href":456,"rel":457},"https://www.bilibili.com/video/BV1Z3cteAEnD/?spm_id_from=333.337.search-card.all.click&vd_source=8e062051896958b92b4759e0f4753657",[36],[459],{"type":23,"value":438},{"type":23,"value":208},{"type":13,"tag":394,"props":462,"children":463},{},[464,466,472],{"type":23,"value":465},"[Oct. 2024] Invited talk @ ",{"type":13,"tag":32,"props":467,"children":469},{"href":107,"rel":468},[36],[470],{"type":23,"value":471},"TEA lab",{"type":23,"value":473}," in Tsinghua University, IIIS on \"Efficient Learning for Long-horizon Deformable Object Manipulation\"",{"type":13,"tag":18,"props":475,"children":477},{"id":476},"awards",[478],{"type":23,"value":479},"üèÜ Awards",{"type":13,"tag":390,"props":481,"children":482},{},[483,488,493,498,503,508,513,518,523],{"type":13,"tag":394,"props":484,"children":485},{},[486],{"type":23,"value":487},"Best Student Paper Award Finalist [ÊúÄ‰Ω≥Â≠¶ÁîüËÆ∫ÊñáÂ•ñÊèêÂêç] in RSS 2025.",{"type":13,"tag":394,"props":489,"children":490},{},[491],{"type":23,"value":492},"Wu Wen Jun Scholarship [Âê¥Êñá‰øäÂ•ñÂ≠¶Èáë] in 2024 and 2025.",{"type":13,"tag":394,"props":494,"children":495},{},[496],{"type":23,"value":497},"Outstanding Graduates in Shanghai (Top 3%) [‰∏äÊµ∑Â∏Ç‰ºòÁßÄÊØï‰∏öÁîü] in 2021.",{"type":13,"tag":394,"props":499,"children":500},{},[501],{"type":23,"value":502},"Rongchang Technology Innovation Scholarship (Top 10 students in SJTU) [Ëç£Êò∂ÁßëÊäÄÂàõÊñ∞Â•ñÂ≠¶Èáë] in 2020.",{"type":13,"tag":394,"props":504,"children":505},{},[506],{"type":23,"value":507},"SenseTime Scholarship (Top 21 undergraduates in China) [ÂïÜÊ±§Â•ñÂ≠¶Èáë] in 2020.",{"type":13,"tag":394,"props":509,"children":510},{},[511],{"type":23,"value":512},"National Scholarship (Top 3 students in CS Department) in 2017, 2018 and 2019 (three consecutive years) [ÂõΩÂÆ∂Â•ñÂ≠¶ÈáëÔºàËøûÁª≠‰∏âÂπ¥Ôºâ].",{"type":13,"tag":394,"props":514,"children":515},{},[516],{"type":23,"value":517},"Academic Excellence Scholarship (Class A) of SJTU (Top 1% in SJTU) in 2018.",{"type":13,"tag":394,"props":519,"children":520},{},[521],{"type":23,"value":522},"Meritorious Winner Prize of Mathematical Contest in Modeling in 2018.",{"type":13,"tag":394,"props":524,"children":525},{},[526],{"type":23,"value":527},"1st Prize in China Undergraduate Mathematical Contest in Modeling (Shanghai Division) in 2017.",{"type":13,"tag":18,"props":529,"children":531},{"id":530},"find-me",[532],{"type":23,"value":533},"üìß Find Me",{"type":13,"tag":535,"props":536,"children":539},"contact-item",{"icon":537,"url":538},"email","mailto:xiaoxiaoxh@sjtu.edu.cn",[540],{"type":13,"tag":26,"props":541,"children":542},{},[543],{"type":23,"value":544},"Email",{"type":13,"tag":535,"props":546,"children":549},{"icon":547,"url":548},"github","https://github.com/xiaoxiaoxh",[550],{"type":13,"tag":26,"props":551,"children":552},{},[553],{"type":23,"value":554},"GitHub",{"type":13,"tag":535,"props":556,"children":559},{"icon":557,"url":558},"twitter","https://twitter.com/HanXue012",[560],{"type":13,"tag":26,"props":561,"children":562},{},[563],{"type":23,"value":564},"Twitter",{"title":5,"searchDepth":566,"depth":566,"links":567},2,[568,569,570,571,572,573,574],{"id":20,"depth":566,"text":24},{"id":116,"depth":566,"text":119},{"id":135,"depth":566,"text":138},{"id":241,"depth":566,"text":244},{"id":385,"depth":566,"text":388},{"id":476,"depth":566,"text":479},{"id":530,"depth":566,"text":533},"markdown","content:index.md","content","index.md","md",{"_path":581,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":582,"description":5,"leadingImage":583,"disableFancyImage":8,"body":584,"_type":575,"_id":784,"_source":577,"_file":785,"_extension":579},"/news","News","me-news-google.png",{"type":10,"children":585,"toc":782},[586,591,669,690],{"type":13,"tag":587,"props":588,"children":590},"MarkdownHeader",{"subtitle":589,"title":582},"üì¢ Latest: One paper (DeformPAM) is accepted by ICRA 2025!",[],{"type":13,"tag":390,"props":592,"children":593},{},[594,620,651],{"type":13,"tag":394,"props":595,"children":596},{},[597,602,604,611,613,618],{"type":13,"tag":148,"props":598,"children":599},{},[600],{"type":23,"value":601},"06/18/2025",{"type":23,"value":603}," üî• ",{"type":13,"tag":32,"props":605,"children":608},{"href":606,"rel":607},"https://reactive-diffusion-policy.github.io/",[36],[609],{"type":23,"value":610},"RDP",{"type":23,"value":612}," is selected as the ",{"type":13,"tag":148,"props":614,"children":615},{},[616],{"type":23,"value":617},"Best Student Paper Award Finalist",{"type":23,"value":619}," @ RSS 2025!",{"type":13,"tag":394,"props":621,"children":622},{},[623,628,629,634,635,640,642,649],{"type":13,"tag":148,"props":624,"children":625},{},[626],{"type":23,"value":627},"05/23/2025",{"type":23,"value":603},{"type":13,"tag":32,"props":630,"children":632},{"href":606,"rel":631},[36],[633],{"type":23,"value":610},{"type":23,"value":612},{"type":13,"tag":148,"props":636,"children":637},{},[638],{"type":23,"value":639},"Best Paper",{"type":23,"value":641}," in ",{"type":13,"tag":32,"props":643,"children":646},{"href":644,"rel":645},"https://sites.google.com/view/icra-2025-beyond-pick-place/home",[36],[647],{"type":23,"value":648},"Beyond Pick and Place workshop",{"type":23,"value":650}," @ ICRA 2025!",{"type":13,"tag":394,"props":652,"children":653},{},[654,659,661,667],{"type":13,"tag":148,"props":655,"children":656},{},[657],{"type":23,"value":658},"04/11/2025",{"type":23,"value":660}," üéâ One paper (",{"type":13,"tag":32,"props":662,"children":664},{"href":606,"rel":663},[36],[665],{"type":23,"value":666},"Reactive Diffusion Policy",{"type":23,"value":668},") is accepted by RSS 2025!",{"type":13,"tag":390,"props":670,"children":671},{},[672],{"type":13,"tag":394,"props":673,"children":674},{},[675,680,681,688],{"type":13,"tag":148,"props":676,"children":677},{},[678],{"type":23,"value":679},"01/29/2025",{"type":23,"value":660},{"type":13,"tag":32,"props":682,"children":685},{"href":683,"rel":684},"https://deform-pam.robotflow.ai/",[36],[686],{"type":23,"value":687},"DeformPAM",{"type":23,"value":689},") is accepted by ICRA 2025!",{"type":13,"tag":390,"props":691,"children":692},{},[693,711,737,755],{"type":13,"tag":394,"props":694,"children":695},{},[696,701,702,709],{"type":13,"tag":148,"props":697,"children":698},{},[699],{"type":23,"value":700},"08/31/2023",{"type":23,"value":660},{"type":13,"tag":32,"props":703,"children":706},{"href":704,"rel":705},"https://unifolding.robotflow.ai/",[36],[707],{"type":23,"value":708},"UniFolding",{"type":23,"value":710},") is accepted by CoRL 2023!",{"type":13,"tag":394,"props":712,"children":713},{},[714,719,721,728,730,735],{"type":13,"tag":148,"props":715,"children":716},{},[717],{"type":23,"value":718},"07/14/2023",{"type":23,"value":720}," üî• One paper (",{"type":13,"tag":32,"props":722,"children":725},{"href":723,"rel":724},"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf",[36],[726],{"type":23,"value":727},"ClothPose",{"type":23,"value":729},") is accepted by ICCV 2023 as an ",{"type":13,"tag":148,"props":731,"children":732},{},[733],{"type":23,"value":734},"oral presentation",{"type":23,"value":736},"!",{"type":13,"tag":394,"props":738,"children":739},{},[740,745,746,753],{"type":13,"tag":148,"props":741,"children":742},{},[743],{"type":23,"value":744},"05/13/2023",{"type":23,"value":660},{"type":13,"tag":32,"props":747,"children":750},{"href":748,"rel":749},"https://sites.google.com/view/rfuniverse",[36],[751],{"type":23,"value":752},"RFUniverse",{"type":23,"value":754},") is accepted by RSS 2023!",{"type":13,"tag":394,"props":756,"children":757},{},[758,763,765,772,773,780],{"type":13,"tag":148,"props":759,"children":760},{},[761],{"type":23,"value":762},"02/28/2023",{"type":23,"value":764}," üéâ Two papers (",{"type":13,"tag":32,"props":766,"children":769},{"href":767,"rel":768},"https://garment-tracking.robotflow.ai/",[36],[770],{"type":23,"value":771},"GarmentTracking",{"type":23,"value":95},{"type":13,"tag":32,"props":774,"children":777},{"href":775,"rel":776},"https://sites.google.com/view/vtaco/",[36],[778],{"type":23,"value":779},"VTaCo",{"type":23,"value":781},") are accepted by CVPR 2023!",{"title":5,"searchDepth":566,"depth":566,"links":783},[],"content:news.md","news.md",1753853682067]