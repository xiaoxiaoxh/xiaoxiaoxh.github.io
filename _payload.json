[{"data":1,"prerenderedAt":652},["Reactive",2],{"content-query-1DxZ1vYQk5":3,"content-query-B6mqoO8PxR":468},{"_path":4,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":7,"description":5,"hideTitle":8,"disableFancyImage":8,"body":9,"_type":463,"_id":464,"_source":465,"_file":466,"_extension":467},"/","",false,"Home",true,{"type":10,"children":11,"toc":453},"root",[12,17,25,59,64,96,102,106,115,121,162,195,215,221,231,239,259,267,274,282,291,299,307,316,325,331,372,378,416,422,433,443],{"type":13,"tag":14,"props":15,"children":16},"element","IndexHeader",{},[],{"type":13,"tag":18,"props":19,"children":21},"h2",{"id":20},"Ô∏è-about-me",[22],{"type":23,"value":24},"text","ü¶∏üèª‚Äç‚ôÇÔ∏è About Me",{"type":13,"tag":26,"props":27,"children":28},"p",{},[29,31,40,42,49,51,57],{"type":23,"value":30},"I am a third-year Computer Science Ph.D. candidate at ",{"type":13,"tag":32,"props":33,"children":37},"a",{"href":34,"rel":35},"https://en.sjtu.edu.cn/",[36],"nofollow",[38],{"type":23,"value":39},"Shanghai Jiao Tong University",{"type":23,"value":41}," and a member of ",{"type":13,"tag":32,"props":43,"children":46},{"href":44,"rel":45},"https://www.mvig.org/",[36],[47],{"type":23,"value":48},"Machine Intelligence and Vision Group (MVIG)",{"type":23,"value":50}," under the supervision of ",{"type":13,"tag":32,"props":52,"children":54},{"href":44,"rel":53},[36],[55],{"type":23,"value":56},"Prof. Cewu Lu",{"type":23,"value":58},".",{"type":13,"tag":26,"props":60,"children":61},{},[62],{"type":23,"value":63},"I receive my bachelor degree from Shanghai Jiao Tong Universiy in 2021. My research interests lie in Robotics and 3D Vision. Previously, I have been working on deformable object perception and manipulation. Now I am particularly interested in tactile sensing and bimanual manipulation.",{"type":13,"tag":26,"props":65,"children":66},{},[67,69,76,78,85,87,94],{"type":23,"value":68},"In the past, I have interned at ",{"type":13,"tag":32,"props":70,"children":73},{"href":71,"rel":72},"https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/",[36],[74],{"type":23,"value":75},"Microsoft Research Asia",{"type":23,"value":77}," under the supervsion of ",{"type":13,"tag":32,"props":79,"children":82},{"href":80,"rel":81},"https://ancientmooner.github.io/",[36],[83],{"type":23,"value":84},"Han Hu",{"type":23,"value":86}," and ",{"type":13,"tag":32,"props":88,"children":91},{"href":89,"rel":90},"http://yue-cao.me/",[36],[92],{"type":23,"value":93},"Yue Cao",{"type":23,"value":95},". In my spare time, I enjoy watching movies and playing with robotsü§ñ.",{"type":13,"tag":18,"props":97,"children":99},{"id":98},"news",[100],{"type":23,"value":101},"üì∞ News",{"type":13,"tag":103,"props":104,"children":105},"ShortNews",{},[],{"type":13,"tag":26,"props":107,"children":108},{},[109],{"type":13,"tag":32,"props":110,"children":112},{"href":111},"/news/",[113],{"type":23,"value":114},"More news >>>",{"type":13,"tag":18,"props":116,"children":118},{"id":117},"experiences",[119],{"type":23,"value":120},"üè´ Experiences",{"type":13,"tag":122,"props":123,"children":125},"ExperienceRow",{"icon":124},"sjtu.png",[126],{"type":13,"tag":26,"props":127,"children":128},{},[129,134,138,140,143,145,150,152,157,160],{"type":13,"tag":130,"props":131,"children":132},"strong",{},[133],{"type":23,"value":39},{"type":13,"tag":135,"props":136,"children":137},"br",{},[],{"type":23,"value":139},"\nPh.D. Student / Master Student.",{"type":13,"tag":135,"props":141,"children":142},{},[],{"type":23,"value":144},"\nResearch assistant in ",{"type":13,"tag":32,"props":146,"children":148},{"href":44,"rel":147},[36],[149],{"type":23,"value":48},{"type":23,"value":151},", advised by ",{"type":13,"tag":32,"props":153,"children":155},{"href":44,"rel":154},[36],[156],{"type":23,"value":56},{"type":13,"tag":135,"props":158,"children":159},{},[],{"type":23,"value":161},"\nSep. 2021 - Present",{"type":13,"tag":122,"props":163,"children":165},{"icon":164},"microsoft.png",[166],{"type":13,"tag":26,"props":167,"children":168},{},[169,173,176,178,183,184,189,190,193],{"type":13,"tag":130,"props":170,"children":171},{},[172],{"type":23,"value":75},{"type":13,"tag":135,"props":174,"children":175},{},[],{"type":23,"value":177},"\nResearch Intern, advised by ",{"type":13,"tag":32,"props":179,"children":181},{"href":80,"rel":180},[36],[182],{"type":23,"value":84},{"type":23,"value":86},{"type":13,"tag":32,"props":185,"children":187},{"href":89,"rel":186},[36],[188],{"type":23,"value":93},{"type":23,"value":58},{"type":13,"tag":135,"props":191,"children":192},{},[],{"type":23,"value":194},"\nJul. 2019 - Mar. 2020",{"type":13,"tag":122,"props":196,"children":197},{"icon":124},[198],{"type":13,"tag":26,"props":199,"children":200},{},[201,205,208,210,213],{"type":13,"tag":130,"props":202,"children":203},{},[204],{"type":23,"value":39},{"type":13,"tag":135,"props":206,"children":207},{},[],{"type":23,"value":209},"\nBachelor of Engineering in Computer Science. ",{"type":13,"tag":135,"props":211,"children":212},{},[],{"type":23,"value":214},"\nSep 2016 - Jun. 2021",{"type":13,"tag":18,"props":216,"children":218},{"id":217},"selected-publications",[219],{"type":23,"value":220},"üìÑ Selected Publications",{"type":13,"tag":222,"props":223,"children":230},"PublicationRow",{":artifactLinks":224,":authors":225,":venue":226,"thumbnail":227,"title":228,"type":229},"{\"Website\":\"https://deform-pam.robotflow.ai/\",\"arXiv\":\"https://arxiv.org/abs/2410.11584\",\"Code\":\"https://github.com/xiaoxiaoxh/DeformPAM\"}","[\"Wendi Chen*\",\"Han Xue*\",\"Fangyuan Zhou\",\"Yuan Fang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICRA\",\"year\":2025,\"name\":\"IEEE International Conference on Robotics and Automation (ICRA)\"}","deform-pam.gif","DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object Manipulation via Preference-based Action Alignment","conference",[],{"type":13,"tag":222,"props":232,"children":238},{":artifactLinks":233,":authors":234,":venue":235,"thumbnail":236,"title":237,"type":229},"{\"Proceeding\":\"https://openreview.net/pdf?id=ANJuNDFdvP\",\"arXiv\":\"https://arxiv.org/abs/2311.01267\",\"Code\":\"https://github.com/xiaoxiaoxh/UniFolding\",\"Website\":\"https://unifolding.robotflow.ai/\"}","[\"Han Xue*\",\"Yutong Li*\",\"Wenqiang Xu\",\"Huanyu Li\",\"Dongzhe Zheng\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"CoRL\",\"year\":2023,\"name\":\"7th Annual Conference on Robot Learning.\"}","unifolding.gif","UniFolding: Towards Sample-efficient, Scalable, and Generalizable Robotic Garment Folding",[],{"type":13,"tag":222,"props":240,"children":246},{":artifactLinks":241,":authors":242,":venue":243,"thumbnail":244,"title":245,"type":229},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf\"}","[\"Wenqiang Xu*\",\"Wenxin Du*\",\"Han Xue\",\"Yutong Li\",\"Ruolin Ye\",\"Yan-Feng Wang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICCV\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF International Conference on Computer Vision\"}","clothpose.png","ClothPose: A Real-world Benchmark for Visual Analysis of Garment Pose via An Indirect Recording Solution",[247],{"type":13,"tag":26,"props":248,"children":249},{},[250],{"type":13,"tag":251,"props":252,"children":256},"span",{"className":253},[254,255],"text-red-600","font-bold",[257],{"type":23,"value":258},"üî• Oral Presentation.",{"type":13,"tag":222,"props":260,"children":266},{":artifactLinks":261,":authors":262,":venue":263,"thumbnail":264,"title":265,"type":229},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_GarmentTracking_Category-Level_Garment_Pose_Tracking_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.13913.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/GarmentTracking\",\"Website\":\"https://garment-tracking.robotflow.ai/\"}","[\"Han Xue\",\"Wenqiang Xu\",\"Jieyi Zhang\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Ruolin Ye\",\"Cewu Lu\"]","{\"acronym\":\"CVPR\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\"}","garment-tracking.gif","GarmentTracking: Category-Level Garment Pose Tracking",[],{"type":13,"tag":222,"props":268,"children":273},{":artifactLinks":269,":authors":270,":venue":263,"thumbnail":271,"title":272,"type":229},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Visual-Tactile_Sensing_for_In-Hand_Object_Reconstruction_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.14498.pdf\",\"Code\":\"https://github.com/jeffsonyu/VTacO\",\"Website\":\"https://sites.google.com/view/vtaco/\"}","[\"Wenqiang Xu*\",\"Zhenjun Yu*\",\"Han Xue\",\"Ruolin Ye\",\"Siqiong Yao\",\"Cewu Lu (*Equal contribution)\"]","vtaco.png","Visual-Tactile Sensing for In-Hand Object Reconstruction",[],{"type":13,"tag":222,"props":275,"children":281},{":artifactLinks":276,":authors":277,":venue":278,"thumbnail":279,"title":280,"type":229},"{\"Proceeding\":\"https://www.roboticsproceedings.org/rss19/p087.pdf\",\"Code\":\"https://github.com/mvig-robotflow/pyrfuniverse\",\"Website\":\"https://sites.google.com/view/rfuniverse\"}","[\"Haoyuan Fu*\",\"Wenqiang Xu*\",\"Ruolin Ye*\",\"Han Xue\",\"Zhenjun Yu\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Jieyi Zhang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"RSS\",\"year\":2023,\"name\":\"Robotics: Science and Systems.\"}","rfuniverse.png","Demonstrating RFUniverse: A Multiphysics Simulation Platform for Embodied AI",[],{"type":13,"tag":222,"props":283,"children":290},{":artifactLinks":284,":authors":285,":venue":286,"thumbnail":287,"title":288,"type":289},"{\"arXiv\":\"https://arxiv.org/pdf/2105.03260\"}","[\"Liu Liu*\",\"Han Xue*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"TIP\",\"year\":2022,\"name\":\"IEEE Transactions on Image Processing.\"}","articulation_real.png","Toward Real-World Category-Level Articulation Pose Estimation","journal",[],{"type":13,"tag":222,"props":292,"children":298},{":artifactLinks":293,":authors":294,":venue":295,"thumbnail":296,"title":297,"type":229},"{\"Proceeding\":\"https://www.bmvc2021-virtualconference.com/assets/papers/0544.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2112.07334.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/OMAD\"}","[\"Han Xue*\",\"Liu Liu*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"BMVC\",\"year\":2021,\"name\":\"The 32nd British Machine Vision Conference.\"}","omad.png","OMAD: Object Model with Articulated Deformations for Pose Estimation and Retrieval",[],{"type":13,"tag":222,"props":300,"children":306},{":artifactLinks":301,":authors":302,":venue":303,"thumbnail":304,"title":305,"type":229},"{\"Proceeding\":\"https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660222.pdf\",\"arXiv\":\"https://arxiv.org/pdf/1912.11473.pdf\",\"Code\":\"https://github.com/justimyhxu/Dense-RepPoints\"}","[\"Ze Yang*\",\"Yinghao Xu*\",\"Han Xue*\",\"Zheng Zhang\",\"Raquel Urtasun\",\"Liwei Wang\",\"Stephen Lin\",\"Han Hu (*Equal contribution)\"]","{\"acronym\":\"ECCV\",\"year\":2020,\"name\":\"The European Conference on Computer Vision.\"}","dense_reppoints.jpg","Dense RepPoints: Representing Visual Objects with Dense Point Sets",[],{"type":13,"tag":222,"props":308,"children":315},{":artifactLinks":309,":authors":310,":venue":311,"thumbnail":312,"title":313,"type":229,":hideBottomBorder":314},"{\"Proceeding\":\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf\",\"Code\":\"https://github.com/driving-behavior/DBNet\",\"Website\":\"http://www.dbehavior.net/\"}","[\"Yiping Chen*\",\"Jingkang Wang*\",\"Jonathan Li\",\"Cewu Lu\",\"Zhipeng Luo\",\"Han Xue\",\"Cheng Wang (*Equal contribution)\"]","{\"acronym\":\"CVPR\",\"year\":2018,\"name\":\"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\"}","dbnet.jpg","Lidar-video driving dataset: Learning driving policies effectively","true",[],{"type":13,"tag":26,"props":317,"children":318},{},[319],{"type":13,"tag":32,"props":320,"children":322},{"href":321},"/publication/",[323],{"type":23,"value":324},"Full publication list >>>",{"type":13,"tag":18,"props":326,"children":328},{"id":327},"talks",[329],{"type":23,"value":330},"‚ú® Talks",{"type":13,"tag":332,"props":333,"children":334},"ul",{},[335,350],{"type":13,"tag":336,"props":337,"children":338},"li",{},[339,341,348],{"type":23,"value":340},"[Oct. 2024] Invited talk @ ",{"type":13,"tag":32,"props":342,"children":345},{"href":343,"rel":344},"http://hxu.rocks/index.html",[36],[346],{"type":23,"value":347},"TEA lab",{"type":23,"value":349}," in Tsinghua University, IIIS on \"Efficient Learning for Long-horizon Deformable Object Manipulation\"",{"type":13,"tag":336,"props":351,"children":352},{},[353,355,362,364,371],{"type":23,"value":354},"[Dec. 2024] Invited talk @ ",{"type":13,"tag":32,"props":356,"children":359},{"href":357,"rel":358},"https://www.roscon.cn/2024/index.html",[36],[360],{"type":23,"value":361},"ROSCon China 2024",{"type":23,"value":363}," workshop on \"Development Trends and Challenges in Embodied AI\". The link of the recorded video is ",{"type":13,"tag":32,"props":365,"children":368},{"href":366,"rel":367},"https://www.bilibili.com/video/BV1Z3cteAEnD/?spm_id_from=333.337.search-card.all.click&vd_source=8e062051896958b92b4759e0f4753657",[36],[369],{"type":23,"value":370},"here",{"type":23,"value":58},{"type":13,"tag":18,"props":373,"children":375},{"id":374},"awards",[376],{"type":23,"value":377},"üèÜ Awards",{"type":13,"tag":332,"props":379,"children":380},{},[381,386,391,396,401,406,411],{"type":13,"tag":336,"props":382,"children":383},{},[384],{"type":23,"value":385},"Outstanding Graduates in Shanghai (Top 3%) in 2021.",{"type":13,"tag":336,"props":387,"children":388},{},[389],{"type":23,"value":390},"Rongchang Technology Innovation Scholarship (Top 10 students in SJTU) in 2020.",{"type":13,"tag":336,"props":392,"children":393},{},[394],{"type":23,"value":395},"SenseTime Scholarship (Top 21 undergraduates in China) in 2020.",{"type":13,"tag":336,"props":397,"children":398},{},[399],{"type":23,"value":400},"National Scholarship (Top 3 students in CS Department) in 2017, 2018 and 2019 (three consecutive years).",{"type":13,"tag":336,"props":402,"children":403},{},[404],{"type":23,"value":405},"Academic Excellence Scholarship (Class A) of SJTU (Top 1% in SJTU) in 2018.",{"type":13,"tag":336,"props":407,"children":408},{},[409],{"type":23,"value":410},"Meritorious Winner Prize of Mathematical Contest in Modeling in 2018.",{"type":13,"tag":336,"props":412,"children":413},{},[414],{"type":23,"value":415},"1st Prize in China Undergraduate Mathematical Contest in Modeling (Shanghai Division) in 2017.",{"type":13,"tag":18,"props":417,"children":419},{"id":418},"find-me",[420],{"type":23,"value":421},"üìß Find Me",{"type":13,"tag":423,"props":424,"children":427},"contact-item",{"icon":425,"url":426},"email","mailto:xiaoxiaoxh@sjtu.edu.cn",[428],{"type":13,"tag":26,"props":429,"children":430},{},[431],{"type":23,"value":432},"Email",{"type":13,"tag":423,"props":434,"children":437},{"icon":435,"url":436},"github","https://github.com/xiaoxiaoxh",[438],{"type":13,"tag":26,"props":439,"children":440},{},[441],{"type":23,"value":442},"GitHub",{"type":13,"tag":423,"props":444,"children":447},{"icon":445,"url":446},"twitter","https://twitter.com/HanXue012",[448],{"type":13,"tag":26,"props":449,"children":450},{},[451],{"type":23,"value":452},"Twitter",{"title":5,"searchDepth":454,"depth":454,"links":455},2,[456,457,458,459,460,461,462],{"id":20,"depth":454,"text":24},{"id":98,"depth":454,"text":101},{"id":117,"depth":454,"text":120},{"id":217,"depth":454,"text":220},{"id":327,"depth":454,"text":330},{"id":374,"depth":454,"text":377},{"id":418,"depth":454,"text":421},"markdown","content:index.md","content","index.md","md",{"_path":469,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":470,"description":5,"leadingImage":471,"disableFancyImage":8,"body":472,"_type":463,"_id":650,"_source":465,"_file":651,"_extension":467},"/news","News","me-news-google.png",{"type":10,"children":473,"toc":648},[474,479],{"type":13,"tag":475,"props":476,"children":478},"MarkdownHeader",{"subtitle":477,"title":470},"üì¢ Latest: I'm a Ph.D. candidate now!",[],{"type":13,"tag":332,"props":480,"children":481},{},[482,501,526,551,561,577,603,621],{"type":13,"tag":336,"props":483,"children":484},{},[485,490,492,499],{"type":13,"tag":130,"props":486,"children":487},{},[488],{"type":23,"value":489},"01/29/2025",{"type":23,"value":491}," üéâ One paper (",{"type":13,"tag":32,"props":493,"children":496},{"href":494,"rel":495},"https://deform-pam.robotflow.ai/",[36],[497],{"type":23,"value":498},"DeformPAM",{"type":23,"value":500},") is accepted by ICRA 2025!",{"type":13,"tag":336,"props":502,"children":503},{},[504,509,511,518,520,525],{"type":13,"tag":130,"props":505,"children":506},{},[507],{"type":23,"value":508},"10/14/2024",{"type":23,"value":510}," üñ•Ô∏è We have released the ",{"type":13,"tag":32,"props":512,"children":515},{"href":513,"rel":514},"https://github.com/xiaoxiaoxh/DeformPAM",[36],[516],{"type":23,"value":517},"code",{"type":23,"value":519}," of ",{"type":13,"tag":32,"props":521,"children":523},{"href":494,"rel":522},[36],[524],{"type":23,"value":498},{"type":23,"value":58},{"type":13,"tag":336,"props":527,"children":528},{},[529,534,535,541,543,550],{"type":13,"tag":130,"props":530,"children":531},{},[532],{"type":23,"value":533},"11/05/2023",{"type":23,"value":510},{"type":13,"tag":32,"props":536,"children":539},{"href":537,"rel":538},"https://github.com/xiaoxiaoxh/UniFolding",[36],[540],{"type":23,"value":517},{"type":23,"value":542}," of our CoRL 2023 paper ",{"type":13,"tag":32,"props":544,"children":547},{"href":545,"rel":546},"https://unifolding.robotflow.ai/",[36],[548],{"type":23,"value":549},"UniFolding",{"type":23,"value":58},{"type":13,"tag":336,"props":552,"children":553},{},[554,559],{"type":13,"tag":130,"props":555,"children":556},{},[557],{"type":23,"value":558},"10/31/2023",{"type":23,"value":560}," üéì Passed my classes and research qualifications, I'm a Ph.D. candidate now!",{"type":13,"tag":336,"props":562,"children":563},{},[564,569,570,575],{"type":13,"tag":130,"props":565,"children":566},{},[567],{"type":23,"value":568},"08/31/2023",{"type":23,"value":491},{"type":13,"tag":32,"props":571,"children":573},{"href":545,"rel":572},[36],[574],{"type":23,"value":549},{"type":23,"value":576},") is accepted by CoRL 2023!",{"type":13,"tag":336,"props":578,"children":579},{},[580,585,587,594,596,601],{"type":13,"tag":130,"props":581,"children":582},{},[583],{"type":23,"value":584},"07/14/2023",{"type":23,"value":586}," üî• One paper (",{"type":13,"tag":32,"props":588,"children":591},{"href":589,"rel":590},"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf",[36],[592],{"type":23,"value":593},"ClothPose",{"type":23,"value":595},") is accepted by ICCV 2023 as an ",{"type":13,"tag":130,"props":597,"children":598},{},[599],{"type":23,"value":600},"oral presentation",{"type":23,"value":602},"!",{"type":13,"tag":336,"props":604,"children":605},{},[606,611,612,619],{"type":13,"tag":130,"props":607,"children":608},{},[609],{"type":23,"value":610},"05/13/2023",{"type":23,"value":491},{"type":13,"tag":32,"props":613,"children":616},{"href":614,"rel":615},"https://sites.google.com/view/rfuniverse",[36],[617],{"type":23,"value":618},"RFUniverse",{"type":23,"value":620},") is accepted by RSS 2023!",{"type":13,"tag":336,"props":622,"children":623},{},[624,629,631,638,639,646],{"type":13,"tag":130,"props":625,"children":626},{},[627],{"type":23,"value":628},"02/28/2023",{"type":23,"value":630}," üéâ Two papers (",{"type":13,"tag":32,"props":632,"children":635},{"href":633,"rel":634},"https://garment-tracking.robotflow.ai/",[36],[636],{"type":23,"value":637},"GarmentTracking",{"type":23,"value":86},{"type":13,"tag":32,"props":640,"children":643},{"href":641,"rel":642},"https://sites.google.com/view/vtaco/",[36],[644],{"type":23,"value":645},"VTaCo",{"type":23,"value":647},") are accepted by CVPR 2023!",{"title":5,"searchDepth":454,"depth":454,"links":649},[],"content:news.md","news.md",1740289188525]