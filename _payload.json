[{"data":1,"prerenderedAt":645},["Reactive",2],{"content-query-1DxZ1vYQk5":3,"content-query-B6mqoO8PxR":491},{"_path":4,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":7,"description":5,"hideTitle":8,"disableFancyImage":8,"body":9,"_type":486,"_id":487,"_source":488,"_file":489,"_extension":490},"/","",false,"Home",true,{"type":10,"children":11,"toc":476},"root",[12,17,25,68,73,114,120,124,133,139,180,214,239,245,255,263,283,291,298,306,315,323,331,340,349,355,395,401,439,445,456,466],{"type":13,"tag":14,"props":15,"children":16},"element","IndexHeader",{},[],{"type":13,"tag":18,"props":19,"children":21},"h2",{"id":20},"Ô∏è-about-me",[22],{"type":23,"value":24},"text","ü¶∏üèª‚Äç‚ôÇÔ∏è About Me",{"type":13,"tag":26,"props":27,"children":28},"p",{},[29,31,40,42,49,51,57,59,66],{"type":23,"value":30},"I am a third-year Computer Science Ph.D. candidate at ",{"type":13,"tag":32,"props":33,"children":37},"a",{"href":34,"rel":35},"https://en.sjtu.edu.cn/",[36],"nofollow",[38],{"type":23,"value":39},"Shanghai Jiao Tong University",{"type":23,"value":41}," and a member of ",{"type":13,"tag":32,"props":43,"children":46},{"href":44,"rel":45},"https://www.mvig.org/",[36],[47],{"type":23,"value":48},"Machine Intelligence and Vision Group (MVIG)",{"type":23,"value":50}," under the supervision of ",{"type":13,"tag":32,"props":52,"children":54},{"href":44,"rel":53},[36],[55],{"type":23,"value":56},"Prof. Cewu Lu",{"type":23,"value":58}," (Âç¢Á≠ñÂêæ). I am also a member of ",{"type":13,"tag":32,"props":60,"children":63},{"href":61,"rel":62},"https://ai.sjtu.edu.cn/info/announcements/204",[36],[64],{"type":23,"value":65},"Wu Wen Jun Honorary Doctoral Program",{"type":23,"value":67}," (Âê¥Êñá‰øäËç£Ë™âÂçöÂ£´Áè≠).",{"type":13,"tag":26,"props":69,"children":70},{},[71],{"type":23,"value":72},"I receive my bachelor degree from Shanghai Jiao Tong Universiy in 2021. My research interests lie in Robotics and 3D Vision. Previously, I have been working on deformable object perception and manipulation. Now I am particularly interested in imitation learning with tactile/force sensing and low-cost data collection system.",{"type":13,"tag":26,"props":74,"children":75},{},[76,78,85,87,94,96,103,105,112],{"type":23,"value":77},"In the past, I have interned at ",{"type":13,"tag":32,"props":79,"children":82},{"href":80,"rel":81},"https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/",[36],[83],{"type":23,"value":84},"Microsoft Research Asia",{"type":23,"value":86}," under the supervsion of ",{"type":13,"tag":32,"props":88,"children":91},{"href":89,"rel":90},"https://ancientmooner.github.io/",[36],[92],{"type":23,"value":93},"Han Hu",{"type":23,"value":95}," and ",{"type":13,"tag":32,"props":97,"children":100},{"href":98,"rel":99},"http://yue-cao.me/",[36],[101],{"type":23,"value":102},"Yue Cao",{"type":23,"value":104},". I also spent time with Prof. ",{"type":13,"tag":32,"props":106,"children":109},{"href":107,"rel":108},"http://hxu.rocks/index.html",[36],[110],{"type":23,"value":111},"Huazhe Xu",{"type":23,"value":113}," at Tsinghua University on my projects. In my spare time, I enjoy watching movies and playing with robotsü§ñ.",{"type":13,"tag":18,"props":115,"children":117},{"id":116},"news",[118],{"type":23,"value":119},"üì∞ News",{"type":13,"tag":121,"props":122,"children":123},"ShortNews",{},[],{"type":13,"tag":26,"props":125,"children":126},{},[127],{"type":13,"tag":32,"props":128,"children":130},{"href":129},"/news/",[131],{"type":23,"value":132},"More news >>>",{"type":13,"tag":18,"props":134,"children":136},{"id":135},"experiences",[137],{"type":23,"value":138},"üè´ Experiences",{"type":13,"tag":140,"props":141,"children":143},"ExperienceRow",{"icon":142},"sjtu.png",[144],{"type":13,"tag":26,"props":145,"children":146},{},[147,152,156,158,161,163,168,170,175,178],{"type":13,"tag":148,"props":149,"children":150},"strong",{},[151],{"type":23,"value":39},{"type":13,"tag":153,"props":154,"children":155},"br",{},[],{"type":23,"value":157},"\nPh.D. Student / Master Student.",{"type":13,"tag":153,"props":159,"children":160},{},[],{"type":23,"value":162},"\nResearch assistant in ",{"type":13,"tag":32,"props":164,"children":166},{"href":44,"rel":165},[36],[167],{"type":23,"value":48},{"type":23,"value":169},", advised by ",{"type":13,"tag":32,"props":171,"children":173},{"href":44,"rel":172},[36],[174],{"type":23,"value":56},{"type":13,"tag":153,"props":176,"children":177},{},[],{"type":23,"value":179},"\nSep. 2021 - Present",{"type":13,"tag":140,"props":181,"children":183},{"icon":182},"microsoft.png",[184],{"type":13,"tag":26,"props":185,"children":186},{},[187,191,194,196,201,202,207,209,212],{"type":13,"tag":148,"props":188,"children":189},{},[190],{"type":23,"value":84},{"type":13,"tag":153,"props":192,"children":193},{},[],{"type":23,"value":195},"\nResearch Intern, advised by ",{"type":13,"tag":32,"props":197,"children":199},{"href":89,"rel":198},[36],[200],{"type":23,"value":93},{"type":23,"value":95},{"type":13,"tag":32,"props":203,"children":205},{"href":98,"rel":204},[36],[206],{"type":23,"value":102},{"type":23,"value":208},".",{"type":13,"tag":153,"props":210,"children":211},{},[],{"type":23,"value":213},"\nJul. 2019 - Mar. 2020",{"type":13,"tag":140,"props":215,"children":216},{"icon":142},[217],{"type":13,"tag":26,"props":218,"children":219},{},[220,224,227,229,232,234,237],{"type":13,"tag":148,"props":221,"children":222},{},[223],{"type":23,"value":39},{"type":13,"tag":153,"props":225,"children":226},{},[],{"type":23,"value":228},"\nBachelor of Engineering in Computer Science. ",{"type":13,"tag":153,"props":230,"children":231},{},[],{"type":23,"value":233},"\nGPA 4.04/4.3, Rank 3/150 (Top 2%)",{"type":13,"tag":153,"props":235,"children":236},{},[],{"type":23,"value":238},"\nSep 2016 - Jun. 2021",{"type":13,"tag":18,"props":240,"children":242},{"id":241},"selected-publications",[243],{"type":23,"value":244},"üìÑ Selected Publications",{"type":13,"tag":246,"props":247,"children":254},"PublicationRow",{":artifactLinks":248,":authors":249,":venue":250,"thumbnail":251,"title":252,"type":253},"{\"Website\":\"https://deform-pam.robotflow.ai/\",\"arXiv\":\"https://arxiv.org/abs/2410.11584\",\"Code\":\"https://github.com/xiaoxiaoxh/DeformPAM\"}","[\"Wendi Chen*\",\"Han Xue*\",\"Fangyuan Zhou\",\"Yuan Fang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICRA\",\"year\":2025,\"name\":\"IEEE International Conference on Robotics and Automation (ICRA)\"}","deform-pam.gif","DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object Manipulation via Preference-based Action Alignment","conference",[],{"type":13,"tag":246,"props":256,"children":262},{":artifactLinks":257,":authors":258,":venue":259,"thumbnail":260,"title":261,"type":253},"{\"Proceeding\":\"https://openreview.net/pdf?id=ANJuNDFdvP\",\"arXiv\":\"https://arxiv.org/abs/2311.01267\",\"Code\":\"https://github.com/xiaoxiaoxh/UniFolding\",\"Website\":\"https://unifolding.robotflow.ai/\"}","[\"Han Xue*\",\"Yutong Li*\",\"Wenqiang Xu\",\"Huanyu Li\",\"Dongzhe Zheng\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"CoRL\",\"year\":2023,\"name\":\"7th Annual Conference on Robot Learning.\"}","unifolding.gif","UniFolding: Towards Sample-efficient, Scalable, and Generalizable Robotic Garment Folding",[],{"type":13,"tag":246,"props":264,"children":270},{":artifactLinks":265,":authors":266,":venue":267,"thumbnail":268,"title":269,"type":253},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf\"}","[\"Wenqiang Xu*\",\"Wenxin Du*\",\"Han Xue\",\"Yutong Li\",\"Ruolin Ye\",\"Yan-Feng Wang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICCV\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF International Conference on Computer Vision\"}","clothpose.png","ClothPose: A Real-world Benchmark for Visual Analysis of Garment Pose via An Indirect Recording Solution",[271],{"type":13,"tag":26,"props":272,"children":273},{},[274],{"type":13,"tag":275,"props":276,"children":280},"span",{"className":277},[278,279],"text-red-600","font-bold",[281],{"type":23,"value":282},"üî• Oral Presentation.",{"type":13,"tag":246,"props":284,"children":290},{":artifactLinks":285,":authors":286,":venue":287,"thumbnail":288,"title":289,"type":253},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_GarmentTracking_Category-Level_Garment_Pose_Tracking_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.13913.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/GarmentTracking\",\"Website\":\"https://garment-tracking.robotflow.ai/\"}","[\"Han Xue\",\"Wenqiang Xu\",\"Jieyi Zhang\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Ruolin Ye\",\"Cewu Lu\"]","{\"acronym\":\"CVPR\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\"}","garment-tracking.gif","GarmentTracking: Category-Level Garment Pose Tracking",[],{"type":13,"tag":246,"props":292,"children":297},{":artifactLinks":293,":authors":294,":venue":287,"thumbnail":295,"title":296,"type":253},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Visual-Tactile_Sensing_for_In-Hand_Object_Reconstruction_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.14498.pdf\",\"Code\":\"https://github.com/jeffsonyu/VTacO\",\"Website\":\"https://sites.google.com/view/vtaco/\"}","[\"Wenqiang Xu*\",\"Zhenjun Yu*\",\"Han Xue\",\"Ruolin Ye\",\"Siqiong Yao\",\"Cewu Lu (*Equal contribution)\"]","vtaco.png","Visual-Tactile Sensing for In-Hand Object Reconstruction",[],{"type":13,"tag":246,"props":299,"children":305},{":artifactLinks":300,":authors":301,":venue":302,"thumbnail":303,"title":304,"type":253},"{\"Proceeding\":\"https://www.roboticsproceedings.org/rss19/p087.pdf\",\"Code\":\"https://github.com/mvig-robotflow/pyrfuniverse\",\"Website\":\"https://sites.google.com/view/rfuniverse\"}","[\"Haoyuan Fu*\",\"Wenqiang Xu*\",\"Ruolin Ye*\",\"Han Xue\",\"Zhenjun Yu\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Jieyi Zhang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"RSS\",\"year\":2023,\"name\":\"Robotics: Science and Systems.\"}","rfuniverse.png","Demonstrating RFUniverse: A Multiphysics Simulation Platform for Embodied AI",[],{"type":13,"tag":246,"props":307,"children":314},{":artifactLinks":308,":authors":309,":venue":310,"thumbnail":311,"title":312,"type":313},"{\"arXiv\":\"https://arxiv.org/pdf/2105.03260\"}","[\"Liu Liu*\",\"Han Xue*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"TIP\",\"year\":2022,\"name\":\"IEEE Transactions on Image Processing.\"}","articulation_real.png","Toward Real-World Category-Level Articulation Pose Estimation","journal",[],{"type":13,"tag":246,"props":316,"children":322},{":artifactLinks":317,":authors":318,":venue":319,"thumbnail":320,"title":321,"type":253},"{\"Proceeding\":\"https://www.bmvc2021-virtualconference.com/assets/papers/0544.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2112.07334.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/OMAD\"}","[\"Han Xue*\",\"Liu Liu*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"BMVC\",\"year\":2021,\"name\":\"The 32nd British Machine Vision Conference.\"}","omad.png","OMAD: Object Model with Articulated Deformations for Pose Estimation and Retrieval",[],{"type":13,"tag":246,"props":324,"children":330},{":artifactLinks":325,":authors":326,":venue":327,"thumbnail":328,"title":329,"type":253},"{\"Proceeding\":\"https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660222.pdf\",\"arXiv\":\"https://arxiv.org/pdf/1912.11473.pdf\",\"Code\":\"https://github.com/justimyhxu/Dense-RepPoints\"}","[\"Ze Yang*\",\"Yinghao Xu*\",\"Han Xue*\",\"Zheng Zhang\",\"Raquel Urtasun\",\"Liwei Wang\",\"Stephen Lin\",\"Han Hu (*Equal contribution)\"]","{\"acronym\":\"ECCV\",\"year\":2020,\"name\":\"The European Conference on Computer Vision.\"}","dense_reppoints.jpg","Dense RepPoints: Representing Visual Objects with Dense Point Sets",[],{"type":13,"tag":246,"props":332,"children":339},{":artifactLinks":333,":authors":334,":venue":335,"thumbnail":336,"title":337,"type":253,":hideBottomBorder":338},"{\"Proceeding\":\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf\",\"Code\":\"https://github.com/driving-behavior/DBNet\",\"Website\":\"http://www.dbehavior.net/\"}","[\"Yiping Chen*\",\"Jingkang Wang*\",\"Jonathan Li\",\"Cewu Lu\",\"Zhipeng Luo\",\"Han Xue\",\"Cheng Wang (*Equal contribution)\"]","{\"acronym\":\"CVPR\",\"year\":2018,\"name\":\"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\"}","dbnet.jpg","Lidar-video driving dataset: Learning driving policies effectively","true",[],{"type":13,"tag":26,"props":341,"children":342},{},[343],{"type":13,"tag":32,"props":344,"children":346},{"href":345},"/publication/",[347],{"type":23,"value":348},"Full publication list >>>",{"type":13,"tag":18,"props":350,"children":352},{"id":351},"talks",[353],{"type":23,"value":354},"‚ú® Talks",{"type":13,"tag":356,"props":357,"children":358},"ul",{},[359,373],{"type":13,"tag":360,"props":361,"children":362},"li",{},[363,365,371],{"type":23,"value":364},"[Oct. 2024] Invited talk @ ",{"type":13,"tag":32,"props":366,"children":368},{"href":107,"rel":367},[36],[369],{"type":23,"value":370},"TEA lab",{"type":23,"value":372}," in Tsinghua University, IIIS on \"Efficient Learning for Long-horizon Deformable Object Manipulation\"",{"type":13,"tag":360,"props":374,"children":375},{},[376,378,385,387,394],{"type":23,"value":377},"[Dec. 2024] Invited talk @ ",{"type":13,"tag":32,"props":379,"children":382},{"href":380,"rel":381},"https://www.roscon.cn/2024/index.html",[36],[383],{"type":23,"value":384},"ROSCon China 2024",{"type":23,"value":386}," workshop on \"Development Trends and Challenges in Embodied AI\". The link of the recorded video is ",{"type":13,"tag":32,"props":388,"children":391},{"href":389,"rel":390},"https://www.bilibili.com/video/BV1Z3cteAEnD/?spm_id_from=333.337.search-card.all.click&vd_source=8e062051896958b92b4759e0f4753657",[36],[392],{"type":23,"value":393},"here",{"type":23,"value":208},{"type":13,"tag":18,"props":396,"children":398},{"id":397},"awards",[399],{"type":23,"value":400},"üèÜ Awards",{"type":13,"tag":356,"props":402,"children":403},{},[404,409,414,419,424,429,434],{"type":13,"tag":360,"props":405,"children":406},{},[407],{"type":23,"value":408},"Outstanding Graduates in Shanghai (Top 3%) [‰∏äÊµ∑Â∏Ç‰ºòÁßÄÊØï‰∏öÁîü] in 2021.",{"type":13,"tag":360,"props":410,"children":411},{},[412],{"type":23,"value":413},"Rongchang Technology Innovation Scholarship (Top 10 students in SJTU) [Ëç£Êò∂ÁßëÊäÄÂàõÊñ∞Â•ñÂ≠¶Èáë] in 2020.",{"type":13,"tag":360,"props":415,"children":416},{},[417],{"type":23,"value":418},"SenseTime Scholarship (Top 21 undergraduates in China) [ÂïÜÊ±§Â•ñÂ≠¶Èáë] in 2020.",{"type":13,"tag":360,"props":420,"children":421},{},[422],{"type":23,"value":423},"National Scholarship (Top 3 students in CS Department) in 2017, 2018 and 2019 (three consecutive years) [ÂõΩÂÆ∂Â•ñÂ≠¶ÈáëÔºàËøûÁª≠‰∏âÂπ¥Ôºâ].",{"type":13,"tag":360,"props":425,"children":426},{},[427],{"type":23,"value":428},"Academic Excellence Scholarship (Class A) of SJTU (Top 1% in SJTU) in 2018.",{"type":13,"tag":360,"props":430,"children":431},{},[432],{"type":23,"value":433},"Meritorious Winner Prize of Mathematical Contest in Modeling in 2018.",{"type":13,"tag":360,"props":435,"children":436},{},[437],{"type":23,"value":438},"1st Prize in China Undergraduate Mathematical Contest in Modeling (Shanghai Division) in 2017.",{"type":13,"tag":18,"props":440,"children":442},{"id":441},"find-me",[443],{"type":23,"value":444},"üìß Find Me",{"type":13,"tag":446,"props":447,"children":450},"contact-item",{"icon":448,"url":449},"email","mailto:xiaoxiaoxh@sjtu.edu.cn",[451],{"type":13,"tag":26,"props":452,"children":453},{},[454],{"type":23,"value":455},"Email",{"type":13,"tag":446,"props":457,"children":460},{"icon":458,"url":459},"github","https://github.com/xiaoxiaoxh",[461],{"type":13,"tag":26,"props":462,"children":463},{},[464],{"type":23,"value":465},"GitHub",{"type":13,"tag":446,"props":467,"children":470},{"icon":468,"url":469},"twitter","https://twitter.com/HanXue012",[471],{"type":13,"tag":26,"props":472,"children":473},{},[474],{"type":23,"value":475},"Twitter",{"title":5,"searchDepth":477,"depth":477,"links":478},2,[479,480,481,482,483,484,485],{"id":20,"depth":477,"text":24},{"id":116,"depth":477,"text":119},{"id":135,"depth":477,"text":138},{"id":241,"depth":477,"text":244},{"id":351,"depth":477,"text":354},{"id":397,"depth":477,"text":400},{"id":441,"depth":477,"text":444},"markdown","content:index.md","content","index.md","md",{"_path":492,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":493,"description":5,"leadingImage":494,"disableFancyImage":8,"body":495,"_type":486,"_id":643,"_source":488,"_file":644,"_extension":490},"/news","News","me-news-google.png",{"type":10,"children":496,"toc":641},[497,502,549],{"type":13,"tag":498,"props":499,"children":501},"MarkdownHeader",{"subtitle":500,"title":493},"üì¢ Latest: One paper ([DeformPAM](https://deform-pam.robotflow.ai/)) is accepted by ICRA 2025!",[],{"type":13,"tag":356,"props":503,"children":504},{},[505,524],{"type":13,"tag":360,"props":506,"children":507},{},[508,513,515,522],{"type":13,"tag":148,"props":509,"children":510},{},[511],{"type":23,"value":512},"01/29/2025",{"type":23,"value":514}," üéâ One paper (",{"type":13,"tag":32,"props":516,"children":519},{"href":517,"rel":518},"https://deform-pam.robotflow.ai/",[36],[520],{"type":23,"value":521},"DeformPAM",{"type":23,"value":523},") is accepted by ICRA 2025!",{"type":13,"tag":360,"props":525,"children":526},{},[527,532,534,541,543,548],{"type":13,"tag":148,"props":528,"children":529},{},[530],{"type":23,"value":531},"10/14/2024",{"type":23,"value":533}," üñ•Ô∏è We have released the ",{"type":13,"tag":32,"props":535,"children":538},{"href":536,"rel":537},"https://github.com/xiaoxiaoxh/DeformPAM",[36],[539],{"type":23,"value":540},"code",{"type":23,"value":542}," of ",{"type":13,"tag":32,"props":544,"children":546},{"href":517,"rel":545},[36],[547],{"type":23,"value":521},{"type":23,"value":208},{"type":13,"tag":356,"props":550,"children":551},{},[552,570,596,614],{"type":13,"tag":360,"props":553,"children":554},{},[555,560,561,568],{"type":13,"tag":148,"props":556,"children":557},{},[558],{"type":23,"value":559},"08/31/2023",{"type":23,"value":514},{"type":13,"tag":32,"props":562,"children":565},{"href":563,"rel":564},"https://unifolding.robotflow.ai/",[36],[566],{"type":23,"value":567},"UniFolding",{"type":23,"value":569},") is accepted by CoRL 2023!",{"type":13,"tag":360,"props":571,"children":572},{},[573,578,580,587,589,594],{"type":13,"tag":148,"props":574,"children":575},{},[576],{"type":23,"value":577},"07/14/2023",{"type":23,"value":579}," üî• One paper (",{"type":13,"tag":32,"props":581,"children":584},{"href":582,"rel":583},"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf",[36],[585],{"type":23,"value":586},"ClothPose",{"type":23,"value":588},") is accepted by ICCV 2023 as an ",{"type":13,"tag":148,"props":590,"children":591},{},[592],{"type":23,"value":593},"oral presentation",{"type":23,"value":595},"!",{"type":13,"tag":360,"props":597,"children":598},{},[599,604,605,612],{"type":13,"tag":148,"props":600,"children":601},{},[602],{"type":23,"value":603},"05/13/2023",{"type":23,"value":514},{"type":13,"tag":32,"props":606,"children":609},{"href":607,"rel":608},"https://sites.google.com/view/rfuniverse",[36],[610],{"type":23,"value":611},"RFUniverse",{"type":23,"value":613},") is accepted by RSS 2023!",{"type":13,"tag":360,"props":615,"children":616},{},[617,622,624,631,632,639],{"type":13,"tag":148,"props":618,"children":619},{},[620],{"type":23,"value":621},"02/28/2023",{"type":23,"value":623}," üéâ Two papers (",{"type":13,"tag":32,"props":625,"children":628},{"href":626,"rel":627},"https://garment-tracking.robotflow.ai/",[36],[629],{"type":23,"value":630},"GarmentTracking",{"type":23,"value":95},{"type":13,"tag":32,"props":633,"children":636},{"href":634,"rel":635},"https://sites.google.com/view/vtaco/",[36],[637],{"type":23,"value":638},"VTaCo",{"type":23,"value":640},") are accepted by CVPR 2023!",{"title":5,"searchDepth":477,"depth":477,"links":642},[],"content:news.md","news.md",1740725457036]