[{"data":1,"prerenderedAt":697},["Reactive",2],{"content-query-1DxZ1vYQk5":3,"content-query-B6mqoO8PxR":525},{"_path":4,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":7,"description":5,"hideTitle":8,"disableFancyImage":8,"body":9,"_type":520,"_id":521,"_source":522,"_file":523,"_extension":524},"/","",false,"Home",true,{"type":10,"children":11,"toc":510},"root",[12,17,25,68,73,114,119,125,129,138,144,185,219,244,250,260,268,276,296,304,311,319,328,336,344,353,362,368,429,435,473,479,490,500],{"type":13,"tag":14,"props":15,"children":16},"element","IndexHeader",{},[],{"type":13,"tag":18,"props":19,"children":21},"h2",{"id":20},"Ô∏è-about-me",[22],{"type":23,"value":24},"text","ü¶∏üèª‚Äç‚ôÇÔ∏è About Me",{"type":13,"tag":26,"props":27,"children":28},"p",{},[29,31,40,42,49,51,57,59,66],{"type":23,"value":30},"I am a fourth-year Computer Science Ph.D. candidate at ",{"type":13,"tag":32,"props":33,"children":37},"a",{"href":34,"rel":35},"https://en.sjtu.edu.cn/",[36],"nofollow",[38],{"type":23,"value":39},"Shanghai Jiao Tong University",{"type":23,"value":41}," and a member of ",{"type":13,"tag":32,"props":43,"children":46},{"href":44,"rel":45},"https://www.mvig.org/",[36],[47],{"type":23,"value":48},"Machine Intelligence and Vision Group (MVIG)",{"type":23,"value":50}," under the supervision of ",{"type":13,"tag":32,"props":52,"children":54},{"href":44,"rel":53},[36],[55],{"type":23,"value":56},"Prof. Cewu Lu",{"type":23,"value":58}," (Âç¢Á≠ñÂêæ). I am also a member of ",{"type":13,"tag":32,"props":60,"children":63},{"href":61,"rel":62},"https://ai.sjtu.edu.cn/info/announcements/204",[36],[64],{"type":23,"value":65},"Wu Wen Jun Honorary Doctoral Program",{"type":23,"value":67}," (Âê¥Êñá‰øäËç£Ë™âÂçöÂ£´Áè≠).",{"type":13,"tag":26,"props":69,"children":70},{},[71],{"type":23,"value":72},"I receive my bachelor degree from Shanghai Jiao Tong Universiy in 2021. My research interests lie in Robotics and Computer Vision. Previously, I have been working on deformable object perception and manipulation. Now I am particularly interested in imitation learning with tactile/force sensing and low-cost data collection system.",{"type":13,"tag":26,"props":74,"children":75},{},[76,78,85,87,94,96,103,105,112],{"type":23,"value":77},"In the past, I have interned at ",{"type":13,"tag":32,"props":79,"children":82},{"href":80,"rel":81},"https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/",[36],[83],{"type":23,"value":84},"Microsoft Research Asia",{"type":23,"value":86}," under the supervsion of ",{"type":13,"tag":32,"props":88,"children":91},{"href":89,"rel":90},"https://ancientmooner.github.io/",[36],[92],{"type":23,"value":93},"Han Hu",{"type":23,"value":95}," and ",{"type":13,"tag":32,"props":97,"children":100},{"href":98,"rel":99},"http://yue-cao.me/",[36],[101],{"type":23,"value":102},"Yue Cao",{"type":23,"value":104},". I also spent time with Prof. ",{"type":13,"tag":32,"props":106,"children":109},{"href":107,"rel":108},"http://hxu.rocks/index.html",[36],[110],{"type":23,"value":111},"Huazhe Xu",{"type":23,"value":113}," at Tsinghua University on my projects. In my spare time, I enjoy watching movies and playing with robotsü§ñ.",{"type":13,"tag":26,"props":115,"children":116},{},[117],{"type":23,"value":118},"I'm on the job market now!",{"type":13,"tag":18,"props":120,"children":122},{"id":121},"news",[123],{"type":23,"value":124},"üì∞ News",{"type":13,"tag":126,"props":127,"children":128},"ShortNews",{},[],{"type":13,"tag":26,"props":130,"children":131},{},[132],{"type":13,"tag":32,"props":133,"children":135},{"href":134},"/news/",[136],{"type":23,"value":137},"More news >>>",{"type":13,"tag":18,"props":139,"children":141},{"id":140},"experiences",[142],{"type":23,"value":143},"üè´ Experiences",{"type":13,"tag":145,"props":146,"children":148},"ExperienceRow",{"icon":147},"sjtu.png",[149],{"type":13,"tag":26,"props":150,"children":151},{},[152,157,161,163,166,168,173,175,180,183],{"type":13,"tag":153,"props":154,"children":155},"strong",{},[156],{"type":23,"value":39},{"type":13,"tag":158,"props":159,"children":160},"br",{},[],{"type":23,"value":162},"\nPh.D. Student ",{"type":13,"tag":158,"props":164,"children":165},{},[],{"type":23,"value":167},"\nResearch assistant in ",{"type":13,"tag":32,"props":169,"children":171},{"href":44,"rel":170},[36],[172],{"type":23,"value":48},{"type":23,"value":174},", advised by ",{"type":13,"tag":32,"props":176,"children":178},{"href":44,"rel":177},[36],[179],{"type":23,"value":56},{"type":13,"tag":158,"props":181,"children":182},{},[],{"type":23,"value":184},"\nSep. 2021 - Present",{"type":13,"tag":145,"props":186,"children":188},{"icon":187},"microsoft.png",[189],{"type":13,"tag":26,"props":190,"children":191},{},[192,196,199,201,206,207,212,214,217],{"type":13,"tag":153,"props":193,"children":194},{},[195],{"type":23,"value":84},{"type":13,"tag":158,"props":197,"children":198},{},[],{"type":23,"value":200},"\nResearch Intern, advised by ",{"type":13,"tag":32,"props":202,"children":204},{"href":89,"rel":203},[36],[205],{"type":23,"value":93},{"type":23,"value":95},{"type":13,"tag":32,"props":208,"children":210},{"href":98,"rel":209},[36],[211],{"type":23,"value":102},{"type":23,"value":213},".",{"type":13,"tag":158,"props":215,"children":216},{},[],{"type":23,"value":218},"\nJul. 2019 - Mar. 2020",{"type":13,"tag":145,"props":220,"children":221},{"icon":147},[222],{"type":13,"tag":26,"props":223,"children":224},{},[225,229,232,234,237,239,242],{"type":13,"tag":153,"props":226,"children":227},{},[228],{"type":23,"value":39},{"type":13,"tag":158,"props":230,"children":231},{},[],{"type":23,"value":233},"\nBachelor of Engineering in Computer Science. ",{"type":13,"tag":158,"props":235,"children":236},{},[],{"type":23,"value":238},"\nGPA 4.04/4.3, Rank 3/150 (Top 2%)",{"type":13,"tag":158,"props":240,"children":241},{},[],{"type":23,"value":243},"\nSep 2016 - Jun. 2021",{"type":13,"tag":18,"props":245,"children":247},{"id":246},"selected-publications",[248],{"type":23,"value":249},"üìÑ Selected Publications",{"type":13,"tag":251,"props":252,"children":259},"PublicationRow",{":artifactLinks":253,":authors":254,":venue":255,"thumbnail":256,"title":257,"type":258},"{\"Website\":\"https://reactive-diffusion-policy.github.io/\",\"arXiv\":\"https://arxiv.org/abs/2503.02881\",\"Code\":\"https://github.com/xiaoxiaoxh/reactive_diffusion_policy\"}","[\"Han Xue*\",\"Jieji Ren*\",\"Wendi Chen*\",\"Gu Zhang\",\"Yuan Fang\",\"Guoying Gu\",\"Huazhe Xu‚Ä†\",\"Cewu Lu‚Ä† (‚Ä†Equal advising)\"]","{\"acronym\":\"RSS\",\"year\":2025,\"name\":\"Robotics: Science and Systems (RSS)\"}","rdp.gif","Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation","conference",[],{"type":13,"tag":251,"props":261,"children":267},{":artifactLinks":262,":authors":263,":venue":264,"thumbnail":265,"title":266,"type":258},"{\"Website\":\"https://deform-pam.robotflow.ai/\",\"arXiv\":\"https://arxiv.org/abs/2410.11584\",\"Code\":\"https://github.com/xiaoxiaoxh/DeformPAM\"}","[\"Wendi Chen*\",\"Han Xue*\",\"Fangyuan Zhou\",\"Yuan Fang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICRA\",\"year\":2025,\"name\":\"IEEE International Conference on Robotics and Automation (ICRA)\"}","deform-pam.gif","DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object Manipulation via Preference-based Action Alignment",[],{"type":13,"tag":251,"props":269,"children":275},{":artifactLinks":270,":authors":271,":venue":272,"thumbnail":273,"title":274,"type":258},"{\"Proceeding\":\"https://openreview.net/pdf?id=ANJuNDFdvP\",\"arXiv\":\"https://arxiv.org/abs/2311.01267\",\"Code\":\"https://github.com/xiaoxiaoxh/UniFolding\",\"Website\":\"https://unifolding.robotflow.ai/\"}","[\"Han Xue*\",\"Yutong Li*\",\"Wenqiang Xu\",\"Huanyu Li\",\"Dongzhe Zheng\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"CoRL\",\"year\":2023,\"name\":\"7th Annual Conference on Robot Learning.\"}","unifolding.gif","UniFolding: Towards Sample-efficient, Scalable, and Generalizable Robotic Garment Folding",[],{"type":13,"tag":251,"props":277,"children":283},{":artifactLinks":278,":authors":279,":venue":280,"thumbnail":281,"title":282,"type":258},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf\"}","[\"Wenqiang Xu*\",\"Wenxin Du*\",\"Han Xue\",\"Yutong Li\",\"Ruolin Ye\",\"Yan-Feng Wang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICCV\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF International Conference on Computer Vision\"}","clothpose.png","ClothPose: A Real-world Benchmark for Visual Analysis of Garment Pose via An Indirect Recording Solution",[284],{"type":13,"tag":26,"props":285,"children":286},{},[287],{"type":13,"tag":288,"props":289,"children":293},"span",{"className":290},[291,292],"text-red-600","font-bold",[294],{"type":23,"value":295},"üî• Oral Presentation.",{"type":13,"tag":251,"props":297,"children":303},{":artifactLinks":298,":authors":299,":venue":300,"thumbnail":301,"title":302,"type":258},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_GarmentTracking_Category-Level_Garment_Pose_Tracking_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.13913.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/GarmentTracking\",\"Website\":\"https://garment-tracking.robotflow.ai/\"}","[\"Han Xue\",\"Wenqiang Xu\",\"Jieyi Zhang\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Ruolin Ye\",\"Cewu Lu\"]","{\"acronym\":\"CVPR\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\"}","garment-tracking.gif","GarmentTracking: Category-Level Garment Pose Tracking",[],{"type":13,"tag":251,"props":305,"children":310},{":artifactLinks":306,":authors":307,":venue":300,"thumbnail":308,"title":309,"type":258},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Visual-Tactile_Sensing_for_In-Hand_Object_Reconstruction_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.14498.pdf\",\"Code\":\"https://github.com/jeffsonyu/VTacO\",\"Website\":\"https://sites.google.com/view/vtaco/\"}","[\"Wenqiang Xu*\",\"Zhenjun Yu*\",\"Han Xue\",\"Ruolin Ye\",\"Siqiong Yao\",\"Cewu Lu (*Equal contribution)\"]","vtaco.png","Visual-Tactile Sensing for In-Hand Object Reconstruction",[],{"type":13,"tag":251,"props":312,"children":318},{":artifactLinks":313,":authors":314,":venue":315,"thumbnail":316,"title":317,"type":258},"{\"Proceeding\":\"https://www.roboticsproceedings.org/rss19/p087.pdf\",\"Code\":\"https://github.com/mvig-robotflow/pyrfuniverse\",\"Website\":\"https://sites.google.com/view/rfuniverse\"}","[\"Haoyuan Fu*\",\"Wenqiang Xu*\",\"Ruolin Ye*\",\"Han Xue\",\"Zhenjun Yu\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Jieyi Zhang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"RSS\",\"year\":2023,\"name\":\"Robotics: Science and Systems.\"}","rfuniverse.png","Demonstrating RFUniverse: A Multiphysics Simulation Platform for Embodied AI",[],{"type":13,"tag":251,"props":320,"children":327},{":artifactLinks":321,":authors":322,":venue":323,"thumbnail":324,"title":325,"type":326},"{\"arXiv\":\"https://arxiv.org/pdf/2105.03260\"}","[\"Liu Liu*\",\"Han Xue*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"TIP\",\"year\":2022,\"name\":\"IEEE Transactions on Image Processing.\"}","articulation_real.png","Toward Real-World Category-Level Articulation Pose Estimation","journal",[],{"type":13,"tag":251,"props":329,"children":335},{":artifactLinks":330,":authors":331,":venue":332,"thumbnail":333,"title":334,"type":258},"{\"Proceeding\":\"https://www.bmvc2021-virtualconference.com/assets/papers/0544.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2112.07334.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/OMAD\"}","[\"Han Xue*\",\"Liu Liu*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"BMVC\",\"year\":2021,\"name\":\"The 32nd British Machine Vision Conference.\"}","omad.png","OMAD: Object Model with Articulated Deformations for Pose Estimation and Retrieval",[],{"type":13,"tag":251,"props":337,"children":343},{":artifactLinks":338,":authors":339,":venue":340,"thumbnail":341,"title":342,"type":258},"{\"Proceeding\":\"https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660222.pdf\",\"arXiv\":\"https://arxiv.org/pdf/1912.11473.pdf\",\"Code\":\"https://github.com/justimyhxu/Dense-RepPoints\"}","[\"Ze Yang*\",\"Yinghao Xu*\",\"Han Xue*\",\"Zheng Zhang\",\"Raquel Urtasun\",\"Liwei Wang\",\"Stephen Lin\",\"Han Hu (*Equal contribution)\"]","{\"acronym\":\"ECCV\",\"year\":2020,\"name\":\"The European Conference on Computer Vision.\"}","dense_reppoints.jpg","Dense RepPoints: Representing Visual Objects with Dense Point Sets",[],{"type":13,"tag":251,"props":345,"children":352},{":artifactLinks":346,":authors":347,":venue":348,"thumbnail":349,"title":350,"type":258,":hideBottomBorder":351},"{\"Proceeding\":\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf\",\"Code\":\"https://github.com/driving-behavior/DBNet\",\"Website\":\"http://www.dbehavior.net/\"}","[\"Yiping Chen*\",\"Jingkang Wang*\",\"Jonathan Li\",\"Cewu Lu\",\"Zhipeng Luo\",\"Han Xue\",\"Cheng Wang (*Equal contribution)\"]","{\"acronym\":\"CVPR\",\"year\":2018,\"name\":\"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\"}","dbnet.jpg","Lidar-video driving dataset: Learning driving policies effectively","true",[],{"type":13,"tag":26,"props":354,"children":355},{},[356],{"type":13,"tag":32,"props":357,"children":359},{"href":358},"/publication/",[360],{"type":23,"value":361},"Full publication list >>>",{"type":13,"tag":18,"props":363,"children":365},{"id":364},"talks",[366],{"type":23,"value":367},"‚ú® Talks",{"type":13,"tag":369,"props":370,"children":371},"ul",{},[372,395,416],{"type":13,"tag":373,"props":374,"children":375},"li",{},[376,378,385,387,394],{"type":23,"value":377},"[Apr. 2025] Invited talk @ ",{"type":13,"tag":32,"props":379,"children":382},{"href":380,"rel":381},"https://www.techbeat.net/",[36],[383],{"type":23,"value":384},"TechBeat",{"type":23,"value":386}," on \"Robotic Imitation Learning with Vision-Tactile/Force Sensing\". The link of the recorded video is ",{"type":13,"tag":32,"props":388,"children":391},{"href":389,"rel":390},"https://www.techbeat.net/talk-info?id=963",[36],[392],{"type":23,"value":393},"here",{"type":23,"value":213},{"type":13,"tag":373,"props":396,"children":397},{},[398,400,407,409,415],{"type":23,"value":399},"[Dec. 2024] Invited talk @ ",{"type":13,"tag":32,"props":401,"children":404},{"href":402,"rel":403},"https://www.roscon.cn/2024/index.html",[36],[405],{"type":23,"value":406},"ROSCon China 2024",{"type":23,"value":408}," workshop on \"Development Trends and Challenges in Embodied AI\". The link of the recorded video is ",{"type":13,"tag":32,"props":410,"children":413},{"href":411,"rel":412},"https://www.bilibili.com/video/BV1Z3cteAEnD/?spm_id_from=333.337.search-card.all.click&vd_source=8e062051896958b92b4759e0f4753657",[36],[414],{"type":23,"value":393},{"type":23,"value":213},{"type":13,"tag":373,"props":417,"children":418},{},[419,421,427],{"type":23,"value":420},"[Oct. 2024] Invited talk @ ",{"type":13,"tag":32,"props":422,"children":424},{"href":107,"rel":423},[36],[425],{"type":23,"value":426},"TEA lab",{"type":23,"value":428}," in Tsinghua University, IIIS on \"Efficient Learning for Long-horizon Deformable Object Manipulation\"",{"type":13,"tag":18,"props":430,"children":432},{"id":431},"awards",[433],{"type":23,"value":434},"üèÜ Awards",{"type":13,"tag":369,"props":436,"children":437},{},[438,443,448,453,458,463,468],{"type":13,"tag":373,"props":439,"children":440},{},[441],{"type":23,"value":442},"Outstanding Graduates in Shanghai (Top 3%) [‰∏äÊµ∑Â∏Ç‰ºòÁßÄÊØï‰∏öÁîü] in 2021.",{"type":13,"tag":373,"props":444,"children":445},{},[446],{"type":23,"value":447},"Rongchang Technology Innovation Scholarship (Top 10 students in SJTU) [Ëç£Êò∂ÁßëÊäÄÂàõÊñ∞Â•ñÂ≠¶Èáë] in 2020.",{"type":13,"tag":373,"props":449,"children":450},{},[451],{"type":23,"value":452},"SenseTime Scholarship (Top 21 undergraduates in China) [ÂïÜÊ±§Â•ñÂ≠¶Èáë] in 2020.",{"type":13,"tag":373,"props":454,"children":455},{},[456],{"type":23,"value":457},"National Scholarship (Top 3 students in CS Department) in 2017, 2018 and 2019 (three consecutive years) [ÂõΩÂÆ∂Â•ñÂ≠¶ÈáëÔºàËøûÁª≠‰∏âÂπ¥Ôºâ].",{"type":13,"tag":373,"props":459,"children":460},{},[461],{"type":23,"value":462},"Academic Excellence Scholarship (Class A) of SJTU (Top 1% in SJTU) in 2018.",{"type":13,"tag":373,"props":464,"children":465},{},[466],{"type":23,"value":467},"Meritorious Winner Prize of Mathematical Contest in Modeling in 2018.",{"type":13,"tag":373,"props":469,"children":470},{},[471],{"type":23,"value":472},"1st Prize in China Undergraduate Mathematical Contest in Modeling (Shanghai Division) in 2017.",{"type":13,"tag":18,"props":474,"children":476},{"id":475},"find-me",[477],{"type":23,"value":478},"üìß Find Me",{"type":13,"tag":480,"props":481,"children":484},"contact-item",{"icon":482,"url":483},"email","mailto:xiaoxiaoxh@sjtu.edu.cn",[485],{"type":13,"tag":26,"props":486,"children":487},{},[488],{"type":23,"value":489},"Email",{"type":13,"tag":480,"props":491,"children":494},{"icon":492,"url":493},"github","https://github.com/xiaoxiaoxh",[495],{"type":13,"tag":26,"props":496,"children":497},{},[498],{"type":23,"value":499},"GitHub",{"type":13,"tag":480,"props":501,"children":504},{"icon":502,"url":503},"twitter","https://twitter.com/HanXue012",[505],{"type":13,"tag":26,"props":506,"children":507},{},[508],{"type":23,"value":509},"Twitter",{"title":5,"searchDepth":511,"depth":511,"links":512},2,[513,514,515,516,517,518,519],{"id":20,"depth":511,"text":24},{"id":121,"depth":511,"text":124},{"id":140,"depth":511,"text":143},{"id":246,"depth":511,"text":249},{"id":364,"depth":511,"text":367},{"id":431,"depth":511,"text":434},{"id":475,"depth":511,"text":478},"markdown","content:index.md","content","index.md","md",{"_path":526,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":527,"description":5,"leadingImage":528,"disableFancyImage":8,"body":529,"_type":520,"_id":695,"_source":522,"_file":696,"_extension":524},"/news","News","me-news-google.png",{"type":10,"children":530,"toc":693},[531,536,602],{"type":13,"tag":532,"props":533,"children":535},"MarkdownHeader",{"subtitle":534,"title":527},"üì¢ Latest: One paper (DeformPAM) is accepted by ICRA 2025!",[],{"type":13,"tag":369,"props":537,"children":538},{},[539,558,584],{"type":13,"tag":373,"props":540,"children":541},{},[542,547,549,556],{"type":13,"tag":153,"props":543,"children":544},{},[545],{"type":23,"value":546},"04/11/2025",{"type":23,"value":548}," üéâ One paper (",{"type":13,"tag":32,"props":550,"children":553},{"href":551,"rel":552},"https://reactive-diffusion-policy.github.io/",[36],[554],{"type":23,"value":555},"Reactive Diffusion Policy",{"type":23,"value":557},") is accepted by RSS 2025!",{"type":13,"tag":373,"props":559,"children":560},{},[561,566,568,575,577,582],{"type":13,"tag":153,"props":562,"children":563},{},[564],{"type":23,"value":565},"04/03/2025",{"type":23,"value":567}," üñ•Ô∏è We have released the ",{"type":13,"tag":32,"props":569,"children":572},{"href":570,"rel":571},"https://github.com/xiaoxiaoxh/reactive_diffusion_policy",[36],[573],{"type":23,"value":574},"code",{"type":23,"value":576}," of ",{"type":13,"tag":32,"props":578,"children":580},{"href":551,"rel":579},[36],[581],{"type":23,"value":555},{"type":23,"value":583},"!",{"type":13,"tag":373,"props":585,"children":586},{},[587,592,593,600],{"type":13,"tag":153,"props":588,"children":589},{},[590],{"type":23,"value":591},"01/29/2025",{"type":23,"value":548},{"type":13,"tag":32,"props":594,"children":597},{"href":595,"rel":596},"https://deform-pam.robotflow.ai/",[36],[598],{"type":23,"value":599},"DeformPAM",{"type":23,"value":601},") is accepted by ICRA 2025!",{"type":13,"tag":369,"props":603,"children":604},{},[605,623,648,666],{"type":13,"tag":373,"props":606,"children":607},{},[608,613,614,621],{"type":13,"tag":153,"props":609,"children":610},{},[611],{"type":23,"value":612},"08/31/2023",{"type":23,"value":548},{"type":13,"tag":32,"props":615,"children":618},{"href":616,"rel":617},"https://unifolding.robotflow.ai/",[36],[619],{"type":23,"value":620},"UniFolding",{"type":23,"value":622},") is accepted by CoRL 2023!",{"type":13,"tag":373,"props":624,"children":625},{},[626,631,633,640,642,647],{"type":13,"tag":153,"props":627,"children":628},{},[629],{"type":23,"value":630},"07/14/2023",{"type":23,"value":632}," üî• One paper (",{"type":13,"tag":32,"props":634,"children":637},{"href":635,"rel":636},"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf",[36],[638],{"type":23,"value":639},"ClothPose",{"type":23,"value":641},") is accepted by ICCV 2023 as an ",{"type":13,"tag":153,"props":643,"children":644},{},[645],{"type":23,"value":646},"oral presentation",{"type":23,"value":583},{"type":13,"tag":373,"props":649,"children":650},{},[651,656,657,664],{"type":13,"tag":153,"props":652,"children":653},{},[654],{"type":23,"value":655},"05/13/2023",{"type":23,"value":548},{"type":13,"tag":32,"props":658,"children":661},{"href":659,"rel":660},"https://sites.google.com/view/rfuniverse",[36],[662],{"type":23,"value":663},"RFUniverse",{"type":23,"value":665},") is accepted by RSS 2023!",{"type":13,"tag":373,"props":667,"children":668},{},[669,674,676,683,684,691],{"type":13,"tag":153,"props":670,"children":671},{},[672],{"type":23,"value":673},"02/28/2023",{"type":23,"value":675}," üéâ Two papers (",{"type":13,"tag":32,"props":677,"children":680},{"href":678,"rel":679},"https://garment-tracking.robotflow.ai/",[36],[681],{"type":23,"value":682},"GarmentTracking",{"type":23,"value":95},{"type":13,"tag":32,"props":685,"children":688},{"href":686,"rel":687},"https://sites.google.com/view/vtaco/",[36],[689],{"type":23,"value":690},"VTaCo",{"type":23,"value":692},") are accepted by CVPR 2023!",{"title":5,"searchDepth":511,"depth":511,"links":694},[],"content:news.md","news.md",1746606873012]