[{"data":1,"prerenderedAt":814},["Reactive",2],{"content-query-1DxZ1vYQk5":3,"content-query-B6mqoO8PxR":623},{"_path":4,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":7,"description":5,"hideTitle":8,"disableFancyImage":8,"body":9,"_type":618,"_id":619,"_source":620,"_file":621,"_extension":622},"/","",false,"Home",true,{"type":10,"children":11,"toc":608},"root",[12,17,25,68,73,123,129,133,142,148,189,229,259,265,275,283,290,310,335,343,360,368,375,383,392,400,408,417,426,432,517,523,571,577,588,598],{"type":13,"tag":14,"props":15,"children":16},"element","IndexHeader",{},[],{"type":13,"tag":18,"props":19,"children":21},"h2",{"id":20},"Ô∏è-about-me",[22],{"type":23,"value":24},"text","ü¶∏üèª‚Äç‚ôÇÔ∏è About Me",{"type":13,"tag":26,"props":27,"children":28},"p",{},[29,31,40,42,49,51,57,59,66],{"type":23,"value":30},"I am a fifth-year Computer Science Ph.D. candidate at ",{"type":13,"tag":32,"props":33,"children":37},"a",{"href":34,"rel":35},"https://en.sjtu.edu.cn/",[36],"nofollow",[38],{"type":23,"value":39},"Shanghai Jiao Tong University",{"type":23,"value":41}," and a member of ",{"type":13,"tag":32,"props":43,"children":46},{"href":44,"rel":45},"https://www.mvig.org/",[36],[47],{"type":23,"value":48},"Machine Intelligence and Vision Group (MVIG)",{"type":23,"value":50}," under the supervision of ",{"type":13,"tag":32,"props":52,"children":54},{"href":44,"rel":53},[36],[55],{"type":23,"value":56},"Prof. Cewu Lu",{"type":23,"value":58}," (Âç¢Á≠ñÂêæ). I am also a member of ",{"type":13,"tag":32,"props":60,"children":63},{"href":61,"rel":62},"https://ai.sjtu.edu.cn/info/announcements/204",[36],[64],{"type":23,"value":65},"Wu Wen Jun Honorary Doctoral Program",{"type":23,"value":67}," (Âê¥Êñá‰øäËç£Ë™âÂçöÂ£´Áè≠).",{"type":13,"tag":26,"props":69,"children":70},{},[71],{"type":23,"value":72},"I receive my bachelor degree from Shanghai Jiao Tong Universiy in 2021. My research interests lie in Robotics and Computer Vision. Previously, I have been working on deformable object perception and manipulation. Now I am particularly interested in imitation learning with tactile/force sensing and low-cost data collection system.",{"type":13,"tag":26,"props":74,"children":75},{},[76,78,85,87,94,96,103,105,112,114,121],{"type":23,"value":77},"In the past, I have interned at ",{"type":13,"tag":32,"props":79,"children":82},{"href":80,"rel":81},"https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/",[36],[83],{"type":23,"value":84},"Microsoft Research Asia",{"type":23,"value":86}," under the supervsion of ",{"type":13,"tag":32,"props":88,"children":91},{"href":89,"rel":90},"https://ancientmooner.github.io/",[36],[92],{"type":23,"value":93},"Han Hu",{"type":23,"value":95},", ",{"type":13,"tag":32,"props":97,"children":100},{"href":98,"rel":99},"http://yue-cao.me/",[36],[101],{"type":23,"value":102},"Yue Cao",{"type":23,"value":104}," and ",{"type":13,"tag":32,"props":106,"children":109},{"href":107,"rel":108},"https://jifengdai.org/",[36],[110],{"type":23,"value":111},"Jifeng Dai",{"type":23,"value":113},". I also spent time with Prof. ",{"type":13,"tag":32,"props":115,"children":118},{"href":116,"rel":117},"http://hxu.rocks/index.html",[36],[119],{"type":23,"value":120},"Huazhe Xu",{"type":23,"value":122}," at Tsinghua University on my projects. In my spare time, I enjoy watching movies and playing with robotsü§ñ.",{"type":13,"tag":18,"props":124,"children":126},{"id":125},"news",[127],{"type":23,"value":128},"üì∞ News",{"type":13,"tag":130,"props":131,"children":132},"ShortNews",{},[],{"type":13,"tag":26,"props":134,"children":135},{},[136],{"type":13,"tag":32,"props":137,"children":139},{"href":138},"/news/",[140],{"type":23,"value":141},"More news >>>",{"type":13,"tag":18,"props":143,"children":145},{"id":144},"experiences",[146],{"type":23,"value":147},"üè´ Experiences",{"type":13,"tag":149,"props":150,"children":152},"ExperienceRow",{"icon":151},"sjtu.png",[153],{"type":13,"tag":26,"props":154,"children":155},{},[156,161,165,167,170,172,177,179,184,187],{"type":13,"tag":157,"props":158,"children":159},"strong",{},[160],{"type":23,"value":39},{"type":13,"tag":162,"props":163,"children":164},"br",{},[],{"type":23,"value":166},"\nPh.D. Student ",{"type":13,"tag":162,"props":168,"children":169},{},[],{"type":23,"value":171},"\nResearch assistant in ",{"type":13,"tag":32,"props":173,"children":175},{"href":44,"rel":174},[36],[176],{"type":23,"value":48},{"type":23,"value":178},", advised by ",{"type":13,"tag":32,"props":180,"children":182},{"href":44,"rel":181},[36],[183],{"type":23,"value":56},{"type":13,"tag":162,"props":185,"children":186},{},[],{"type":23,"value":188},"\nSep. 2021 - Present",{"type":13,"tag":149,"props":190,"children":192},{"icon":191},"microsoft.png",[193],{"type":13,"tag":26,"props":194,"children":195},{},[196,200,203,205,210,211,216,217,222,224,227],{"type":13,"tag":157,"props":197,"children":198},{},[199],{"type":23,"value":84},{"type":13,"tag":162,"props":201,"children":202},{},[],{"type":23,"value":204},"\nResearch Intern, advised by ",{"type":13,"tag":32,"props":206,"children":208},{"href":89,"rel":207},[36],[209],{"type":23,"value":93},{"type":23,"value":95},{"type":13,"tag":32,"props":212,"children":214},{"href":98,"rel":213},[36],[215],{"type":23,"value":102},{"type":23,"value":104},{"type":13,"tag":32,"props":218,"children":220},{"href":107,"rel":219},[36],[221],{"type":23,"value":111},{"type":23,"value":223},".",{"type":13,"tag":162,"props":225,"children":226},{},[],{"type":23,"value":228},"\nJul. 2019 - Mar. 2020",{"type":13,"tag":149,"props":230,"children":231},{"icon":151},[232],{"type":13,"tag":26,"props":233,"children":234},{},[235,239,242,244,247,249,254,257],{"type":13,"tag":157,"props":236,"children":237},{},[238],{"type":23,"value":39},{"type":13,"tag":162,"props":240,"children":241},{},[],{"type":23,"value":243},"\nBachelor of Engineering in Computer Science. ",{"type":13,"tag":162,"props":245,"children":246},{},[],{"type":23,"value":248},"\nGPA 4.04/4.3, ",{"type":13,"tag":157,"props":250,"children":251},{},[252],{"type":23,"value":253},"Rank 3/150 (Top 2%)",{"type":13,"tag":162,"props":255,"children":256},{},[],{"type":23,"value":258},"\nSep 2016 - Jun. 2021",{"type":13,"tag":18,"props":260,"children":262},{"id":261},"selected-publications",[263],{"type":23,"value":264},"üìÑ Selected Publications",{"type":13,"tag":266,"props":267,"children":274},"PublicationRow",{":artifactLinks":268,":authors":269,":venue":270,"thumbnail":271,"title":272,"type":273},"{\"Website\":\"https://implicit-rdp.github.io/\",\"arXiv\":\"https://arxiv.org/abs/2512.10946\"}","[\"Wendi Chen\",\"Han Xue\",\"Yi Wang\",\"Fangyuan Zhou\",\"Jun Lv\",\"Yang Jin\",\"Shirun Tang\",\"Chuan Wen‚Ä†\",\"Cewu Lu‚Ä† (‚Ä†equal advising)\"]","{\"acronym\":\"arXiv\",\"year\":2025,\"name\":\"arXiv\"}","ImplicitRDP.gif","ImplicitRDP: An End-to-End Visual-Force Diffusion Policy with Structural Slow-Fast Learning","conference",[],{"type":13,"tag":266,"props":276,"children":282},{":artifactLinks":277,":authors":278,":venue":279,"thumbnail":280,"title":281,"type":273},"{\"Website\":\"https://ericjin2002.github.io/SOE\",\"arXiv\":\"https://arxiv.org/abs/2509.19292\"}","[\"Yang Jin\",\"Jun Lv\",\"Han Xue\",\"Wendi Chen\",\"Chuan Wen‚Ä†\",\"Cewu Lu‚Ä† (‚Ä†equal advising)\"]","{\"acronym\":\"ICRA\",\"year\":2026,\"name\":\"IEEE International Conference on Robotics and Automation (ICRA)\"}","SOE.gif","SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration",[],{"type":13,"tag":266,"props":284,"children":289},{":artifactLinks":285,":authors":286,":venue":279,"thumbnail":287,"title":288,"type":273},"{\"Website\":\"https://right-side-out.github.io/\",\"arXiv\":\"https://arxiv.org/abs/2509.15953\"}","[\"Chang Yu*\",\"Siyu Ma*\",\"Wenxin Du\",\"Zeshun Zong\",\"Han Xue\",\"Wendi Chen\",\"Cewu Lu\",\"Yin Yang\",\"Xuchen Han\",\"Joseph Masterjohn\",\"Alejandro Castro\",\"Chenfanfu Jiang (*Equal contribution)\"]","Right-Side-Out.gif","Right-Side-Out: Learning Zero-Shot Sim-to-Real Garment Reversal",[],{"type":13,"tag":266,"props":291,"children":297},{":artifactLinks":292,":authors":293,":venue":294,"thumbnail":295,"title":296,"type":273},"{\"Website\":\"https://reactive-diffusion-policy.github.io/\",\"arXiv\":\"https://arxiv.org/abs/2503.02881\",\"Code\":\"https://github.com/xiaoxiaoxh/reactive_diffusion_policy\"}","[\"Han Xue*\",\"Jieji Ren*\",\"Wendi Chen*\",\"Gu Zhang\",\"Yuan Fang\",\"Guoying Gu\",\"Huazhe Xu‚Ä†\",\"Cewu Lu‚Ä† (‚Ä†Equal advising)\"]","{\"acronym\":\"RSS\",\"year\":2025,\"name\":\"Robotics: Science and Systems (RSS)\"}","rdp.gif","Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation",[298],{"type":13,"tag":26,"props":299,"children":300},{},[301],{"type":13,"tag":302,"props":303,"children":307},"span",{"className":304},[305,306],"text-red-600","font-bold",[308],{"type":23,"value":309},"üî• Best Student Paper Finalist.",{"type":13,"tag":266,"props":311,"children":317},{":artifactLinks":312,":authors":313,":venue":314,"thumbnail":315,"title":316,"type":273},"{\"Website\":\"https://deform-pam.robotflow.ai/\",\"arXiv\":\"https://arxiv.org/abs/2410.11584\",\"Code\":\"https://github.com/xiaoxiaoxh/DeformPAM\"}","[\"Wendi Chen*\",\"Han Xue*\",\"Fangyuan Zhou\",\"Yuan Fang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICRA\",\"year\":2025,\"name\":\"IEEE International Conference on Robotics and Automation (ICRA)\"}","deform-pam.gif","DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object Manipulation via Preference-based Action Alignment",[318],{"type":13,"tag":26,"props":319,"children":320},{},[321],{"type":13,"tag":302,"props":322,"children":324},{"className":323},[305,306],[325,327,334],{"type":23,"value":326},"üî• Best Paper Finalist ",{"type":13,"tag":32,"props":328,"children":331},{"href":329,"rel":330},"https://deformable-workshop.github.io/icra2025/",[36],[332],{"type":23,"value":333},"@ RMDO Workshop in ICRA 2025",{"type":23,"value":223},{"type":13,"tag":266,"props":336,"children":342},{":artifactLinks":337,":authors":338,":venue":339,"thumbnail":340,"title":341,"type":273},"{\"Proceeding\":\"https://openreview.net/pdf?id=ANJuNDFdvP\",\"arXiv\":\"https://arxiv.org/abs/2311.01267\",\"Code\":\"https://github.com/xiaoxiaoxh/UniFolding\",\"Website\":\"https://unifolding.robotflow.ai/\"}","[\"Han Xue*\",\"Yutong Li*\",\"Wenqiang Xu\",\"Huanyu Li\",\"Dongzhe Zheng\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"CoRL\",\"year\":2023,\"name\":\"7th Annual Conference on Robot Learning.\"}","unifolding.gif","UniFolding: Towards Sample-efficient, Scalable, and Generalizable Robotic Garment Folding",[],{"type":13,"tag":266,"props":344,"children":350},{":artifactLinks":345,":authors":346,":venue":347,"thumbnail":348,"title":349,"type":273},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf\"}","[\"Wenqiang Xu*\",\"Wenxin Du*\",\"Han Xue\",\"Yutong Li\",\"Ruolin Ye\",\"Yan-Feng Wang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICCV\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF International Conference on Computer Vision\"}","clothpose.png","ClothPose: A Real-world Benchmark for Visual Analysis of Garment Pose via An Indirect Recording Solution",[351],{"type":13,"tag":26,"props":352,"children":353},{},[354],{"type":13,"tag":302,"props":355,"children":357},{"className":356},[305,306],[358],{"type":23,"value":359},"üî• Oral Presentation.",{"type":13,"tag":266,"props":361,"children":367},{":artifactLinks":362,":authors":363,":venue":364,"thumbnail":365,"title":366,"type":273},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_GarmentTracking_Category-Level_Garment_Pose_Tracking_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.13913.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/GarmentTracking\",\"Website\":\"https://garment-tracking.robotflow.ai/\"}","[\"Han Xue\",\"Wenqiang Xu\",\"Jieyi Zhang\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Ruolin Ye\",\"Cewu Lu\"]","{\"acronym\":\"CVPR\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\"}","garment-tracking.gif","GarmentTracking: Category-Level Garment Pose Tracking",[],{"type":13,"tag":266,"props":369,"children":374},{":artifactLinks":370,":authors":371,":venue":364,"thumbnail":372,"title":373,"type":273},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Visual-Tactile_Sensing_for_In-Hand_Object_Reconstruction_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.14498.pdf\",\"Code\":\"https://github.com/jeffsonyu/VTacO\",\"Website\":\"https://sites.google.com/view/vtaco/\"}","[\"Wenqiang Xu*\",\"Zhenjun Yu*\",\"Han Xue\",\"Ruolin Ye\",\"Siqiong Yao\",\"Cewu Lu (*Equal contribution)\"]","vtaco.png","Visual-Tactile Sensing for In-Hand Object Reconstruction",[],{"type":13,"tag":266,"props":376,"children":382},{":artifactLinks":377,":authors":378,":venue":379,"thumbnail":380,"title":381,"type":273},"{\"Proceeding\":\"https://www.roboticsproceedings.org/rss19/p087.pdf\",\"Code\":\"https://github.com/mvig-robotflow/pyrfuniverse\",\"Website\":\"https://sites.google.com/view/rfuniverse\"}","[\"Haoyuan Fu*\",\"Wenqiang Xu*\",\"Ruolin Ye*\",\"Han Xue\",\"Zhenjun Yu\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Jieyi Zhang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"RSS\",\"year\":2023,\"name\":\"Robotics: Science and Systems.\"}","rfuniverse.png","Demonstrating RFUniverse: A Multiphysics Simulation Platform for Embodied AI",[],{"type":13,"tag":266,"props":384,"children":391},{":artifactLinks":385,":authors":386,":venue":387,"thumbnail":388,"title":389,"type":390},"{\"arXiv\":\"https://arxiv.org/pdf/2105.03260\"}","[\"Liu Liu*\",\"Han Xue*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"TIP\",\"year\":2022,\"name\":\"IEEE Transactions on Image Processing.\"}","articulation_real.png","Toward Real-World Category-Level Articulation Pose Estimation","journal",[],{"type":13,"tag":266,"props":393,"children":399},{":artifactLinks":394,":authors":395,":venue":396,"thumbnail":397,"title":398,"type":273},"{\"Proceeding\":\"https://www.bmvc2021-virtualconference.com/assets/papers/0544.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2112.07334.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/OMAD\"}","[\"Han Xue*\",\"Liu Liu*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"BMVC\",\"year\":2021,\"name\":\"The 32nd British Machine Vision Conference.\"}","omad.png","OMAD: Object Model with Articulated Deformations for Pose Estimation and Retrieval",[],{"type":13,"tag":266,"props":401,"children":407},{":artifactLinks":402,":authors":403,":venue":404,"thumbnail":405,"title":406,"type":273},"{\"Proceeding\":\"https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660222.pdf\",\"arXiv\":\"https://arxiv.org/pdf/1912.11473.pdf\",\"Code\":\"https://github.com/justimyhxu/Dense-RepPoints\"}","[\"Ze Yang*\",\"Yinghao Xu*\",\"Han Xue*\",\"Zheng Zhang\",\"Raquel Urtasun\",\"Liwei Wang\",\"Stephen Lin\",\"Han Hu (*Equal contribution)\"]","{\"acronym\":\"ECCV\",\"year\":2020,\"name\":\"The European Conference on Computer Vision.\"}","dense_reppoints.jpg","Dense RepPoints: Representing Visual Objects with Dense Point Sets",[],{"type":13,"tag":266,"props":409,"children":416},{":artifactLinks":410,":authors":411,":venue":412,"thumbnail":413,"title":414,"type":273,":hideBottomBorder":415},"{\"Proceeding\":\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf\",\"Code\":\"https://github.com/driving-behavior/DBNet\",\"Website\":\"http://www.dbehavior.net/\"}","[\"Yiping Chen*\",\"Jingkang Wang*\",\"Jonathan Li\",\"Cewu Lu\",\"Zhipeng Luo\",\"Han Xue\",\"Cheng Wang (*Equal contribution)\"]","{\"acronym\":\"CVPR\",\"year\":2018,\"name\":\"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\"}","dbnet.jpg","Lidar-video driving dataset: Learning driving policies effectively","true",[],{"type":13,"tag":26,"props":418,"children":419},{},[420],{"type":13,"tag":32,"props":421,"children":423},{"href":422},"/publication/",[424],{"type":23,"value":425},"Full publication list >>>",{"type":13,"tag":18,"props":427,"children":429},{"id":428},"talks",[430],{"type":23,"value":431},"‚ú® Talks",{"type":13,"tag":433,"props":434,"children":435},"ul",{},[436,449,461,483,504],{"type":13,"tag":437,"props":438,"children":439},"li",{},[440,442,448],{"type":23,"value":441},"[Jun. 2025] Invited talk @ ",{"type":13,"tag":32,"props":443,"children":445},{"href":80,"rel":444},[36],[446],{"type":23,"value":447},"Microsoft Research Aisa",{"type":23,"value":223},{"type":13,"tag":437,"props":450,"children":451},{},[452,454,459],{"type":23,"value":453},"[May. 2025] Invited talk @ ",{"type":13,"tag":302,"props":455,"children":456},{},[457],{"type":23,"value":458},"ÂÖ∑Ë∫´Êô∫ËÉΩ‰πãÂøÉ",{"type":23,"value":460}," on \"Robotic Imitation Learning with Vision-Tactile/Force Sensing\".",{"type":13,"tag":437,"props":462,"children":463},{},[464,466,473,475,482],{"type":23,"value":465},"[Apr. 2025] Invited talk @ ",{"type":13,"tag":32,"props":467,"children":470},{"href":468,"rel":469},"https://www.techbeat.net/",[36],[471],{"type":23,"value":472},"TechBeat",{"type":23,"value":474}," (Â∞ÜÈó®ÂàõÊäï) on \"Robotic Imitation Learning with Vision-Tactile/Force Sensing\". The link of the recorded video is ",{"type":13,"tag":32,"props":476,"children":479},{"href":477,"rel":478},"https://www.techbeat.net/talk-info?id=963",[36],[480],{"type":23,"value":481},"here",{"type":23,"value":223},{"type":13,"tag":437,"props":484,"children":485},{},[486,488,495,497,503],{"type":23,"value":487},"[Dec. 2024] Invited talk @ ",{"type":13,"tag":32,"props":489,"children":492},{"href":490,"rel":491},"https://www.roscon.cn/2024/index.html",[36],[493],{"type":23,"value":494},"ROSCon China 2024",{"type":23,"value":496}," workshop on \"Development Trends and Challenges in Embodied AI\". The link of the recorded video is ",{"type":13,"tag":32,"props":498,"children":501},{"href":499,"rel":500},"https://www.bilibili.com/video/BV1Z3cteAEnD/?spm_id_from=333.337.search-card.all.click&vd_source=8e062051896958b92b4759e0f4753657",[36],[502],{"type":23,"value":481},{"type":23,"value":223},{"type":13,"tag":437,"props":505,"children":506},{},[507,509,515],{"type":23,"value":508},"[Oct. 2024] Invited talk @ ",{"type":13,"tag":32,"props":510,"children":512},{"href":116,"rel":511},[36],[513],{"type":23,"value":514},"TEA lab",{"type":23,"value":516}," in Tsinghua University, IIIS on \"Efficient Learning for Long-horizon Deformable Object Manipulation\"",{"type":13,"tag":18,"props":518,"children":520},{"id":519},"awards",[521],{"type":23,"value":522},"üèÜ Awards",{"type":13,"tag":433,"props":524,"children":525},{},[526,531,536,541,546,551,556,561,566],{"type":13,"tag":437,"props":527,"children":528},{},[529],{"type":23,"value":530},"Best Student Paper Award Finalist [ÊúÄ‰Ω≥Â≠¶ÁîüËÆ∫ÊñáÂ•ñÊèêÂêç] in RSS 2025.",{"type":13,"tag":437,"props":532,"children":533},{},[534],{"type":23,"value":535},"Wu Wen Jun Scholarship [Âê¥Êñá‰øäÂ•ñÂ≠¶Èáë] in 2024 and 2025.",{"type":13,"tag":437,"props":537,"children":538},{},[539],{"type":23,"value":540},"Outstanding Graduates in Shanghai (Top 3%) [‰∏äÊµ∑Â∏Ç‰ºòÁßÄÊØï‰∏öÁîü] in 2021.",{"type":13,"tag":437,"props":542,"children":543},{},[544],{"type":23,"value":545},"Rongchang Technology Innovation Scholarship (Top 10 students in SJTU) [Ëç£Êò∂ÁßëÊäÄÂàõÊñ∞Â•ñÂ≠¶Èáë] in 2020.",{"type":13,"tag":437,"props":547,"children":548},{},[549],{"type":23,"value":550},"SenseTime Scholarship (Top 21 undergraduates in China) [ÂïÜÊ±§Â•ñÂ≠¶Èáë] in 2020.",{"type":13,"tag":437,"props":552,"children":553},{},[554],{"type":23,"value":555},"National Scholarship (Top 3 students in CS Department) in 2017, 2018 and 2019 (three consecutive years) [ÂõΩÂÆ∂Â•ñÂ≠¶ÈáëÔºàËøûÁª≠‰∏âÂπ¥Ôºâ].",{"type":13,"tag":437,"props":557,"children":558},{},[559],{"type":23,"value":560},"Academic Excellence Scholarship (Class A) of SJTU (Top 1% in SJTU) in 2018.",{"type":13,"tag":437,"props":562,"children":563},{},[564],{"type":23,"value":565},"Meritorious Winner Prize of Mathematical Contest in Modeling in 2018.",{"type":13,"tag":437,"props":567,"children":568},{},[569],{"type":23,"value":570},"1st Prize in China Undergraduate Mathematical Contest in Modeling (Shanghai Division) in 2017.",{"type":13,"tag":18,"props":572,"children":574},{"id":573},"find-me",[575],{"type":23,"value":576},"üìß Find Me",{"type":13,"tag":578,"props":579,"children":582},"contact-item",{"icon":580,"url":581},"email","mailto:xiaoxiaoxh@sjtu.edu.cn",[583],{"type":13,"tag":26,"props":584,"children":585},{},[586],{"type":23,"value":587},"Email",{"type":13,"tag":578,"props":589,"children":592},{"icon":590,"url":591},"github","https://github.com/xiaoxiaoxh",[593],{"type":13,"tag":26,"props":594,"children":595},{},[596],{"type":23,"value":597},"GitHub",{"type":13,"tag":578,"props":599,"children":602},{"icon":600,"url":601},"twitter","https://twitter.com/HanXue012",[603],{"type":13,"tag":26,"props":604,"children":605},{},[606],{"type":23,"value":607},"Twitter",{"title":5,"searchDepth":609,"depth":609,"links":610},2,[611,612,613,614,615,616,617],{"id":20,"depth":609,"text":24},{"id":125,"depth":609,"text":128},{"id":144,"depth":609,"text":147},{"id":261,"depth":609,"text":264},{"id":428,"depth":609,"text":431},{"id":519,"depth":609,"text":522},{"id":573,"depth":609,"text":576},"markdown","content:index.md","content","index.md","md",{"_path":624,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":625,"description":5,"leadingImage":626,"disableFancyImage":8,"body":627,"_type":618,"_id":812,"_source":620,"_file":813,"_extension":622},"/news","News","me-news-google.png",{"type":10,"children":628,"toc":810},[629,634,664,742,763],{"type":13,"tag":630,"props":631,"children":633},"MarkdownHeader",{"subtitle":632,"title":625},"üì¢ Latest: One paper (DeformPAM) is accepted by ICRA 2025!",[],{"type":13,"tag":433,"props":635,"children":636},{},[637],{"type":13,"tag":437,"props":638,"children":639},{},[640,645,647,654,655,662],{"type":13,"tag":157,"props":641,"children":642},{},[643],{"type":23,"value":644},"02/06/2026",{"type":23,"value":646}," üéâ Two Papers (",{"type":13,"tag":32,"props":648,"children":651},{"href":649,"rel":650},"https://ericjin2002.github.io/SOE",[36],[652],{"type":23,"value":653},"SOE",{"type":23,"value":104},{"type":13,"tag":32,"props":656,"children":659},{"href":657,"rel":658},"https://right-side-out.github.io/",[36],[660],{"type":23,"value":661},"Right-Side-Out",{"type":23,"value":663},") are accepted by ICRA 2026!",{"type":13,"tag":433,"props":665,"children":666},{},[667,693,724],{"type":13,"tag":437,"props":668,"children":669},{},[670,675,677,684,686,691],{"type":13,"tag":157,"props":671,"children":672},{},[673],{"type":23,"value":674},"06/18/2025",{"type":23,"value":676}," üî• ",{"type":13,"tag":32,"props":678,"children":681},{"href":679,"rel":680},"https://reactive-diffusion-policy.github.io/",[36],[682],{"type":23,"value":683},"RDP",{"type":23,"value":685}," is selected as the ",{"type":13,"tag":157,"props":687,"children":688},{},[689],{"type":23,"value":690},"Best Student Paper Award Finalist",{"type":23,"value":692}," @ RSS 2025!",{"type":13,"tag":437,"props":694,"children":695},{},[696,701,702,707,708,713,715,722],{"type":13,"tag":157,"props":697,"children":698},{},[699],{"type":23,"value":700},"05/23/2025",{"type":23,"value":676},{"type":13,"tag":32,"props":703,"children":705},{"href":679,"rel":704},[36],[706],{"type":23,"value":683},{"type":23,"value":685},{"type":13,"tag":157,"props":709,"children":710},{},[711],{"type":23,"value":712},"Best Paper",{"type":23,"value":714}," in ",{"type":13,"tag":32,"props":716,"children":719},{"href":717,"rel":718},"https://sites.google.com/view/icra-2025-beyond-pick-place/home",[36],[720],{"type":23,"value":721},"Beyond Pick and Place workshop",{"type":23,"value":723}," @ ICRA 2025!",{"type":13,"tag":437,"props":725,"children":726},{},[727,732,734,740],{"type":13,"tag":157,"props":728,"children":729},{},[730],{"type":23,"value":731},"04/11/2025",{"type":23,"value":733}," üéâ One paper (",{"type":13,"tag":32,"props":735,"children":737},{"href":679,"rel":736},[36],[738],{"type":23,"value":739},"Reactive Diffusion Policy",{"type":23,"value":741},") is accepted by RSS 2025!",{"type":13,"tag":433,"props":743,"children":744},{},[745],{"type":13,"tag":437,"props":746,"children":747},{},[748,753,754,761],{"type":13,"tag":157,"props":749,"children":750},{},[751],{"type":23,"value":752},"01/29/2025",{"type":23,"value":733},{"type":13,"tag":32,"props":755,"children":758},{"href":756,"rel":757},"https://deform-pam.robotflow.ai/",[36],[759],{"type":23,"value":760},"DeformPAM",{"type":23,"value":762},") is accepted by ICRA 2025!",{"type":13,"tag":433,"props":764,"children":765},{},[766,784],{"type":13,"tag":437,"props":767,"children":768},{},[769,774,775,782],{"type":13,"tag":157,"props":770,"children":771},{},[772],{"type":23,"value":773},"08/31/2023",{"type":23,"value":733},{"type":13,"tag":32,"props":776,"children":779},{"href":777,"rel":778},"https://unifolding.robotflow.ai/",[36],[780],{"type":23,"value":781},"UniFolding",{"type":23,"value":783},") is accepted by CoRL 2023!",{"type":13,"tag":437,"props":785,"children":786},{},[787,792,794,801,803,808],{"type":13,"tag":157,"props":788,"children":789},{},[790],{"type":23,"value":791},"07/14/2023",{"type":23,"value":793}," üî• One paper (",{"type":13,"tag":32,"props":795,"children":798},{"href":796,"rel":797},"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf",[36],[799],{"type":23,"value":800},"ClothPose",{"type":23,"value":802},") is accepted by ICCV 2023 as an ",{"type":13,"tag":157,"props":804,"children":805},{},[806],{"type":23,"value":807},"oral presentation",{"type":23,"value":809},"!",{"title":5,"searchDepth":609,"depth":609,"links":811},[],"content:news.md","news.md",1770691374555]