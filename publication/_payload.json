[{"data":1,"prerenderedAt":180},["Reactive",2],{"content-query-FpPJaOzdgK":3},{"_path":4,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":7,"description":5,"body":8,"_type":175,"_id":176,"_source":177,"_file":178,"_extension":179},"/publication","",false,"Publications",{"type":9,"children":10,"toc":172},"root",[11,16,26,33,40,62,90,98,115,123,130,138,147,155,163],{"type":12,"tag":13,"props":14,"children":15},"element","MarkdownHeader",{"title":7},[],{"type":12,"tag":17,"props":18,"children":25},"PublicationRow",{":artifactLinks":19,":authors":20,":venue":21,"thumbnail":22,"title":23,"type":24},"{\"Website\":\"https://implicit-rdp.github.io/\",\"arXiv\":\"https://arxiv.org/abs/2512.10946\"}","[\"Wendi Chen\",\"Han Xue\",\"Yi Wang\",\"Fangyuan Zhou\",\"Jun Lv\",\"Yang Jin\",\"Shirun Tang\",\"Chuan Wenâ€ \",\"Cewu Luâ€  (â€ equal advising)\"]","{\"acronym\":\"arXiv\",\"year\":2025,\"name\":\"arXiv\"}","ImplicitRDP.gif","ImplicitRDP: An End-to-End Visual-Force Diffusion Policy with Structural Slow-Fast Learning","conference",[],{"type":12,"tag":17,"props":27,"children":32},{":artifactLinks":28,":authors":29,":venue":21,"thumbnail":30,"title":31,"type":24},"{\"Website\":\"https://ericjin2002.github.io/SOE\",\"arXiv\":\"https://arxiv.org/abs/2509.19292\"}","[\"Yang Jin\",\"Jun Lv\",\"Han Xue\",\"Wendi Chen\",\"Chuan Wenâ€ \",\"Cewu Luâ€  (â€ equal advising)\"]","SOE.gif","SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration",[],{"type":12,"tag":17,"props":34,"children":39},{":artifactLinks":35,":authors":36,":venue":21,"thumbnail":37,"title":38,"type":24},"{\"Website\":\"https://right-side-out.github.io/\",\"arXiv\":\"https://arxiv.org/abs/2509.15953\"}","[\"Chang Yu*\",\"Siyu Ma*\",\"Wenxin Du\",\"Zeshun Zong\",\"Han Xue\",\"Wendi Chen\",\"Cewu Lu\",\"Yin Yang\",\"Xuchen Han\",\"Joseph Masterjohn\",\"Alejandro Castro\",\"Chenfanfu Jiang (*Equal contribution)\"]","Right-Side-Out.gif","Right-Side-Out: Learning Zero-Shot Sim-to-Real Garment Reversal",[],{"type":12,"tag":17,"props":41,"children":47},{":artifactLinks":42,":authors":43,":venue":44,"thumbnail":45,"title":46,"type":24},"{\"Website\":\"https://reactive-diffusion-policy.github.io/\",\"arXiv\":\"https://arxiv.org/abs/2503.02881\",\"Code\":\"https://github.com/xiaoxiaoxh/reactive_diffusion_policy\"}","[\"Han Xue*\",\"Jieji Ren*\",\"Wendi Chen*\",\"Gu Zhang\",\"Yuan Fang\",\"Guoying Gu\",\"Huazhe Xuâ€ \",\"Cewu Luâ€  (â€ Equal advising)\"]","{\"acronym\":\"RSS\",\"year\":2025,\"name\":\"Robotics: Science and Systems (RSS)\"}","rdp.gif","Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation",[48],{"type":12,"tag":49,"props":50,"children":51},"p",{},[52],{"type":12,"tag":53,"props":54,"children":58},"span",{"className":55},[56,57],"text-red-600","font-bold",[59],{"type":60,"value":61},"text","ðŸ”¥ Best Student Paper Finalist.",{"type":12,"tag":17,"props":63,"children":69},{":artifactLinks":64,":authors":65,":venue":66,"thumbnail":67,"title":68,"type":24},"{\"Website\":\"https://deform-pam.robotflow.ai/\",\"arXiv\":\"https://arxiv.org/abs/2410.11584\",\"Code\":\"https://github.com/xiaoxiaoxh/DeformPAM\"}","[\"Wendi Chen*\",\"Han Xue*\",\"Fangyuan Zhou\",\"Yuan Fang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICRA\",\"year\":2025,\"name\":\"IEEE International Conference on Robotics and Automation (ICRA)\"}","deform-pam.gif","DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object Manipulation via Preference-based Action Alignment",[70],{"type":12,"tag":49,"props":71,"children":72},{},[73],{"type":12,"tag":53,"props":74,"children":76},{"className":75},[56,57],[77,79,88],{"type":60,"value":78},"ðŸ”¥ Best Paper Finalist ",{"type":12,"tag":80,"props":81,"children":85},"a",{"href":82,"rel":83},"https://deformable-workshop.github.io/icra2025/",[84],"nofollow",[86],{"type":60,"value":87},"@ RMDO Workshop in ICRA 2025",{"type":60,"value":89},".",{"type":12,"tag":17,"props":91,"children":97},{":artifactLinks":92,":authors":93,":venue":94,"thumbnail":95,"title":96,"type":24},"{\"Proceeding\":\"https://openreview.net/pdf?id=ANJuNDFdvP\",\"arXiv\":\"https://arxiv.org/abs/2311.01267\",\"Code\":\"https://github.com/xiaoxiaoxh/UniFolding\",\"Website\":\"https://unifolding.robotflow.ai/\"}","[\"Han Xue*\",\"Yutong Li*\",\"Wenqiang Xu\",\"Huanyu Li\",\"Dongzhe Zheng\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"CoRL\",\"year\":2023,\"name\":\"7th Annual Conference on Robot Learning.\"}","unifolding.gif","UniFolding: Towards Sample-efficient, Scalable, and Generalizable Robotic Garment Folding",[],{"type":12,"tag":17,"props":99,"children":105},{":artifactLinks":100,":authors":101,":venue":102,"thumbnail":103,"title":104,"type":24},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf\"}","[\"Wenqiang Xu*\",\"Wenxin Du*\",\"Han Xue\",\"Yutong Li\",\"Ruolin Ye\",\"Yan-Feng Wang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICCV\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF International Conference on Computer Vision\"}","clothpose.png","ClothPose: A Real-world Benchmark for Visual Analysis of Garment Pose via An Indirect Recording Solution",[106],{"type":12,"tag":49,"props":107,"children":108},{},[109],{"type":12,"tag":53,"props":110,"children":112},{"className":111},[56,57],[113],{"type":60,"value":114},"ðŸ”¥ Oral Presentation.",{"type":12,"tag":17,"props":116,"children":122},{":artifactLinks":117,":authors":118,":venue":119,"thumbnail":120,"title":121,"type":24},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_GarmentTracking_Category-Level_Garment_Pose_Tracking_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.13913.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/GarmentTracking\",\"Website\":\"https://garment-tracking.robotflow.ai/\"}","[\"Han Xue\",\"Wenqiang Xu\",\"Jieyi Zhang\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Ruolin Ye\",\"Cewu Lu\"]","{\"acronym\":\"CVPR\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\"}","garment-tracking.gif","GarmentTracking: Category-Level Garment Pose Tracking",[],{"type":12,"tag":17,"props":124,"children":129},{":artifactLinks":125,":authors":126,":venue":119,"thumbnail":127,"title":128,"type":24},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Visual-Tactile_Sensing_for_In-Hand_Object_Reconstruction_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.14498.pdf\",\"Code\":\"https://github.com/jeffsonyu/VTacO\",\"Website\":\"https://sites.google.com/view/vtaco/\"}","[\"Wenqiang Xu*\",\"Zhenjun Yu*\",\"Han Xue\",\"Ruolin Ye\",\"Siqiong Yao\",\"Cewu Lu (*Equal contribution)\"]","vtaco.png","Visual-Tactile Sensing for In-Hand Object Reconstruction",[],{"type":12,"tag":17,"props":131,"children":137},{":artifactLinks":132,":authors":133,":venue":134,"thumbnail":135,"title":136,"type":24},"{\"Proceeding\":\"https://www.roboticsproceedings.org/rss19/p087.pdf\",\"Code\":\"https://github.com/mvig-robotflow/pyrfuniverse\",\"Website\":\"https://sites.google.com/view/rfuniverse\"}","[\"Haoyuan Fu*\",\"Wenqiang Xu*\",\"Ruolin Ye*\",\"Han Xue\",\"Zhenjun Yu\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Jieyi Zhang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"RSS\",\"year\":2023,\"name\":\"Robotics: Science and Systems.\"}","rfuniverse.png","Demonstrating RFUniverse: A Multiphysics Simulation Platform for Embodied AI",[],{"type":12,"tag":17,"props":139,"children":146},{":artifactLinks":140,":authors":141,":venue":142,"thumbnail":143,"title":144,"type":145},"{\"arXiv\":\"https://arxiv.org/pdf/2105.03260\"}","[\"Liu Liu*\",\"Han Xue*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"TIP\",\"year\":2022,\"name\":\"IEEE Transactions on Image Processing.\"}","articulation_real.png","Toward Real-World Category-Level Articulation Pose Estimation","journal",[],{"type":12,"tag":17,"props":148,"children":154},{":artifactLinks":149,":authors":150,":venue":151,"thumbnail":152,"title":153,"type":24},"{\"Proceeding\":\"https://www.bmvc2021-virtualconference.com/assets/papers/0544.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2112.07334.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/OMAD\"}","[\"Han Xue*\",\"Liu Liu*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"BMVC\",\"year\":2021,\"name\":\"The 32nd British Machine Vision Conference.\"}","omad.png","OMAD: Object Model with Articulated Deformations for Pose Estimation and Retrieval",[],{"type":12,"tag":17,"props":156,"children":162},{":artifactLinks":157,":authors":158,":venue":159,"thumbnail":160,"title":161,"type":24},"{\"Proceeding\":\"https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660222.pdf\",\"arXiv\":\"https://arxiv.org/pdf/1912.11473.pdf\",\"Code\":\"https://github.com/justimyhxu/Dense-RepPoints\"}","[\"Ze Yang*\",\"Yinghao Xu*\",\"Han Xue*\",\"Zheng Zhang\",\"Raquel Urtasun\",\"Liwei Wang\",\"Stephen Lin\",\"Han Hu (*Equal contribution)\"]","{\"acronym\":\"ECCV\",\"year\":2020,\"name\":\"The European Conference on Computer Vision.\"}","dense_reppoints.jpg","Dense RepPoints: Representing Visual Objects with Dense Point Sets",[],{"type":12,"tag":17,"props":164,"children":171},{":artifactLinks":165,":authors":166,":venue":167,"thumbnail":168,"title":169,"type":24,":hideBottomBorder":170},"{\"Proceeding\":\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf\",\"Code\":\"https://github.com/driving-behavior/DBNet\",\"Website\":\"http://www.dbehavior.net/\"}","[\"Yiping Chen*\",\"Jingkang Wang*\",\"Jonathan Li\",\"Cewu Lu\",\"Zhipeng Luo\",\"Han Xue\",\"Cheng Wang (*Equal contribution)\"]","{\"acronym\":\"CVPR\",\"year\":2018,\"name\":\"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\"}","dbnet.jpg","Lidar-video driving dataset: Learning driving policies effectively","true",[],{"title":5,"searchDepth":173,"depth":173,"links":174},2,[],"markdown","content:publication.md","content","publication.md","md",1766458176521]