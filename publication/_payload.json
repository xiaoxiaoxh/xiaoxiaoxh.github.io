[{"data":1,"prerenderedAt":166},["Reactive",2],{"content-query-FpPJaOzdgK":3},{"_path":4,"_dir":5,"_draft":6,"_partial":6,"_locale":5,"title":7,"description":5,"body":8,"_type":161,"_id":162,"_source":163,"_file":164,"_extension":165},"/publication","",false,"Publications",{"type":9,"children":10,"toc":158},"root",[11,16,51,76,84,101,109,116,124,133,141,149],{"type":12,"tag":13,"props":14,"children":15},"element","MarkdownHeader",{"title":7},[],{"type":12,"tag":17,"props":18,"children":25},"PublicationRow",{":artifactLinks":19,":authors":20,":venue":21,"thumbnail":22,"title":23,"type":24},"{\"Website\":\"https://reactive-diffusion-policy.github.io/\",\"arXiv\":\"https://arxiv.org/abs/2503.02881\",\"Code\":\"https://github.com/xiaoxiaoxh/reactive_diffusion_policy\"}","[\"Han Xue*\",\"Jieji Ren*\",\"Wendi Chen*\",\"Gu Zhang\",\"Yuan Fang\",\"Guoying Gu\",\"Huazhe Xuâ€ \",\"Cewu Luâ€  (â€ Equal advising)\"]","{\"acronym\":\"RSS\",\"year\":2025,\"name\":\"Robotics: Science and Systems (RSS)\"}","rdp.gif","Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation","conference",[26],{"type":12,"tag":27,"props":28,"children":29},"p",{},[30],{"type":12,"tag":31,"props":32,"children":36},"span",{"className":33},[34,35],"text-red-600","font-bold",[37,40,49],{"type":38,"value":39},"text","ðŸ”¥ Best Paper Award ",{"type":12,"tag":41,"props":42,"children":46},"a",{"href":43,"rel":44},"https://sites.google.com/view/icra-2025-beyond-pick-place/home",[45],"nofollow",[47],{"type":38,"value":48},"@ Beyond P&P Workshop in ICRA 2025",{"type":38,"value":50},".",{"type":12,"tag":17,"props":52,"children":58},{":artifactLinks":53,":authors":54,":venue":55,"thumbnail":56,"title":57,"type":24},"{\"Website\":\"https://deform-pam.robotflow.ai/\",\"arXiv\":\"https://arxiv.org/abs/2410.11584\",\"Code\":\"https://github.com/xiaoxiaoxh/DeformPAM\"}","[\"Wendi Chen*\",\"Han Xue*\",\"Fangyuan Zhou\",\"Yuan Fang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICRA\",\"year\":2025,\"name\":\"IEEE International Conference on Robotics and Automation (ICRA)\"}","deform-pam.gif","DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object Manipulation via Preference-based Action Alignment",[59],{"type":12,"tag":27,"props":60,"children":61},{},[62],{"type":12,"tag":31,"props":63,"children":65},{"className":64},[34,35],[66,68,75],{"type":38,"value":67},"ðŸ”¥ Best Paper Finalist ",{"type":12,"tag":41,"props":69,"children":72},{"href":70,"rel":71},"https://deformable-workshop.github.io/icra2025/",[45],[73],{"type":38,"value":74},"@ RMDO Workshop in ICRA 2025",{"type":38,"value":50},{"type":12,"tag":17,"props":77,"children":83},{":artifactLinks":78,":authors":79,":venue":80,"thumbnail":81,"title":82,"type":24},"{\"Proceeding\":\"https://openreview.net/pdf?id=ANJuNDFdvP\",\"arXiv\":\"https://arxiv.org/abs/2311.01267\",\"Code\":\"https://github.com/xiaoxiaoxh/UniFolding\",\"Website\":\"https://unifolding.robotflow.ai/\"}","[\"Han Xue*\",\"Yutong Li*\",\"Wenqiang Xu\",\"Huanyu Li\",\"Dongzhe Zheng\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"CoRL\",\"year\":2023,\"name\":\"7th Annual Conference on Robot Learning.\"}","unifolding.gif","UniFolding: Towards Sample-efficient, Scalable, and Generalizable Robotic Garment Folding",[],{"type":12,"tag":17,"props":85,"children":91},{":artifactLinks":86,":authors":87,":venue":88,"thumbnail":89,"title":90,"type":24},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf\"}","[\"Wenqiang Xu*\",\"Wenxin Du*\",\"Han Xue\",\"Yutong Li\",\"Ruolin Ye\",\"Yan-Feng Wang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"ICCV\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF International Conference on Computer Vision\"}","clothpose.png","ClothPose: A Real-world Benchmark for Visual Analysis of Garment Pose via An Indirect Recording Solution",[92],{"type":12,"tag":27,"props":93,"children":94},{},[95],{"type":12,"tag":31,"props":96,"children":98},{"className":97},[34,35],[99],{"type":38,"value":100},"ðŸ”¥ Oral Presentation.",{"type":12,"tag":17,"props":102,"children":108},{":artifactLinks":103,":authors":104,":venue":105,"thumbnail":106,"title":107,"type":24},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_GarmentTracking_Category-Level_Garment_Pose_Tracking_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.13913.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/GarmentTracking\",\"Website\":\"https://garment-tracking.robotflow.ai/\"}","[\"Han Xue\",\"Wenqiang Xu\",\"Jieyi Zhang\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Ruolin Ye\",\"Cewu Lu\"]","{\"acronym\":\"CVPR\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\"}","garment-tracking.gif","GarmentTracking: Category-Level Garment Pose Tracking",[],{"type":12,"tag":17,"props":110,"children":115},{":artifactLinks":111,":authors":112,":venue":105,"thumbnail":113,"title":114,"type":24},"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Visual-Tactile_Sensing_for_In-Hand_Object_Reconstruction_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.14498.pdf\",\"Code\":\"https://github.com/jeffsonyu/VTacO\",\"Website\":\"https://sites.google.com/view/vtaco/\"}","[\"Wenqiang Xu*\",\"Zhenjun Yu*\",\"Han Xue\",\"Ruolin Ye\",\"Siqiong Yao\",\"Cewu Lu (*Equal contribution)\"]","vtaco.png","Visual-Tactile Sensing for In-Hand Object Reconstruction",[],{"type":12,"tag":17,"props":117,"children":123},{":artifactLinks":118,":authors":119,":venue":120,"thumbnail":121,"title":122,"type":24},"{\"Proceeding\":\"https://www.roboticsproceedings.org/rss19/p087.pdf\",\"Code\":\"https://github.com/mvig-robotflow/pyrfuniverse\",\"Website\":\"https://sites.google.com/view/rfuniverse\"}","[\"Haoyuan Fu*\",\"Wenqiang Xu*\",\"Ruolin Ye*\",\"Han Xue\",\"Zhenjun Yu\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Jieyi Zhang\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"RSS\",\"year\":2023,\"name\":\"Robotics: Science and Systems.\"}","rfuniverse.png","Demonstrating RFUniverse: A Multiphysics Simulation Platform for Embodied AI",[],{"type":12,"tag":17,"props":125,"children":132},{":artifactLinks":126,":authors":127,":venue":128,"thumbnail":129,"title":130,"type":131},"{\"arXiv\":\"https://arxiv.org/pdf/2105.03260\"}","[\"Liu Liu*\",\"Han Xue*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"TIP\",\"year\":2022,\"name\":\"IEEE Transactions on Image Processing.\"}","articulation_real.png","Toward Real-World Category-Level Articulation Pose Estimation","journal",[],{"type":12,"tag":17,"props":134,"children":140},{":artifactLinks":135,":authors":136,":venue":137,"thumbnail":138,"title":139,"type":24},"{\"Proceeding\":\"https://www.bmvc2021-virtualconference.com/assets/papers/0544.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2112.07334.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/OMAD\"}","[\"Han Xue*\",\"Liu Liu*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]","{\"acronym\":\"BMVC\",\"year\":2021,\"name\":\"The 32nd British Machine Vision Conference.\"}","omad.png","OMAD: Object Model with Articulated Deformations for Pose Estimation and Retrieval",[],{"type":12,"tag":17,"props":142,"children":148},{":artifactLinks":143,":authors":144,":venue":145,"thumbnail":146,"title":147,"type":24},"{\"Proceeding\":\"https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660222.pdf\",\"arXiv\":\"https://arxiv.org/pdf/1912.11473.pdf\",\"Code\":\"https://github.com/justimyhxu/Dense-RepPoints\"}","[\"Ze Yang*\",\"Yinghao Xu*\",\"Han Xue*\",\"Zheng Zhang\",\"Raquel Urtasun\",\"Liwei Wang\",\"Stephen Lin\",\"Han Hu (*Equal contribution)\"]","{\"acronym\":\"ECCV\",\"year\":2020,\"name\":\"The European Conference on Computer Vision.\"}","dense_reppoints.jpg","Dense RepPoints: Representing Visual Objects with Dense Point Sets",[],{"type":12,"tag":17,"props":150,"children":157},{":artifactLinks":151,":authors":152,":venue":153,"thumbnail":154,"title":155,"type":24,":hideBottomBorder":156},"{\"Proceeding\":\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf\",\"Code\":\"https://github.com/driving-behavior/DBNet\",\"Website\":\"http://www.dbehavior.net/\"}","[\"Yiping Chen*\",\"Jingkang Wang*\",\"Jonathan Li\",\"Cewu Lu\",\"Zhipeng Luo\",\"Han Xue\",\"Cheng Wang (*Equal contribution)\"]","{\"acronym\":\"CVPR\",\"year\":2018,\"name\":\"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\"}","dbnet.jpg","Lidar-video driving dataset: Learning driving policies effectively","true",[],{"title":5,"searchDepth":159,"depth":159,"links":160},2,[],"markdown","content:publication.md","content","publication.md","md",1748495866694]