{"generatedAt":1770691374371,"generateTime":562,"contents":[{"_path":"/","_dir":"","_draft":false,"_partial":false,"_locale":"","title":"Home","description":"","hideTitle":true,"disableFancyImage":true,"body":{"type":"root","children":[{"type":"element","tag":"IndexHeader","props":{},"children":[]},{"type":"element","tag":"h2","props":{"id":"Ô∏è-about-me"},"children":[{"type":"text","value":"ü¶∏üèª‚Äç‚ôÇÔ∏è About Me"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"I am a fifth-year Computer Science Ph.D. candidate at "},{"type":"element","tag":"a","props":{"href":"https://en.sjtu.edu.cn/","rel":["nofollow"]},"children":[{"type":"text","value":"Shanghai Jiao Tong University"}]},{"type":"text","value":" and a member of "},{"type":"element","tag":"a","props":{"href":"https://www.mvig.org/","rel":["nofollow"]},"children":[{"type":"text","value":"Machine Intelligence and Vision Group (MVIG)"}]},{"type":"text","value":" under the supervision of "},{"type":"element","tag":"a","props":{"href":"https://www.mvig.org/","rel":["nofollow"]},"children":[{"type":"text","value":"Prof. Cewu Lu"}]},{"type":"text","value":" (Âç¢Á≠ñÂêæ). I am also a member of "},{"type":"element","tag":"a","props":{"href":"https://ai.sjtu.edu.cn/info/announcements/204","rel":["nofollow"]},"children":[{"type":"text","value":"Wu Wen Jun Honorary Doctoral Program"}]},{"type":"text","value":" (Âê¥Êñá‰øäËç£Ë™âÂçöÂ£´Áè≠)."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"I receive my bachelor degree from Shanghai Jiao Tong Universiy in 2021. My research interests lie in Robotics and Computer Vision. Previously, I have been working on deformable object perception and manipulation. Now I am particularly interested in imitation learning with tactile/force sensing and low-cost data collection system."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"In the past, I have interned at "},{"type":"element","tag":"a","props":{"href":"https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/","rel":["nofollow"]},"children":[{"type":"text","value":"Microsoft Research Asia"}]},{"type":"text","value":" under the supervsion of "},{"type":"element","tag":"a","props":{"href":"https://ancientmooner.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Han Hu"}]},{"type":"text","value":", "},{"type":"element","tag":"a","props":{"href":"http://yue-cao.me/","rel":["nofollow"]},"children":[{"type":"text","value":"Yue Cao"}]},{"type":"text","value":" and "},{"type":"element","tag":"a","props":{"href":"https://jifengdai.org/","rel":["nofollow"]},"children":[{"type":"text","value":"Jifeng Dai"}]},{"type":"text","value":". I also spent time with Prof. "},{"type":"element","tag":"a","props":{"href":"http://hxu.rocks/index.html","rel":["nofollow"]},"children":[{"type":"text","value":"Huazhe Xu"}]},{"type":"text","value":" at Tsinghua University on my projects. In my spare time, I enjoy watching movies and playing with robotsü§ñ."}]},{"type":"element","tag":"h2","props":{"id":"news"},"children":[{"type":"text","value":"üì∞ News"}]},{"type":"element","tag":"ShortNews","props":{},"children":[]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"a","props":{"href":"/news/"},"children":[{"type":"text","value":"More news >>>"}]}]},{"type":"element","tag":"h2","props":{"id":"experiences"},"children":[{"type":"text","value":"üè´ Experiences"}]},{"type":"element","tag":"ExperienceRow","props":{"icon":"sjtu.png"},"children":[{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Shanghai Jiao Tong University"}]},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nPh.D. Student "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nResearch assistant in "},{"type":"element","tag":"a","props":{"href":"https://www.mvig.org/","rel":["nofollow"]},"children":[{"type":"text","value":"Machine Intelligence and Vision Group (MVIG)"}]},{"type":"text","value":", advised by "},{"type":"element","tag":"a","props":{"href":"https://www.mvig.org/","rel":["nofollow"]},"children":[{"type":"text","value":"Prof. Cewu Lu"}]},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nSep. 2021 - Present"}]}]},{"type":"element","tag":"ExperienceRow","props":{"icon":"microsoft.png"},"children":[{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Microsoft Research Asia"}]},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nResearch Intern, advised by "},{"type":"element","tag":"a","props":{"href":"https://ancientmooner.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Han Hu"}]},{"type":"text","value":", "},{"type":"element","tag":"a","props":{"href":"http://yue-cao.me/","rel":["nofollow"]},"children":[{"type":"text","value":"Yue Cao"}]},{"type":"text","value":" and "},{"type":"element","tag":"a","props":{"href":"https://jifengdai.org/","rel":["nofollow"]},"children":[{"type":"text","value":"Jifeng Dai"}]},{"type":"text","value":"."},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nJul. 2019 - Mar. 2020"}]}]},{"type":"element","tag":"ExperienceRow","props":{"icon":"sjtu.png"},"children":[{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Shanghai Jiao Tong University"}]},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nBachelor of Engineering in Computer Science. "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nGPA 4.04/4.3, "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Rank 3/150 (Top 2%)"}]},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nSep 2016 - Jun. 2021"}]}]},{"type":"element","tag":"h2","props":{"id":"selected-publications"},"children":[{"type":"text","value":"üìÑ Selected Publications"}]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Website\":\"https://implicit-rdp.github.io/\",\"arXiv\":\"https://arxiv.org/abs/2512.10946\"}",":authors":"[\"Wendi Chen\",\"Han Xue\",\"Yi Wang\",\"Fangyuan Zhou\",\"Jun Lv\",\"Yang Jin\",\"Shirun Tang\",\"Chuan Wen‚Ä†\",\"Cewu Lu‚Ä† (‚Ä†equal advising)\"]",":venue":"{\"acronym\":\"arXiv\",\"year\":2025,\"name\":\"arXiv\"}","thumbnail":"ImplicitRDP.gif","title":"ImplicitRDP: An End-to-End Visual-Force Diffusion Policy with Structural Slow-Fast Learning","type":"conference"},"children":[]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Website\":\"https://ericjin2002.github.io/SOE\",\"arXiv\":\"https://arxiv.org/abs/2509.19292\"}",":authors":"[\"Yang Jin\",\"Jun Lv\",\"Han Xue\",\"Wendi Chen\",\"Chuan Wen‚Ä†\",\"Cewu Lu‚Ä† (‚Ä†equal advising)\"]",":venue":"{\"acronym\":\"ICRA\",\"year\":2026,\"name\":\"IEEE International Conference on Robotics and Automation (ICRA)\"}","thumbnail":"SOE.gif","title":"SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration","type":"conference"},"children":[]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Website\":\"https://right-side-out.github.io/\",\"arXiv\":\"https://arxiv.org/abs/2509.15953\"}",":authors":"[\"Chang Yu*\",\"Siyu Ma*\",\"Wenxin Du\",\"Zeshun Zong\",\"Han Xue\",\"Wendi Chen\",\"Cewu Lu\",\"Yin Yang\",\"Xuchen Han\",\"Joseph Masterjohn\",\"Alejandro Castro\",\"Chenfanfu Jiang (*Equal contribution)\"]",":venue":"{\"acronym\":\"ICRA\",\"year\":2026,\"name\":\"IEEE International Conference on Robotics and Automation (ICRA)\"}","thumbnail":"Right-Side-Out.gif","title":"Right-Side-Out: Learning Zero-Shot Sim-to-Real Garment Reversal","type":"conference"},"children":[]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Website\":\"https://reactive-diffusion-policy.github.io/\",\"arXiv\":\"https://arxiv.org/abs/2503.02881\",\"Code\":\"https://github.com/xiaoxiaoxh/reactive_diffusion_policy\"}",":authors":"[\"Han Xue*\",\"Jieji Ren*\",\"Wendi Chen*\",\"Gu Zhang\",\"Yuan Fang\",\"Guoying Gu\",\"Huazhe Xu‚Ä†\",\"Cewu Lu‚Ä† (‚Ä†Equal advising)\"]",":venue":"{\"acronym\":\"RSS\",\"year\":2025,\"name\":\"Robotics: Science and Systems (RSS)\"}","thumbnail":"rdp.gif","title":"Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation","type":"conference"},"children":[{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"span","props":{"className":["text-red-600","font-bold"]},"children":[{"type":"text","value":"üî• Best Student Paper Finalist."}]}]}]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Website\":\"https://deform-pam.robotflow.ai/\",\"arXiv\":\"https://arxiv.org/abs/2410.11584\",\"Code\":\"https://github.com/xiaoxiaoxh/DeformPAM\"}",":authors":"[\"Wendi Chen*\",\"Han Xue*\",\"Fangyuan Zhou\",\"Yuan Fang\",\"Cewu Lu (*Equal contribution)\"]",":venue":"{\"acronym\":\"ICRA\",\"year\":2025,\"name\":\"IEEE International Conference on Robotics and Automation (ICRA)\"}","thumbnail":"deform-pam.gif","title":"DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object Manipulation via Preference-based Action Alignment","type":"conference"},"children":[{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"span","props":{"className":["text-red-600","font-bold"]},"children":[{"type":"text","value":"üî• Best Paper Finalist "},{"type":"element","tag":"a","props":{"href":"https://deformable-workshop.github.io/icra2025/","rel":["nofollow"]},"children":[{"type":"text","value":"@ RMDO Workshop in ICRA 2025"}]},{"type":"text","value":"."}]}]}]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Proceeding\":\"https://openreview.net/pdf?id=ANJuNDFdvP\",\"arXiv\":\"https://arxiv.org/abs/2311.01267\",\"Code\":\"https://github.com/xiaoxiaoxh/UniFolding\",\"Website\":\"https://unifolding.robotflow.ai/\"}",":authors":"[\"Han Xue*\",\"Yutong Li*\",\"Wenqiang Xu\",\"Huanyu Li\",\"Dongzhe Zheng\",\"Cewu Lu (*Equal contribution)\"]",":venue":"{\"acronym\":\"CoRL\",\"year\":2023,\"name\":\"7th Annual Conference on Robot Learning.\"}","thumbnail":"unifolding.gif","title":"UniFolding: Towards Sample-efficient, Scalable, and Generalizable Robotic Garment Folding","type":"conference"},"children":[]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Proceeding\":\"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf\"}",":authors":"[\"Wenqiang Xu*\",\"Wenxin Du*\",\"Han Xue\",\"Yutong Li\",\"Ruolin Ye\",\"Yan-Feng Wang\",\"Cewu Lu (*Equal contribution)\"]",":venue":"{\"acronym\":\"ICCV\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF International Conference on Computer Vision\"}","thumbnail":"clothpose.png","title":"ClothPose: A Real-world Benchmark for Visual Analysis of Garment Pose via An Indirect Recording Solution","type":"conference"},"children":[{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"span","props":{"className":["text-red-600","font-bold"]},"children":[{"type":"text","value":"üî• Oral Presentation."}]}]}]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_GarmentTracking_Category-Level_Garment_Pose_Tracking_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.13913.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/GarmentTracking\",\"Website\":\"https://garment-tracking.robotflow.ai/\"}",":authors":"[\"Han Xue\",\"Wenqiang Xu\",\"Jieyi Zhang\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Ruolin Ye\",\"Cewu Lu\"]",":venue":"{\"acronym\":\"CVPR\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\"}","thumbnail":"garment-tracking.gif","title":"GarmentTracking: Category-Level Garment Pose Tracking","type":"conference"},"children":[]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Visual-Tactile_Sensing_for_In-Hand_Object_Reconstruction_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.14498.pdf\",\"Code\":\"https://github.com/jeffsonyu/VTacO\",\"Website\":\"https://sites.google.com/view/vtaco/\"}",":authors":"[\"Wenqiang Xu*\",\"Zhenjun Yu*\",\"Han Xue\",\"Ruolin Ye\",\"Siqiong Yao\",\"Cewu Lu (*Equal contribution)\"]",":venue":"{\"acronym\":\"CVPR\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\"}","thumbnail":"vtaco.png","title":"Visual-Tactile Sensing for In-Hand Object Reconstruction","type":"conference"},"children":[]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Proceeding\":\"https://www.roboticsproceedings.org/rss19/p087.pdf\",\"Code\":\"https://github.com/mvig-robotflow/pyrfuniverse\",\"Website\":\"https://sites.google.com/view/rfuniverse\"}",":authors":"[\"Haoyuan Fu*\",\"Wenqiang Xu*\",\"Ruolin Ye*\",\"Han Xue\",\"Zhenjun Yu\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Jieyi Zhang\",\"Cewu Lu (*Equal contribution)\"]",":venue":"{\"acronym\":\"RSS\",\"year\":2023,\"name\":\"Robotics: Science and Systems.\"}","thumbnail":"rfuniverse.png","title":"Demonstrating RFUniverse: A Multiphysics Simulation Platform for Embodied AI","type":"conference"},"children":[]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"arXiv\":\"https://arxiv.org/pdf/2105.03260\"}",":authors":"[\"Liu Liu*\",\"Han Xue*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]",":venue":"{\"acronym\":\"TIP\",\"year\":2022,\"name\":\"IEEE Transactions on Image Processing.\"}","thumbnail":"articulation_real.png","title":"Toward Real-World Category-Level Articulation Pose Estimation","type":"journal"},"children":[]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Proceeding\":\"https://www.bmvc2021-virtualconference.com/assets/papers/0544.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2112.07334.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/OMAD\"}",":authors":"[\"Han Xue*\",\"Liu Liu*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]",":venue":"{\"acronym\":\"BMVC\",\"year\":2021,\"name\":\"The 32nd British Machine Vision Conference.\"}","thumbnail":"omad.png","title":"OMAD: Object Model with Articulated Deformations for Pose Estimation and Retrieval","type":"conference"},"children":[]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Proceeding\":\"https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660222.pdf\",\"arXiv\":\"https://arxiv.org/pdf/1912.11473.pdf\",\"Code\":\"https://github.com/justimyhxu/Dense-RepPoints\"}",":authors":"[\"Ze Yang*\",\"Yinghao Xu*\",\"Han Xue*\",\"Zheng Zhang\",\"Raquel Urtasun\",\"Liwei Wang\",\"Stephen Lin\",\"Han Hu (*Equal contribution)\"]",":venue":"{\"acronym\":\"ECCV\",\"year\":2020,\"name\":\"The European Conference on Computer Vision.\"}","thumbnail":"dense_reppoints.jpg","title":"Dense RepPoints: Representing Visual Objects with Dense Point Sets","type":"conference"},"children":[]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Proceeding\":\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf\",\"Code\":\"https://github.com/driving-behavior/DBNet\",\"Website\":\"http://www.dbehavior.net/\"}",":authors":"[\"Yiping Chen*\",\"Jingkang Wang*\",\"Jonathan Li\",\"Cewu Lu\",\"Zhipeng Luo\",\"Han Xue\",\"Cheng Wang (*Equal contribution)\"]",":venue":"{\"acronym\":\"CVPR\",\"year\":2018,\"name\":\"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\"}","thumbnail":"dbnet.jpg","title":"Lidar-video driving dataset: Learning driving policies effectively","type":"conference",":hideBottomBorder":"true"},"children":[]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"a","props":{"href":"/publication/"},"children":[{"type":"text","value":"Full publication list >>>"}]}]},{"type":"element","tag":"h2","props":{"id":"talks"},"children":[{"type":"text","value":"‚ú® Talks"}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"[Jun. 2025] Invited talk @ "},{"type":"element","tag":"a","props":{"href":"https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/","rel":["nofollow"]},"children":[{"type":"text","value":"Microsoft Research Aisa"}]},{"type":"text","value":"."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"[May. 2025] Invited talk @ "},{"type":"element","tag":"span","props":{},"children":[{"type":"text","value":"ÂÖ∑Ë∫´Êô∫ËÉΩ‰πãÂøÉ"}]},{"type":"text","value":" on \"Robotic Imitation Learning with Vision-Tactile/Force Sensing\"."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"[Apr. 2025] Invited talk @ "},{"type":"element","tag":"a","props":{"href":"https://www.techbeat.net/","rel":["nofollow"]},"children":[{"type":"text","value":"TechBeat"}]},{"type":"text","value":" (Â∞ÜÈó®ÂàõÊäï) on \"Robotic Imitation Learning with Vision-Tactile/Force Sensing\". The link of the recorded video is "},{"type":"element","tag":"a","props":{"href":"https://www.techbeat.net/talk-info?id=963","rel":["nofollow"]},"children":[{"type":"text","value":"here"}]},{"type":"text","value":"."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"[Dec. 2024] Invited talk @ "},{"type":"element","tag":"a","props":{"href":"https://www.roscon.cn/2024/index.html","rel":["nofollow"]},"children":[{"type":"text","value":"ROSCon China 2024"}]},{"type":"text","value":" workshop on \"Development Trends and Challenges in Embodied AI\". The link of the recorded video is "},{"type":"element","tag":"a","props":{"href":"https://www.bilibili.com/video/BV1Z3cteAEnD/?spm_id_from=333.337.search-card.all.click&vd_source=8e062051896958b92b4759e0f4753657","rel":["nofollow"]},"children":[{"type":"text","value":"here"}]},{"type":"text","value":"."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"[Oct. 2024] Invited talk @ "},{"type":"element","tag":"a","props":{"href":"http://hxu.rocks/index.html","rel":["nofollow"]},"children":[{"type":"text","value":"TEA lab"}]},{"type":"text","value":" in Tsinghua University, IIIS on \"Efficient Learning for Long-horizon Deformable Object Manipulation\""}]}]},{"type":"element","tag":"h2","props":{"id":"awards"},"children":[{"type":"text","value":"üèÜ Awards"}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Best Student Paper Award Finalist [ÊúÄ‰Ω≥Â≠¶ÁîüËÆ∫ÊñáÂ•ñÊèêÂêç] in RSS 2025."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Wu Wen Jun Scholarship [Âê¥Êñá‰øäÂ•ñÂ≠¶Èáë] in 2024 and 2025."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Outstanding Graduates in Shanghai (Top 3%) [‰∏äÊµ∑Â∏Ç‰ºòÁßÄÊØï‰∏öÁîü] in 2021."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Rongchang Technology Innovation Scholarship (Top 10 students in SJTU) [Ëç£Êò∂ÁßëÊäÄÂàõÊñ∞Â•ñÂ≠¶Èáë] in 2020."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"SenseTime Scholarship (Top 21 undergraduates in China) [ÂïÜÊ±§Â•ñÂ≠¶Èáë] in 2020."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"National Scholarship (Top 3 students in CS Department) in 2017, 2018 and 2019 (three consecutive years) [ÂõΩÂÆ∂Â•ñÂ≠¶ÈáëÔºàËøûÁª≠‰∏âÂπ¥Ôºâ]."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Academic Excellence Scholarship (Class A) of SJTU (Top 1% in SJTU) in 2018."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Meritorious Winner Prize of Mathematical Contest in Modeling in 2018."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"1st Prize in China Undergraduate Mathematical Contest in Modeling (Shanghai Division) in 2017."}]}]},{"type":"element","tag":"h2","props":{"id":"find-me"},"children":[{"type":"text","value":"üìß Find Me"}]},{"type":"element","tag":"contact-item","props":{"icon":"email","url":"mailto:xiaoxiaoxh@sjtu.edu.cn"},"children":[{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"Email"}]}]},{"type":"element","tag":"contact-item","props":{"icon":"github","url":"https://github.com/xiaoxiaoxh"},"children":[{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"GitHub"}]}]},{"type":"element","tag":"contact-item","props":{"icon":"twitter","url":"https://twitter.com/HanXue012"},"children":[{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"Twitter"}]}]}],"toc":{"title":"","searchDepth":2,"depth":2,"links":[{"id":"Ô∏è-about-me","depth":2,"text":"ü¶∏üèª‚Äç‚ôÇÔ∏è About Me"},{"id":"news","depth":2,"text":"üì∞ News"},{"id":"experiences","depth":2,"text":"üè´ Experiences"},{"id":"selected-publications","depth":2,"text":"üìÑ Selected Publications"},{"id":"talks","depth":2,"text":"‚ú® Talks"},{"id":"awards","depth":2,"text":"üèÜ Awards"},{"id":"find-me","depth":2,"text":"üìß Find Me"}]}},"_type":"markdown","_id":"content:index.md","_source":"content","_file":"index.md","_extension":"md"},{"_path":"/news","_dir":"","_draft":false,"_partial":false,"_locale":"","title":"News","description":"","leadingImage":"me-news-google.png","disableFancyImage":true,"body":{"type":"root","children":[{"type":"element","tag":"MarkdownHeader","props":{"subtitle":"üì¢ Latest: One paper (DeformPAM) is accepted by ICRA 2025!","title":"News"},"children":[]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"02/06/2026"}]},{"type":"text","value":" üéâ Two Papers ("},{"type":"element","tag":"a","props":{"href":"https://ericjin2002.github.io/SOE","rel":["nofollow"]},"children":[{"type":"text","value":"SOE"}]},{"type":"text","value":" and "},{"type":"element","tag":"a","props":{"href":"https://right-side-out.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Right-Side-Out"}]},{"type":"text","value":") are accepted by ICRA 2026!"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"06/18/2025"}]},{"type":"text","value":" üî• "},{"type":"element","tag":"a","props":{"href":"https://reactive-diffusion-policy.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"RDP"}]},{"type":"text","value":" is selected as the "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Best Student Paper Award Finalist"}]},{"type":"text","value":" @ RSS 2025!"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"05/23/2025"}]},{"type":"text","value":" üî• "},{"type":"element","tag":"a","props":{"href":"https://reactive-diffusion-policy.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"RDP"}]},{"type":"text","value":" is selected as the "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Best Paper"}]},{"type":"text","value":" in "},{"type":"element","tag":"a","props":{"href":"https://sites.google.com/view/icra-2025-beyond-pick-place/home","rel":["nofollow"]},"children":[{"type":"text","value":"Beyond Pick and Place workshop"}]},{"type":"text","value":" @ ICRA 2025!"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"04/11/2025"}]},{"type":"text","value":" üéâ One paper ("},{"type":"element","tag":"a","props":{"href":"https://reactive-diffusion-policy.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Reactive Diffusion Policy"}]},{"type":"text","value":") is accepted by RSS 2025!"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"01/29/2025"}]},{"type":"text","value":" üéâ One paper ("},{"type":"element","tag":"a","props":{"href":"https://deform-pam.robotflow.ai/","rel":["nofollow"]},"children":[{"type":"text","value":"DeformPAM"}]},{"type":"text","value":") is accepted by ICRA 2025!"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"08/31/2023"}]},{"type":"text","value":" üéâ One paper ("},{"type":"element","tag":"a","props":{"href":"https://unifolding.robotflow.ai/","rel":["nofollow"]},"children":[{"type":"text","value":"UniFolding"}]},{"type":"text","value":") is accepted by CoRL 2023!"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"07/14/2023"}]},{"type":"text","value":" üî• One paper ("},{"type":"element","tag":"a","props":{"href":"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf","rel":["nofollow"]},"children":[{"type":"text","value":"ClothPose"}]},{"type":"text","value":") is accepted by ICCV 2023 as an "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"oral presentation"}]},{"type":"text","value":"!"}]}]}],"toc":{"title":"","searchDepth":2,"depth":2,"links":[]}},"_type":"markdown","_id":"content:news.md","_source":"content","_file":"news.md","_extension":"md"},{"_path":"/publication","_dir":"","_draft":false,"_partial":false,"_locale":"","title":"Publications","description":"","body":{"type":"root","children":[{"type":"element","tag":"MarkdownHeader","props":{"title":"Publications"},"children":[]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Website\":\"https://implicit-rdp.github.io/\",\"arXiv\":\"https://arxiv.org/abs/2512.10946\"}",":authors":"[\"Wendi Chen\",\"Han Xue\",\"Yi Wang\",\"Fangyuan Zhou\",\"Jun Lv\",\"Yang Jin\",\"Shirun Tang\",\"Chuan Wen‚Ä†\",\"Cewu Lu‚Ä† (‚Ä†equal advising)\"]",":venue":"{\"acronym\":\"arXiv\",\"year\":2025,\"name\":\"arXiv\"}","thumbnail":"ImplicitRDP.gif","title":"ImplicitRDP: An End-to-End Visual-Force Diffusion Policy with Structural Slow-Fast Learning","type":"conference"},"children":[]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Website\":\"https://ericjin2002.github.io/SOE\",\"arXiv\":\"https://arxiv.org/abs/2509.19292\"}",":authors":"[\"Yang Jin\",\"Jun Lv\",\"Han Xue\",\"Wendi Chen\",\"Chuan Wen‚Ä†\",\"Cewu Lu‚Ä† (‚Ä†equal advising)\"]",":venue":"{\"acronym\":\"ICRA\",\"year\":2026,\"name\":\"IEEE International Conference on Robotics and Automation (ICRA)\"}","thumbnail":"SOE.gif","title":"SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration","type":"conference"},"children":[]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Website\":\"https://right-side-out.github.io/\",\"arXiv\":\"https://arxiv.org/abs/2509.15953\"}",":authors":"[\"Chang Yu*\",\"Siyu Ma*\",\"Wenxin Du\",\"Zeshun Zong\",\"Han Xue\",\"Wendi Chen\",\"Cewu Lu\",\"Yin Yang\",\"Xuchen Han\",\"Joseph Masterjohn\",\"Alejandro Castro\",\"Chenfanfu Jiang (*Equal contribution)\"]",":venue":"{\"acronym\":\"ICRA\",\"year\":2026,\"name\":\"IEEE International Conference on Robotics and Automation (ICRA)\"}","thumbnail":"Right-Side-Out.gif","title":"Right-Side-Out: Learning Zero-Shot Sim-to-Real Garment Reversal","type":"conference"},"children":[]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Website\":\"https://reactive-diffusion-policy.github.io/\",\"arXiv\":\"https://arxiv.org/abs/2503.02881\",\"Code\":\"https://github.com/xiaoxiaoxh/reactive_diffusion_policy\"}",":authors":"[\"Han Xue*\",\"Jieji Ren*\",\"Wendi Chen*\",\"Gu Zhang\",\"Yuan Fang\",\"Guoying Gu\",\"Huazhe Xu‚Ä†\",\"Cewu Lu‚Ä† (‚Ä†Equal advising)\"]",":venue":"{\"acronym\":\"RSS\",\"year\":2025,\"name\":\"Robotics: Science and Systems (RSS)\"}","thumbnail":"rdp.gif","title":"Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation","type":"conference"},"children":[{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"span","props":{"className":["text-red-600","font-bold"]},"children":[{"type":"text","value":"üî• Best Student Paper Finalist."}]}]}]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Website\":\"https://deform-pam.robotflow.ai/\",\"arXiv\":\"https://arxiv.org/abs/2410.11584\",\"Code\":\"https://github.com/xiaoxiaoxh/DeformPAM\"}",":authors":"[\"Wendi Chen*\",\"Han Xue*\",\"Fangyuan Zhou\",\"Yuan Fang\",\"Cewu Lu (*Equal contribution)\"]",":venue":"{\"acronym\":\"ICRA\",\"year\":2025,\"name\":\"IEEE International Conference on Robotics and Automation (ICRA)\"}","thumbnail":"deform-pam.gif","title":"DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object Manipulation via Preference-based Action Alignment","type":"conference"},"children":[{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"span","props":{"className":["text-red-600","font-bold"]},"children":[{"type":"text","value":"üî• Best Paper Finalist "},{"type":"element","tag":"a","props":{"href":"https://deformable-workshop.github.io/icra2025/","rel":["nofollow"]},"children":[{"type":"text","value":"@ RMDO Workshop in ICRA 2025"}]},{"type":"text","value":"."}]}]}]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Proceeding\":\"https://openreview.net/pdf?id=ANJuNDFdvP\",\"arXiv\":\"https://arxiv.org/abs/2311.01267\",\"Code\":\"https://github.com/xiaoxiaoxh/UniFolding\",\"Website\":\"https://unifolding.robotflow.ai/\"}",":authors":"[\"Han Xue*\",\"Yutong Li*\",\"Wenqiang Xu\",\"Huanyu Li\",\"Dongzhe Zheng\",\"Cewu Lu (*Equal contribution)\"]",":venue":"{\"acronym\":\"CoRL\",\"year\":2023,\"name\":\"7th Annual Conference on Robot Learning.\"}","thumbnail":"unifolding.gif","title":"UniFolding: Towards Sample-efficient, Scalable, and Generalizable Robotic Garment Folding","type":"conference"},"children":[]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Proceeding\":\"https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf\"}",":authors":"[\"Wenqiang Xu*\",\"Wenxin Du*\",\"Han Xue\",\"Yutong Li\",\"Ruolin Ye\",\"Yan-Feng Wang\",\"Cewu Lu (*Equal contribution)\"]",":venue":"{\"acronym\":\"ICCV\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF International Conference on Computer Vision\"}","thumbnail":"clothpose.png","title":"ClothPose: A Real-world Benchmark for Visual Analysis of Garment Pose via An Indirect Recording Solution","type":"conference"},"children":[{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"span","props":{"className":["text-red-600","font-bold"]},"children":[{"type":"text","value":"üî• Oral Presentation."}]}]}]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_GarmentTracking_Category-Level_Garment_Pose_Tracking_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.13913.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/GarmentTracking\",\"Website\":\"https://garment-tracking.robotflow.ai/\"}",":authors":"[\"Han Xue\",\"Wenqiang Xu\",\"Jieyi Zhang\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Ruolin Ye\",\"Cewu Lu\"]",":venue":"{\"acronym\":\"CVPR\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\"}","thumbnail":"garment-tracking.gif","title":"GarmentTracking: Category-Level Garment Pose Tracking","type":"conference"},"children":[]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Proceeding\":\"https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Visual-Tactile_Sensing_for_In-Hand_Object_Reconstruction_CVPR_2023_paper.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2303.14498.pdf\",\"Code\":\"https://github.com/jeffsonyu/VTacO\",\"Website\":\"https://sites.google.com/view/vtaco/\"}",":authors":"[\"Wenqiang Xu*\",\"Zhenjun Yu*\",\"Han Xue\",\"Ruolin Ye\",\"Siqiong Yao\",\"Cewu Lu (*Equal contribution)\"]",":venue":"{\"acronym\":\"CVPR\",\"year\":2023,\"name\":\"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\"}","thumbnail":"vtaco.png","title":"Visual-Tactile Sensing for In-Hand Object Reconstruction","type":"conference"},"children":[]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Proceeding\":\"https://www.roboticsproceedings.org/rss19/p087.pdf\",\"Code\":\"https://github.com/mvig-robotflow/pyrfuniverse\",\"Website\":\"https://sites.google.com/view/rfuniverse\"}",":authors":"[\"Haoyuan Fu*\",\"Wenqiang Xu*\",\"Ruolin Ye*\",\"Han Xue\",\"Zhenjun Yu\",\"Tutian Tang\",\"Yutong Li\",\"Wenxin Du\",\"Jieyi Zhang\",\"Cewu Lu (*Equal contribution)\"]",":venue":"{\"acronym\":\"RSS\",\"year\":2023,\"name\":\"Robotics: Science and Systems.\"}","thumbnail":"rfuniverse.png","title":"Demonstrating RFUniverse: A Multiphysics Simulation Platform for Embodied AI","type":"conference"},"children":[]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"arXiv\":\"https://arxiv.org/pdf/2105.03260\"}",":authors":"[\"Liu Liu*\",\"Han Xue*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]",":venue":"{\"acronym\":\"TIP\",\"year\":2022,\"name\":\"IEEE Transactions on Image Processing.\"}","thumbnail":"articulation_real.png","title":"Toward Real-World Category-Level Articulation Pose Estimation","type":"journal"},"children":[]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Proceeding\":\"https://www.bmvc2021-virtualconference.com/assets/papers/0544.pdf\",\"arXiv\":\"https://arxiv.org/pdf/2112.07334.pdf\",\"Code\":\"https://github.com/xiaoxiaoxh/OMAD\"}",":authors":"[\"Han Xue*\",\"Liu Liu*\",\"Wenqiang Xu\",\"Haoyuan Fu\",\"Cewu Lu (*Equal contribution)\"]",":venue":"{\"acronym\":\"BMVC\",\"year\":2021,\"name\":\"The 32nd British Machine Vision Conference.\"}","thumbnail":"omad.png","title":"OMAD: Object Model with Articulated Deformations for Pose Estimation and Retrieval","type":"conference"},"children":[]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Proceeding\":\"https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660222.pdf\",\"arXiv\":\"https://arxiv.org/pdf/1912.11473.pdf\",\"Code\":\"https://github.com/justimyhxu/Dense-RepPoints\"}",":authors":"[\"Ze Yang*\",\"Yinghao Xu*\",\"Han Xue*\",\"Zheng Zhang\",\"Raquel Urtasun\",\"Liwei Wang\",\"Stephen Lin\",\"Han Hu (*Equal contribution)\"]",":venue":"{\"acronym\":\"ECCV\",\"year\":2020,\"name\":\"The European Conference on Computer Vision.\"}","thumbnail":"dense_reppoints.jpg","title":"Dense RepPoints: Representing Visual Objects with Dense Point Sets","type":"conference"},"children":[]},{"type":"element","tag":"PublicationRow","props":{":artifactLinks":"{\"Proceeding\":\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf\",\"Code\":\"https://github.com/driving-behavior/DBNet\",\"Website\":\"http://www.dbehavior.net/\"}",":authors":"[\"Yiping Chen*\",\"Jingkang Wang*\",\"Jonathan Li\",\"Cewu Lu\",\"Zhipeng Luo\",\"Han Xue\",\"Cheng Wang (*Equal contribution)\"]",":venue":"{\"acronym\":\"CVPR\",\"year\":2018,\"name\":\"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\"}","thumbnail":"dbnet.jpg","title":"Lidar-video driving dataset: Learning driving policies effectively","type":"conference",":hideBottomBorder":"true"},"children":[]}],"toc":{"title":"","searchDepth":2,"depth":2,"links":[]}},"_type":"markdown","_id":"content:publication.md","_source":"content","_file":"publication.md","_extension":"md"}],"navigation":[{"title":"Home","_path":"/"},{"title":"News","_path":"/news"},{"title":"Publications","_path":"/publication"}]}